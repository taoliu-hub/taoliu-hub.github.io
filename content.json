{"pages":[{"title":"About","date":"2022-05-12T08:53:51.333Z","path":"about/index.html","text":"Welcome TaoLiu’s Blog"},{"title":"Categories","date":"2022-05-12T08:42:52.543Z","path":"categories/index.html","text":""},{"title":"Tags","date":"2022-05-12T08:42:52.567Z","path":"tags/index.html","text":""}],"posts":[{"title":"JAVA数据结构和算法","date":"2022-05-12T09:35:21.866Z","path":"blog/数据结构和算法/JAVA数据结构和算法/","text":"JAVA数据结构和算法： 数据结构分类：线性结构和非线性结构： 问题一：什么是线性和非线性； 个人的理解是：数据结构中线性结构指的是数据元素之间存在着“一对一”的线性关系的数据结构；线性结构包括：数组，链表，队列，栈；非线性结构包括：树，图，表； 详解：一.线性结构 1.数组特点：我们都知道数组中的元素在内存中连续存储的，可以根据是下标快速访问元素，因此，查询速度很快，然而插入和删除时，需要对元素移动空间，比较慢。数组使用场景：频繁查询，很少增加和删除的情况。 2.链表特点：元素可以不连续内存中，是以索引将数据联系起来的，当查询元素的时候需要从头开始查询，所以效率比较低，然而添加和删除的只需要修改索引就可以了链表使用场景：少查询，需要频繁的插入或删除情况 3.队列特点：先进先出，队列使用场景：多线程阻塞队列管理非常有用 4.栈特点：先进后出，就像一个箱子，栈使用场景：实现递归以及表示式 5.数组与链表的区别数组连续，链表不连续（从数据存储形式来说）数组内存静态分配，链表动态分配数组查询复杂度O(1)，链表查询复杂度O(n)数组添加或删除，复杂度O(n),链表添加删除，复杂度O(1)数组从栈中分配内存。链表从堆中分配内存。 补充：时间复杂度O(1), O(n), O(logn), O(nlogn)指什么 描述算法复杂度时,常用o(1), o(n), o(logn), o(nlogn)表示对应算法的时间复杂度，是算法的时空复杂度的表示。不仅仅用于表示时间复杂度，也用于表示空间复杂度。O后面的括号中有一个函数，指明某个算法的耗时&#x2F;耗空间与数据增长量之间的关系。其中的n代表输入数据的量。 O(1)： 是最低的时空复杂度了，代表耗时&#x2F;耗空间与输入数据大小无关，无论输入数据增大多少倍，耗时&#x2F;耗空间都不变。 哈希算法就是典型的O(1)时间复杂度，无论数据规模多大，都可以在一次计算后找到目标（不考虑冲突的话） O(n)： 代表数据量增大几倍，耗时也增大几倍。比如常见的遍历算法。 O(n^2)： 代表数据量增大n倍时，耗时增大n的平方倍，这是比线性更高的时间复杂度。比如冒泡排序，就是典型的O(n^2)的算法，对n个数排序，需要扫描n×n次。 O(logn)： 代表当数据增大n倍时，耗时增大logn倍（这里的log是以2为底的，比如，当数据增大256倍时，耗时只增大8倍，是比线性还要低的时间复杂度）。二分查找就是O(logn)的算法，每找一次排除一半的可能，256个数据中查找只要找8次就可以找到目标。 O(nlogn)： 代表n乘以logn，当数据增大256倍时，耗时增大256*8&#x3D;2048倍。这个复杂度高于线性低于平方。归并排序就是O(nlogn)的时间复杂度。 问题二：c1）插入排序（直接插入排序、希尔排序） 2）交换排序（冒泡排序、快速排序）3）选择排序（直接选择排序、堆排序）4）归并排序5）分配排序（基数排序）特点:所需辅助空间最多：归并排序所需辅助空间最少：堆排序平均速度最快：快速排序不稳定：快速排序，希尔排序，堆排序 直接插入排序 基本思想：在要排序的一组数中，假设前面(n-1)[n&gt;&#x3D;2] 个数已经是排好顺序的，现在要把第n 个数插到前面的有序数中，使得这 n个数也是排好顺序的。如此反复循环，直到全部排好顺序 12345678910111213141516/** * 插入排序法 * * @param datas */ public static int[] sortInsert(int[] datas) &#123; for (int i = 1; i &lt; datas.length; i++) &#123; int j = i - 1; AlgorithmUtil.temp = datas[i]; for (; j &gt;= 0 &amp;&amp; AlgorithmUtil.temp &lt; datas[j]; j--) &#123; datas[j + 1] = datas[j]; &#125; datas[j + 1] = AlgorithmUtil.temp; &#125; return datas; &#125; 简单选择排序 基本思想：在要排序的一组数中，选出最小的一个数与第一个位置的数交换；然后在剩下的数当中再找最小的与第二个位置的数交换，如此循环到倒数第二个数和最后一个数比较为止。 1234567891011121314151617/** * 选择排序 * * @return */ public static int[] sortSelect(int[] datas) &#123; for (int i = 0; i &lt; datas.length; i++) &#123; int index = i; for (int j = i + 1; j &lt; datas.length; j++) &#123; if (datas[j] &lt; datas[index]) index = j; &#125; if (i != index) AlgorithmUtil.swap(datas, i, index); &#125; return datas; &#125; 冒泡排序 基本思想：在要排序的一组数中，对当前还未排好序的范围内的全部数，自上而下对相邻的两个数依次进行比较和调整，让较大的数往下沉，较小的往上冒。即：每当两相邻的数比较后发现它们的排序与排序要求相反时，就将它们互换。 1234567891011121314/** * 冒泡排序 * * @return */ public static int[] sortBubble(int[] datas) &#123; for (int i = 0; i &lt; datas.length - 1; i++) &#123; for (int j = 0; j &lt; datas.length - 1 - i; j++) &#123; if (datas[j] &gt; datas[j + 1]) AlgorithmUtil.swap(datas, j, j + 1); &#125; &#125; return datas; &#125; 快速排序 基本思想：选择一个基准元素,通常选择第一个元素或者最后一个元素,通过一趟扫描，将待排序列分成两部分,一部分比基准元素小,一部分大于等于基准元素,此时基准元素在其排好序后的正确位置,然后再用同样的方法递归地排序划分的两部分。 1234567891011121314151617181920212223242526272829303132/** * 快速排序；分割数组 * * @param datas */ public static int QuickPartition(int[] datas, int left, int right) &#123; int pivot = datas[left]; while (left &lt; right) &#123; while (left &lt; right &amp;&amp; datas[right] &gt;= pivot) --right; datas[left] = datas[right]; // 将比枢轴小的元素移到低端，此时right位相当于空，等待低位比pivotkey大的数补上 while (left &lt; right &amp;&amp; datas[left] &lt;= pivot) ++left; datas[right] = datas[left]; // 将比枢轴大的元素移到高端，此时left位相当于空，等待高位比pivotkey小的数补上 &#125; datas[left] = pivot; // 当left == right，完成一趟快速排序，此时left位相当于空，等待pivotkey补上 return left; &#125; /** * 快速排序；递归返回数组 * * @param datas */ public static int[] sortQuick(int[] datas, int left, int right) &#123; if (left &lt; right) &#123; int data = QuickPartition(datas, left, right); sortQuick(datas, left, data - 1); sortQuick(datas, data + 1, right); &#125; return datas; &#125; 1.冒泡算法，2.选择算法，3.快速算法。4.插入算法，5.希尔算法，6.堆算法 基本思想：在要排序的一组数中，选出最小的一个数与第一个位置的数交换；然后在剩下的数当中再找最小的与第二个位置的数交换，如此循环到倒数第二个数和最后一个数比较为止。 123456789101112131415161718192021222324252627282930313233343536public class AlgorithmUtil &#123; public static int temp,index = 0; /** * 临时值交换 * * @param datas * 数组 * @param i * @param j */ public static void swap(int[] datas, int i, int j) &#123; temp = datas[i]; datas[i] = datas[j]; datas[j] = temp; &#125; /** * 扩充数组长度 * * @param datas * @param value * @return */ public static int[] expandArray(int[] datas, int value) &#123; if (datas.length &lt;= index) &#123; int[] arrays = new int[datas.length * 2]; System.arraycopy(datas, 0, arrays, 0, datas.length); datas = arrays; &#125; datas[index] = value; index++; return datas; &#125; &#125;","tags":[{"name":"JAVA数据结构和算法","slug":"JAVA数据结构和算法","permalink":"http://example.com/tags/JAVA%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E5%92%8C%E7%AE%97%E6%B3%95/"}],"categories":[{"name":"数据结构和算法","slug":"数据结构和算法","permalink":"http://example.com/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E5%92%8C%E7%AE%97%E6%B3%95/"}]},{"title":"Hello hexo","date":"2022-05-12T09:35:21.839Z","path":"blog/hello-hexo/","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","tags":[],"categories":[]},{"title":"ElasticSearch-Springboot","date":"2022-01-01T16:00:00.000Z","path":"blog/工具和中间件/搜索引擎技术/ElasticSearch/ElasticSearch-Springboot/","text":"版本问题springboot 有一个 spring data 组件，可以用来连接各种数据源。 用来连接 elasticsearch 的是 spring-data-elasticsearch。 启动 elasticsearch运行elasticsearch.bat, 启动后, 可以看到左上角的版本号。 创建 springboot 项目:pom.xml各种jar包，主要是 spring-boot-starter-data-elastisearch 这个 jar包。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.howToElasticSearch&lt;/groupId&gt; &lt;artifactId&gt;springboot&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;name&gt;springboot&lt;/name&gt; &lt;description&gt;springboot&lt;/description&gt; &lt;packaging&gt;war&lt;/packaging&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;1.5.9.RELEASE&lt;/version&gt; &lt;/parent&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-tomcat&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;3.8.1&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;!-- servlet依赖. --&gt; &lt;dependency&gt; &lt;groupId&gt;javax.servlet&lt;/groupId&gt; &lt;artifactId&gt;javax.servlet-api&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;javax.servlet&lt;/groupId&gt; &lt;artifactId&gt;jstl&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- tomcat的支持.--&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.tomcat.embed&lt;/groupId&gt; &lt;artifactId&gt;tomcat-embed-jasper&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-devtools&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;!-- 这个需要为 true 热部署才有效 --&gt; &lt;/dependency&gt; &lt;!-- mysql--&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.21&lt;/version&gt; &lt;/dependency&gt; &lt;!-- elastisearch依赖包 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-elasticsearch&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;properties&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;/properties&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;/project&gt; Category.javaCategory 实体类，其中的 @Document就表明了要连接到 ElasticSearch 的哪个索引和哪个 type 上 @Document(indexName &#x3D; “howToElasticSearch”,type &#x3D; “category”)索引相当于就是数据库，type 相当于就是表 1234567891011121314151617181920212223package cn.peach.springboot.pojo; import org.springframework.data.elasticsearch.annotations.Document; @Document(indexName = &quot;howToElasticSearch&quot;,type = &quot;category&quot;)public class Category &#123; private int id; private String name; public int getId() &#123; return id; &#125; public void setId(int id) &#123; this.id = id; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; &#125; 控制类：CategoryController.java控制类提供 CRUD 一套 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889package cn.peach.springboot.web;import java.text.SimpleDateFormat;import java.util.Date;import org.elasticsearch.index.query.QueryBuilders;import org.elasticsearch.index.query.functionscore.FunctionScoreQueryBuilder;import org.elasticsearch.index.query.functionscore.ScoreFunctionBuilders;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.data.domain.Page;import org.springframework.data.domain.PageRequest;import org.springframework.data.domain.Pageable;import org.springframework.data.domain.Sort;import org.springframework.data.elasticsearch.core.query.NativeSearchQueryBuilder;import org.springframework.data.elasticsearch.core.query.SearchQuery;import org.springframework.stereotype.Controller;import org.springframework.ui.Model;import org.springframework.web.bind.annotation.GetMapping;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RequestParam;import cn.peach.springboot.dao.CategoryDAO;import cn.peach.springboot.pojo.Category; @Controllerpublic class CategoryController &#123; @Autowired CategoryDAO categoryDAO; //每页数量 @GetMapping(&quot;/listCategory&quot;) public String listCategory(Model m,@RequestParam(value = &quot;start&quot;, defaultValue = &quot;0&quot;) int start,@RequestParam(value = &quot;size&quot;, defaultValue = &quot;5&quot;) int size)&#123; String query = &quot;商品&quot;; //查询条件，但是并未使用，放在这里，为的是将来使用，方便参考，知道如何用 SearchQuery searchQuery=getEntitySearchQuery(start,size,query); Page&lt;Category&gt; page = categoryDAO.search(searchQuery); m.addAttribute(&quot;page&quot;, page); return &quot;listCategory&quot;; &#125; private SearchQuery getEntitySearchQuery(int start, int size, String searchContent) &#123; FunctionScoreQueryBuilder functionScoreQueryBuilder = QueryBuilders.functionScoreQuery() .add(QueryBuilders.matchAllQuery(), //查询所有 ScoreFunctionBuilders.weightFactorFunction(100))// 查询条件，但是并未使用，放在这里，为的是将来使用，方便参考，知道如何用// .add(QueryBuilders.matchPhraseQuery(&quot;name&quot;, searchContent),// ScoreFunctionBuilders.weightFactorFunction(100)) //设置权重分 求和模式 .scoreMode(&quot;sum&quot;) //设置权重分最低分 .setMinScore(10); // 设置分页 Sort sort = new Sort(Sort.Direction.DESC,&quot;id&quot;); Pageable pageable = new PageRequest(start, size,sort); return new NativeSearchQueryBuilder() .withPageable(pageable) .withQuery(functionScoreQueryBuilder).build(); &#125; @RequestMapping(&quot;/addCategory&quot;) public String addCategory(Category c) throws Exception &#123; int id = currentTime(); c.setId(id); categoryDAO.save(c); return &quot;redirect:listCategory&quot;; &#125; private int currentTime() &#123; SimpleDateFormat sdf = new SimpleDateFormat(&quot;MMddHHmmss&quot;); String time= sdf.format(new Date()); return Integer.parseInt(time); &#125; @RequestMapping(&quot;/deleteCategory&quot;) public String deleteCategory(Category c) throws Exception &#123; categoryDAO.delete(c); return &quot;redirect:listCategory&quot;; &#125; @RequestMapping(&quot;/updateCategory&quot;) public String updateCategory(Category c) throws Exception &#123; categoryDAO.save(c); return &quot;redirect:listCategory&quot;; &#125; @RequestMapping(&quot;/editCategory&quot;) public String ediitCategory(int id,Model m) throws Exception &#123; Category c= categoryDAO.findOne(id); m.addAttribute(&quot;c&quot;, c); return &quot;editCategory&quot;; &#125;&#125; 配置文件：application.properties配置 jsp 作为视图配置spring端口 为8080配置 elastic链接地址为 127.0.0.1:9300 1234spring.mvc.view.prefix=/WEB-INF/jsp/spring.mvc.view.suffix=.jspserver.port=8080spring.data.elasticsearch.cluster-nodes = 127.0.0.1:9300 启动并测试:运行 Application.java 启动项目, 接着访问地址： 1http://127.0.0.1:8080/listCategory kibana查看数据启动 kibana 并访问: http://127.0.0.1:5601, 选择索引刚开始是没有选定索引的，所以要自己指定索引。 把默认勾选的 Index contians time-based evens 去掉 输入 howToElasticSearch 点击 Create 按钮 查看数据然后点击上面的Discover，就可以看到左边是当前的索引 :howToElasticSearch. 右边就是数据了。","tags":[{"name":"ElasticSearch","slug":"ElasticSearch","permalink":"http://example.com/tags/ElasticSearch/"}],"categories":[{"name":"工具和中间件","slug":"工具和中间件","permalink":"http://example.com/categories/%E5%B7%A5%E5%85%B7%E5%92%8C%E4%B8%AD%E9%97%B4%E4%BB%B6/"},{"name":"搜索引擎技术","slug":"工具和中间件/搜索引擎技术","permalink":"http://example.com/categories/%E5%B7%A5%E5%85%B7%E5%92%8C%E4%B8%AD%E9%97%B4%E4%BB%B6/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E6%8A%80%E6%9C%AF/"},{"name":"ElasticSearch","slug":"工具和中间件/搜索引擎技术/ElasticSearch","permalink":"http://example.com/categories/%E5%B7%A5%E5%85%B7%E5%92%8C%E4%B8%AD%E9%97%B4%E4%BB%B6/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E6%8A%80%E6%9C%AF/ElasticSearch/"}]},{"title":"ElasticSearch实战","date":"2022-01-01T16:00:00.000Z","path":"blog/工具和中间件/搜索引擎技术/ElasticSearch/ElasticSearch实战/","text":"引入pom依赖：12345&lt;dependency&gt; &lt;groupId&gt;org.elasticsearch.client&lt;/groupId&gt; &lt;artifactId&gt;elasticsearch-rest-high-level-client&lt;/artifactId&gt; &lt;version&gt;7.6.1&lt;/version&gt;&lt;/dependency&gt; 使用Java API来操作ES集群初始化连接，基于 RestClient.builder 方法来构建 RestClientBuilder, 使用 RestHighLevelClient 去连接ES集群，用 HttpHost 来添加ES的节点。 123456789// 建立与ES的连接// 1. 使用RestHighLevelClient构建客户端连接。// 2. 基于RestClient.builder方法来构建RestClientBuilder// 3. 用HttpHost来添加ES的节点RestClientBuilder restClientBuilder = RestClient.builder( new HttpHost(&quot;192.168.21.130&quot;, 9200, &quot;http&quot;) , new HttpHost(&quot;192.168.21.131&quot;, 9200, &quot;http&quot;) , new HttpHost(&quot;192.168.21.132&quot;, 9200, &quot;http&quot;));RestHighLevelClient restHighLevelClient = new RestHighLevelClient(restClientBuilder); 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155public void add(JobDetail jobDetail) throws IOException &#123; // 构建IndexRequest对象，用来描述ES发起请求的数据 IndexRequest indexRequest = new IndexRequest(JOB_IDX); // 设置文档ID indexRequest.id(String.valueOf(jobDetail.getId())); // 使用FastJSON将实体类对象转换为JSON String json = JSONObject.toJSONString(jobDetail); // 使用IndexRequest.source方法设置文档数据，并设置请求的数据为JSON格式 indexRequest.source(json, XContentType.JSON); // 使用ES RestHighLevelClient调用index方法发起请求，将一个文档添加到索引中 restHighLevelClient.index(indexRequest, RequestOptions.DEFAULT);&#125;public JobDetail findById(long id) throws IOException &#123; GetRequest getRequest = new GetRequest(JOB_IDX, id + &quot;&quot;); // 构建GetRequest请求 // 使用RestHighLevelClient.get发送GetRequest请求，并获取到ES服务器的响应。 GetResponse getResponse = restHighLevelClient.get(getRequest, RequestOptions.DEFAULT); String json = getResponse.getSourceAsString();// 将ES响应的数据转换为JSON字符串 // 并使用FastJSON将JSON字符串转换为JobDetail类对象 JobDetail jobDetail = JSONObject.parseObject(json, JobDetail.class); jobDetail.setId(id);// 单独设置ID return jobDetail;&#125;public void update(JobDetail jobDetail) throws IOException &#123; // 判断对应ID的文档是否存在，构建GetRequest GetRequest getRequest = new GetRequest(JOB_IDX, jobDetail.getId() + &quot;&quot;); // 执行client的exists方法，发起请求，判断是否存在 boolean exists = restHighLevelClient.exists(getRequest, RequestOptions.DEFAULT); if(exists) &#123; // 构建UpdateRequest请求 UpdateRequest updateRequest = new UpdateRequest(JOB_IDX, jobDetail.getId() + &quot;&quot;); // 设置UpdateRequest的文档，并配置为JSON格式 updateRequest.doc(JSONObject.toJSONString(jobDetail), XContentType.JSON); // 执行client发起update请求 restHighLevelClient.update(updateRequest, RequestOptions.DEFAULT); &#125;&#125;public void deleteById(long id) throws IOException &#123; DeleteRequest deleteRequest = new DeleteRequest(JOB_IDX, id + &quot;&quot;);// 构建delete请求 restHighLevelClient.delete(deleteRequest, RequestOptions.DEFAULT);// 使用RestHighLevelClient执行delete请求&#125;public List&lt;JobDetail&gt; searchByKeywords(String keywords) throws IOException &#123; // 构建SearchRequest检索请求 专门用来进行全文检索、关键字检索的API SearchRequest searchRequest = new SearchRequest(JOB_IDX); // 创建一个SearchSourceBuilder专门用于构建查询条件 SearchSourceBuilder searchSourceBuilder = new SearchSourceBuilder(); // 使用QueryBuilders.multiMatchQuery构建一个查询条件（搜索title、jd），并配置到SearchSourceBuilder MultiMatchQueryBuilder multiMatchQueryBuilder = QueryBuilders.multiMatchQuery(keywords, &quot;title&quot;, &quot;jd&quot;); searchSourceBuilder.query(multiMatchQueryBuilder);// 将查询条件设置到查询请求构建器中 searchRequest.source(searchSourceBuilder);// 调用SearchRequest.source将查询条件设置到检索请求 // 执行RestHighLevelClient.search发起请求 SearchResponse searchResponse = restHighLevelClient.search(searchRequest, RequestOptions.DEFAULT); SearchHit[] hitArray = searchResponse.getHits().getHits(); ArrayList&lt;JobDetail&gt; jobDetailArrayList = new ArrayList&lt;&gt;(); for (SearchHit documentFields : hitArray) &#123;// 遍历结果 String json = documentFields.getSourceAsString();// 获取命中的结果 JobDetail jobDetail = JSONObject.parseObject(json, JobDetail.class);// 将JSON字符串转换为对象 jobDetail.setId(Long.parseLong(documentFields.getId()));// 使用SearchHit.getId设置文档ID jobDetailArrayList.add(jobDetail); &#125; return jobDetailArrayList;&#125;public Map&lt;String, Object&gt; searchByPage(String keywords, int pageNum, int pageSize) throws IOException &#123; // 构建SearchRequest检索请求 专门用来进行全文检索、关键字检索的API SearchRequest searchRequest = new SearchRequest(JOB_IDX); // 创建一个SearchSourceBuilder专门用于构建查询条件 SearchSourceBuilder searchSourceBuilder = new SearchSourceBuilder(); // 使用QueryBuilders.multiMatchQuery构建一个查询条件（搜索title、jd），并配置到SearchSourceBuilder MultiMatchQueryBuilder multiMatchQueryBuilder = QueryBuilders.multiMatchQuery(keywords, &quot;title&quot;, &quot;jd&quot;); searchSourceBuilder.query(multiMatchQueryBuilder); // 将查询条件设置到查询请求构建器中 searchSourceBuilder.size(pageSize);// 每页显示多少条 searchSourceBuilder.from((pageNum - 1) * pageSize);// 设置从第几条开始查询 searchRequest.source(searchSourceBuilder);// 调用SearchRequest.source将查询条件设置到检索请求 // 执行RestHighLevelClient.search发起请求 SearchResponse searchResponse = restHighLevelClient.search(searchRequest, RequestOptions.DEFAULT); SearchHit[] hitArray = searchResponse.getHits().getHits(); ArrayList&lt;JobDetail&gt; jobDetailArrayList = new ArrayList&lt;&gt;(); for (SearchHit documentFields : hitArray) &#123;// 遍历结果 String json = documentFields.getSourceAsString();// 获取命中的结果 JobDetail jobDetail = JSONObject.parseObject(json, JobDetail.class);// 将JSON字符串转换为对象 jobDetail.setId(Long.parseLong(documentFields.getId()));// 使用SearchHit.getId设置文档ID jobDetailArrayList.add(jobDetail); &#125; // 将结果封装到Map结构中（带有分页信息） long totalNum = searchResponse.getHits().getTotalHits().value; Map&lt;String, Object&gt; resultMap = new HashMap&lt;&gt;(); resultMap.put(&quot;total&quot;, totalNum); // total -&gt; 使用SearchHits.getTotalHits().value获取到所有的记录数 resultMap.put(&quot;content&quot;, jobDetailArrayList); content -&gt; 当前分页中的数据 return resultMap;&#125;public Map&lt;String, Object&gt; searchByScrollPage(String keywords, String scrollId, int pageSize) throws IOException &#123; SearchResponse searchResponse = null; if(scrollId == null) &#123; // 构建SearchRequest检索请求 专门用来进行全文检索、关键字检索的API SearchRequest searchRequest = new SearchRequest(JOB_IDX); // 创建一个SearchSourceBuilder专门用于构建查询条件 SearchSourceBuilder searchSourceBuilder = new SearchSourceBuilder(); // 使用QueryBuilders.multiMatchQuery构建一个查询条件（搜索title、jd），并配置到SearchSourceBuilder MultiMatchQueryBuilder multiMatchQueryBuilder = QueryBuilders.multiMatchQuery(keywords, &quot;title&quot;, &quot;jd&quot;); searchSourceBuilder.query(multiMatchQueryBuilder);// 将查询条件设置到查询请求构建器中 HighlightBuilder highlightBuilder = new HighlightBuilder(); // 设置高亮 highlightBuilder.field(&quot;title&quot;); highlightBuilder.field(&quot;jd&quot;); highlightBuilder.preTags(&quot;&lt;font color=&#x27;red&#x27;&gt;&quot;); highlightBuilder.postTags(&quot;&lt;/font&gt;&quot;); searchSourceBuilder.highlighter(highlightBuilder); // 给请求设置高亮 searchSourceBuilder.size(pageSize); // 每页显示多少条 searchRequest.source(searchSourceBuilder); // 调用SearchRequest.source将查询条件设置到检索请求 searchRequest.scroll(TimeValue.timeValueMinutes(5)); // 设置scroll查询 // 执行RestHighLevelClient.search发起请求 searchResponse = restHighLevelClient.search(searchRequest, RequestOptions.DEFAULT); &#125; else &#123; // 第二次查询的时候，直接通过scroll id查询数据 SearchScrollRequest searchScrollRequest = new SearchScrollRequest(scrollId); searchScrollRequest.scroll(TimeValue.timeValueMinutes(5)); // 使用RestHighLevelClient发送scroll请求 searchResponse = restHighLevelClient.scroll(searchScrollRequest, RequestOptions.DEFAULT); &#125; SearchHit[] hitArray = searchResponse.getHits().getHits(); ArrayList&lt;JobDetail&gt; jobDetailArrayList = new ArrayList&lt;&gt;(); for (SearchHit documentFields : hitArray) &#123; // 遍历结果，迭代ES响应的数据 String json = documentFields.getSourceAsString(); // 获取命中的结果 JobDetail jobDetail = JSONObject.parseObject(json, JobDetail.class); // 将JSON字符串转换为对象 jobDetail.setId(Long.parseLong(documentFields.getId())); // 使用SearchHit.getId设置文档ID jobDetailArrayList.add(jobDetail); // 设置高亮的一些文本到实体类中 封装了高亮 Map&lt;String, HighlightField&gt; highlightFieldMap = documentFields.getHighlightFields(); HighlightField titleHL = highlightFieldMap.get(&quot;title&quot;); HighlightField jdHL = highlightFieldMap.get(&quot;jd&quot;); if(titleHL != null) &#123; Text[] fragments = titleHL.getFragments(); // 获取指定字段的高亮片段 StringBuilder builder = new StringBuilder(); for(Text text : fragments) &#123; // 将这些高亮片段拼接成一个完整的高亮字段 builder.append(text); &#125; jobDetail.setTitle(builder.toString()); // 设置到实体类中 &#125; if(jdHL != null) &#123; Text[] fragments = jdHL.getFragments(); // 获取指定字段的高亮片段 StringBuilder builder = new StringBuilder(); for(Text text : fragments) &#123;// 将这些高亮片段拼接成一个完整的高亮字段 builder.append(text); &#125; jobDetail.setJd(builder.toString()); // 设置到实体类中 &#125; &#125; // 将结果封装到Map结构中，带有分页信息 long totalNum = searchResponse.getHits().getTotalHits().value; Map&lt;String, Object&gt; hashMap = new HashMap&lt;&gt;(); hashMap.put(&quot;scroll_id&quot;, searchResponse.getScrollId()); hashMap.put(&quot;content&quot;, jobDetailArrayList); // content -&gt; 当前分页中的数据 hashMap.put(&quot;total_num&quot;, totalNum); // total -&gt; 使用SearchHits.getTotalHits().value获取到所有的记录数 return hashMap;&#125;public void close() throws IOException &#123; restHighLevelClient.close();&#125; 京东商城搜索效果实现ES索引库表结构分析 1234567891011121314151617181920212223242526272829303132333435363738PUT product_db // 创建索引库&#123;&quot;mappings&quot;:&#123;&quot;properties&quot;:&#123;&quot;id&quot;:&#123;&quot;type&quot;:&quot;long&quot;&#125;,&quot;name&quot;:&#123;&quot;type&quot;:&quot;text&quot;,&quot;analyzer&quot;:&quot;ik_max_word&quot;&#125;,&quot;keywords&quot;:&#123;&quot;type&quot;:&quot;text&quot;,&quot;analyzer&quot;:&quot;ik_max_word&quot;&#125;,&quot;subTitle&quot;:&#123;&quot;type&quot;:&quot;text&quot;,&quot;analyzer&quot;:&quot;ik_max_word&quot;&#125;,&quot;salecount&quot;:&#123;&quot;type&quot;:&quot;long&quot;&#125;,&quot;putawayDate&quot;:&#123;&quot;type&quot;:&quot;date&quot;&#125;,&quot;price&quot;:&#123;&quot;type&quot;:&quot;double&quot;&#125;,&quot;promotionPrice&quot;:&#123;&quot;type&quot;:&quot;keyword&quot;&#125;,&quot;originalPrice&quot;:&#123;&quot;type&quot;:&quot;keyword&quot;&#125;,&quot;pic&quot;:&#123;&quot;type&quot;:&quot;keyword&quot;&#125;,&quot;sale&quot;:&#123;&quot;type&quot;:&quot;long&quot;&#125;,&quot;hasStock&quot;:&#123;&quot;type&quot;:&quot;boolean&quot;&#125;,&quot;brandId&quot;:&#123;&quot;type&quot;:&quot;long&quot;&#125;,&quot;brandName&quot;:&#123;&quot;type&quot;:&quot;keyword&quot;&#125;,&quot;brandImg&quot;:&#123;&quot;type&quot;:&quot;keyword&quot;&#125;,&quot;categoryId&quot;:&#123;&quot;type&quot;:&quot;long&quot;&#125;,&quot;categoryName&quot;:&#123;&quot;type&quot;:&quot;keyword&quot;&#125;,&quot;attrs&quot;:&#123;&quot;type&quot;:&quot;nested&quot;,&quot;properties&quot;:&#123;&quot;attrId&quot;:&#123;&quot;type&quot;:&quot;long&quot;&#125;,&quot;attrName&quot;:&#123;&quot;type&quot;:&quot;keyword&quot;&#125;,&quot;attrValue&quot;:&#123;&quot;type&quot;:&quot;keyword&quot;&#125;&#125;&#125;&#125;&#125;&#125;// 索引数据准备PUT /product_db/_doc/1&#123;&quot;id&quot;:&quot;26&quot;,&quot;name&quot;:&quot;小米 11 手机&quot;,&quot;keywords&quot;:&quot;小米手机&quot;,&quot;subTitle&quot;:&quot;AI智慧全面屏 6GB +64GB 亮黑色 全网通版 移动联通电信4G手机 双卡双待 双卡双待&quot;,&quot;price&quot;:&quot;3999&quot;,&quot;promotionPrice&quot;:&quot;2999&quot;,&quot;originalPrice&quot;:&quot;5999&quot;,&quot;pic&quot;:&quot;http://macro-oss.oss-cn-shenzhen.aliyuncs.com/mall/images/20180615/xiaomi.jpg&quot;,&quot;sale&quot;:999,&quot;hasStock&quot;:true,&quot;salecount&quot;:999,&quot;putawayDate&quot;:&quot;2021-04-01&quot;,&quot;brandId&quot;:6,&quot;brandName&quot;:&quot;小米&quot;,&quot;brandImg&quot;:&quot;http://macro-oss.oss-cn-shenzhen.aliyuncs.com/mall/images/20190129/1e34aef2a409119018a4c6258e39ecfb_222_222.png&quot;,&quot;categoryId&quot;:19,&quot;categoryName&quot;:&quot;手机通讯&quot;,&quot;attrs&quot;:[&#123;&quot;attrId&quot;:1,&quot;attrName&quot;:&quot;cpu&quot;,&quot;attrValue&quot;:&quot;2核&quot;&#125;,&#123;&quot;attrId&quot;:2,&quot;attrName&quot;:&quot;颜色&quot;,&quot;attrValue&quot;:&quot;黑色&quot;&#125;]&#125;PUT /product_db/_doc/2&#123;&quot;id&quot;:&quot;27&quot;,&quot;name&quot;:&quot;小米 10 手机&quot;,&quot;keywords&quot;:&quot;小米手机&quot;,&quot;subTitle&quot;:&quot;AI智慧全面屏 4GB +64GB 亮白色 全网通版 移动联通电信4G手机 双卡双待 双卡双待&quot;,&quot;price&quot;:&quot;2999&quot;,&quot;promotionPrice&quot;:&quot;1999&quot;,&quot;originalPrice&quot;:&quot;3999&quot;,&quot;pic&quot;:&quot;http://macro-oss.oss-cn-shenzhen.aliyuncs.com/mall/images/20180615/xiaomi.jpg&quot;,&quot;sale&quot;:999,&quot;hasStock&quot;:false,&quot;salecount&quot;:99,&quot;putawayDate&quot;:&quot;2021-04-02&quot;,&quot;brandId&quot;:6,&quot;brandName&quot;:&quot;小米&quot;,&quot;brandImg&quot;:&quot;http://macro-oss.oss-cn-shenzhen.aliyuncs.com/mall/images/20190129/1e34aef2a409119018a4c6258e39ecfb_222_222.png&quot;,&quot;categoryId&quot;:19,&quot;categoryName&quot;:&quot;手机通讯&quot;,&quot;attrs&quot;:[&#123;&quot;attrId&quot;:1,&quot;attrName&quot;:&quot;cpu&quot;,&quot;attrValue&quot;:&quot;4核&quot;&#125;,&#123;&quot;attrId&quot;:2,&quot;attrName&quot;:&quot;颜色&quot;,&quot;attrValue&quot;:&quot;白色&quot;&#125;]&#125;PUT /product_db/_doc/3&#123;&quot;id&quot;:&quot;28&quot;,&quot;name&quot;:&quot;小米 手机&quot;,&quot;keywords&quot;:&quot;小米手机&quot;,&quot;subTitle&quot;:&quot;AI智慧全面屏 4GB +64GB 亮蓝色 全网通版 移动联通电信4G手机 双卡双待 双卡双待&quot;,&quot;price&quot;:&quot;2999&quot;,&quot;promotionPrice&quot;:&quot;1999&quot;,&quot;originalPrice&quot;:&quot;3999&quot;,&quot;pic&quot;:&quot;http://macro-oss.oss-cn-shenzhen.aliyuncs.com/mall/images/20180615/xiaomi.jpg&quot;,&quot;sale&quot;:999,&quot;hasStock&quot;:true,&quot;salecount&quot;:199,&quot;putawayDate&quot;:&quot;2021-04-03&quot;,&quot;brandId&quot;:6,&quot;brandName&quot;:&quot;小米&quot;,&quot;brandImg&quot;:&quot;http://macro-oss.oss-cn-shenzhen.aliyuncs.com/mall/images/20190129/1e34aef2a409119018a4c6258e39ecfb_222_222.png&quot;,&quot;categoryId&quot;:19,&quot;categoryName&quot;:&quot;手机通讯&quot;,&quot;attrs&quot;:[&#123;&quot;attrId&quot;:1,&quot;attrName&quot;:&quot;cpu&quot;,&quot;attrValue&quot;:&quot;2核&quot;&#125;,&#123;&quot;attrId&quot;:2,&quot;attrName&quot;:&quot;颜色&quot;,&quot;attrValue&quot;:&quot;蓝色&quot;&#125;]&#125;PUT /product_db/_doc/4&#123;&quot;id&quot;:&quot;29&quot;,&quot;name&quot;:&quot;Apple iPhone 8 Plus 64GB 金色特别版 移动联通电信4G手机&quot;,&quot;keywords&quot;:&quot;苹果手机&quot;,&quot;subTitle&quot;:&quot;苹果手机 Apple产品年中狂欢节，好物尽享，美在智慧！速来 &gt;&gt; 勾选[保障服务][原厂保2年]，获得AppleCare+全方位服务计划，原厂延保售后无忧。&quot;,&quot;price&quot;:&quot;5999&quot;,&quot;promotionPrice&quot;:&quot;4999&quot;,&quot;originalPrice&quot;:&quot;7999&quot;,&quot;pic&quot;:&quot;http://macro-oss.oss-cn-shenzhen.aliyuncs.com/mall/images/20180615/5acc5248N6a5f81cd.jpg&quot;,&quot;sale&quot;:999,&quot;hasStock&quot;:true,&quot;salecount&quot;:1199,&quot;putawayDate&quot;:&quot;2021-04-04&quot;,&quot;brandId&quot;:51,&quot;brandName&quot;:&quot;苹果&quot;,&quot;brandImg&quot;:&quot;http://macro-oss.oss-cn-shenzhen.aliyuncs.com/mall/images/20180607/timg.jpg&quot;,&quot;categoryId&quot;:19,&quot;categoryName&quot;:&quot;手机通讯&quot;,&quot;attrs&quot;:[&#123;&quot;attrId&quot;:1,&quot;attrName&quot;:&quot;cpu&quot;,&quot;attrValue&quot;:&quot;4核&quot;&#125;,&#123;&quot;attrId&quot;:2,&quot;attrName&quot;:&quot;颜色&quot;,&quot;attrValue&quot;:&quot;金色&quot;&#125;]&#125;PUT /product_db/_doc/5&#123;&quot;id&quot;:&quot;30&quot;,&quot;name&quot;:&quot;HLA海澜之家简约动物印花短袖T恤&quot;,&quot;keywords&quot;:&quot;海澜之家衣服&quot;,&quot;subTitle&quot;:&quot;HLA海澜之家短袖T恤&quot;,&quot;price&quot;:&quot;199&quot;,&quot;promotionPrice&quot;:&quot;99&quot;,&quot;originalPrice&quot;:&quot;299&quot;,&quot;pic&quot;:&quot;http://macro-oss.oss-cn-shenzhen.aliyuncs.com/mall/images/20180615/5ad83a4fN6ff67ecd.jpg!cc_350x449.jpg&quot;,&quot;sale&quot;:999,&quot;hasStock&quot;:true,&quot;salecount&quot;:19,&quot;putawayDate&quot;:&quot;2021-04-05&quot;,&quot;brandId&quot;:50,&quot;brandName&quot;:&quot;海澜之家&quot;,&quot;brandImg&quot;:&quot;http://macro-oss.oss-cn-shenzhen.aliyuncs.com/mall/images/20190129/99d3279f1029d32b929343b09d3c72de_222_222.jpg&quot;,&quot;categoryId&quot;:8,&quot;categoryName&quot;:&quot;T恤&quot;,&quot;attrs&quot;:[&#123;&quot;attrId&quot;:3,&quot;attrName&quot;:&quot;尺寸&quot;,&quot;attrValue&quot;:&quot;M&quot;&#125;,&#123;&quot;attrId&quot;:4,&quot;attrName&quot;:&quot;颜色&quot;,&quot;attrValue&quot;:&quot;黑色&quot;&#125;]&#125;PUT /product_db/_doc/6&#123;&quot;id&quot;:&quot;31&quot;,&quot;name&quot;:&quot;HLA海澜之家蓝灰花纹圆领针织布短袖T恤&quot;,&quot;keywords&quot;:&quot;海澜之家衣服&quot;,&quot;subTitle&quot;:&quot;HLA海澜之家短袖T恤&quot;,&quot;price&quot;:&quot;299&quot;,&quot;promotionPrice&quot;:&quot;199&quot;,&quot;originalPrice&quot;:&quot;299&quot;,&quot;pic&quot;:&quot;http://macro-oss.oss-cn-shenzhen.aliyuncs.com/mall/images/20180615/5ac98b64N70acd82f.jpg!cc_350x449.jpg&quot;,&quot;sale&quot;:999,&quot;hasStock&quot;:true,&quot;salecount&quot;:399,&quot;putawayDate&quot;:&quot;2021-04-06&quot;,&quot;brandId&quot;:50,&quot;brandName&quot;:&quot;海澜之家&quot;,&quot;brandImg&quot;:&quot;http://macro-oss.oss-cn-shenzhen.aliyuncs.com/mall/images/20190129/99d3279f1029d32b929343b09d3c72de_222_222.jpg&quot;,&quot;categoryId&quot;:8,&quot;categoryName&quot;:&quot;T恤&quot;,&quot;attrs&quot;:[&#123;&quot;attrId&quot;:3,&quot;attrName&quot;:&quot;尺寸&quot;,&quot;attrValue&quot;:&quot;X&quot;&#125;,&#123;&quot;attrId&quot;:4,&quot;attrName&quot;:&quot;颜色&quot;,&quot;attrValue&quot;:&quot;蓝灰&quot;&#125;]&#125;PUT /product_db/_doc/7&#123;&quot;id&quot;:&quot;32&quot;,&quot;name&quot;:&quot;HLA海澜之家短袖T恤男基础款&quot;,&quot;keywords&quot;:&quot;海澜之家衣服&quot;,&quot;subTitle&quot;:&quot;HLA海澜之家短袖T恤&quot;,&quot;price&quot;:&quot;269&quot;,&quot;promotionPrice&quot;:&quot;169&quot;,&quot;originalPrice&quot;:&quot;399&quot;,&quot;pic&quot;:&quot;http://macro-oss.oss-cn-shenzhen.aliyuncs.com/mall/images/20180615/5a51eb88Na4797877.jpg&quot;,&quot;sale&quot;:999,&quot;hasStock&quot;:true,&quot;salecount&quot;:399,&quot;putawayDate&quot;:&quot;2021-04-07&quot;,&quot;brandId&quot;:50,&quot;brandName&quot;:&quot;海澜之家&quot;,&quot;brandImg&quot;:&quot;http://macro-oss.oss-cn-shenzhen.aliyuncs.com/mall/images/20190129/99d3279f1029d32b929343b09d3c72de_222_222.jpg&quot;,&quot;categoryId&quot;:8,&quot;categoryName&quot;:&quot;T恤&quot;,&quot;attrs&quot;:[&#123;&quot;attrId&quot;:3,&quot;attrName&quot;:&quot;尺寸&quot;,&quot;attrValue&quot;:&quot;L&quot;&#125;,&#123;&quot;attrId&quot;:4,&quot;attrName&quot;:&quot;颜色&quot;,&quot;attrValue&quot;:&quot;蓝色&quot;&#125;]&#125;PUT /product_db/_doc/8&#123;&quot;id&quot;:&quot;33&quot;,&quot;name&quot;:&quot;小米（MI）小米电视4A &quot;,&quot;keywords&quot;:&quot;小米电视机家用电器&quot;,&quot;subTitle&quot;:&quot;小米（MI）小米电视4A 55英寸 L55M5-AZ/L55M5-AD 2GB+8GB HDR 4K超高清 人工智能网络液晶平板电视&quot;,&quot;price&quot;:&quot;2269&quot;,&quot;promotionPrice&quot;:&quot;2169&quot;,&quot;originalPrice&quot;:&quot;2399&quot;,&quot;pic&quot;:&quot;http://macro-oss.oss-cn-shenzhen.aliyuncs.com/mall/images/20180615/5b02804dN66004d73.jpg&quot;,&quot;sale&quot;:999,&quot;hasStock&quot;:true,&quot;salecount&quot;:132,&quot;putawayDate&quot;:&quot;2021-04-09&quot;,&quot;brandId&quot;:6,&quot;brandName&quot;:&quot;小米&quot;,&quot;brandImg&quot;:&quot;http://macro-oss.oss-cn-shenzhen.aliyuncs.com/mall/images/20190129/1e34aef2a409119018a4c6258e39ecfb_222_222.png&quot;,&quot;categoryId&quot;:35,&quot;categoryName&quot;:&quot;手机数码&quot;,&quot;attrs&quot;:[&#123;&quot;attrId&quot;:5,&quot;attrName&quot;:&quot;屏幕尺寸&quot;,&quot;attrValue&quot;:&quot;52&quot;&#125;,&#123;&quot;attrId&quot;:6,&quot;attrName&quot;:&quot;机身颜色&quot;,&quot;attrValue&quot;:&quot;黑色&quot;&#125;]&#125;PUT /product_db/_doc/9&#123;&quot;id&quot;:&quot;34&quot;,&quot;name&quot;:&quot;小米（MI）小米电视4A 65英寸&quot;,&quot;keywords&quot;:&quot;小米电视机家用电器&quot;,&quot;subTitle&quot;:&quot;小米（MI）小米电视4A 65英寸 L55M5-AZ/L55M5-AD 2GB+8GB HDR 4K超高清 人工智能网络液晶平板电视&quot;,&quot;price&quot;:&quot;3269&quot;,&quot;promotionPrice&quot;:&quot;3169&quot;,&quot;originalPrice&quot;:&quot;3399&quot;,&quot;pic&quot;:&quot;http://macro-oss.oss-cn-shenzhen.aliyuncs.com/mall/images/20180615/5b028530N51eee7d4.jpg&quot;,&quot;sale&quot;:999,&quot;hasStock&quot;:true,&quot;salecount&quot;:999,&quot;putawayDate&quot;:&quot;2021-04-10&quot;,&quot;brandId&quot;:6,&quot;brandName&quot;:&quot;小米&quot;,&quot;brandImg&quot;:&quot;http://macro-oss.oss-cn-shenzhen.aliyuncs.com/mall/images/20190129/1e34aef2a409119018a4c6258e39ecfb_222_222.png&quot;,&quot;categoryId&quot;:35,&quot;categoryName&quot;:&quot;手机数码&quot;,&quot;attrs&quot;:[&#123;&quot;attrId&quot;:5,&quot;attrName&quot;:&quot;屏幕尺寸&quot;,&quot;attrValue&quot;:&quot;65&quot;&#125;,&#123;&quot;attrId&quot;:6,&quot;attrName&quot;:&quot;机身颜色&quot;,&quot;attrValue&quot;:&quot;金色&quot;&#125;]&#125;PUT /product_db/_doc/10&#123;&quot;id&quot;:&quot;35&quot;,&quot;name&quot;:&quot;耐克NIKE 男子 休闲鞋 ROSHE RUN 运动鞋 511881-010黑色41码&quot;,&quot;keywords&quot;:&quot;耐克运动鞋 鞋子&quot;,&quot;subTitle&quot;:&quot;耐克NIKE 男子 休闲鞋 ROSHE RUN 运动鞋 511881-010黑色41码&quot;,&quot;price&quot;:&quot;569&quot;,&quot;promotionPrice&quot;:&quot;369&quot;,&quot;originalPrice&quot;:&quot;899&quot;,&quot;pic&quot;:&quot;http://macro-oss.oss-cn-shenzhen.aliyuncs.com/mall/images/20180615/5b235bb9Nf606460b.jpg&quot;,&quot;sale&quot;:999,&quot;hasStock&quot;:true,&quot;salecount&quot;:399,&quot;putawayDate&quot;:&quot;2021-04-11&quot;,&quot;brandId&quot;:58,&quot;brandName&quot;:&quot;NIKE&quot;,&quot;brandImg&quot;:&quot;http://macro-oss.oss-cn-shenzhen.aliyuncs.com/mall/images/20180615/timg (51).jpg&quot;,&quot;categoryId&quot;:29,&quot;categoryName&quot;:&quot;男鞋&quot;,&quot;attrs&quot;:[&#123;&quot;attrId&quot;:7,&quot;attrName&quot;:&quot;尺码&quot;,&quot;attrValue&quot;:&quot;42&quot;&#125;,&#123;&quot;attrId&quot;:8,&quot;attrName&quot;:&quot;颜色&quot;,&quot;attrValue&quot;:&quot;黑色&quot;&#125;]&#125;PUT /product_db/_doc/11&#123;&quot;id&quot;:&quot;36&quot;,&quot;name&quot;:&quot;耐克NIKE 男子 气垫 休闲鞋 AIR MAX 90 ESSENTIAL 运动鞋 AJ1285-101白色41码&quot;,&quot;keywords&quot;:&quot;耐克运动鞋 鞋子&quot;,&quot;subTitle&quot;:&quot;AIR MAX 90 ESSENTIAL 运动鞋 AJ1285-101白色&quot;,&quot;price&quot;:&quot;769&quot;,&quot;promotionPrice&quot;:&quot;469&quot;,&quot;originalPrice&quot;:&quot;999&quot;,&quot;pic&quot;:&quot;http://macro-oss.oss-cn-shenzhen.aliyuncs.com/mall/images/20180615/5b19403eN9f0b3cb8.jpg&quot;,&quot;sale&quot;:999,&quot;hasStock&quot;:true,&quot;salecount&quot;:499,&quot;putawayDate&quot;:&quot;2021-04-13&quot;,&quot;brandId&quot;:58,&quot;brandName&quot;:&quot;NIKE&quot;,&quot;brandImg&quot;:&quot;http://macro-oss.oss-cn-shenzhen.aliyuncs.com/mall/images/20180615/timg (51).jpg&quot;,&quot;categoryId&quot;:29,&quot;categoryName&quot;:&quot;男鞋&quot;,&quot;attrs&quot;:[&#123;&quot;attrId&quot;:7,&quot;attrName&quot;:&quot;尺码&quot;,&quot;attrValue&quot;:&quot;44&quot;&#125;,&#123;&quot;attrId&quot;:8,&quot;attrName&quot;:&quot;颜色&quot;,&quot;attrValue&quot;:&quot;白色&quot;&#125;]&#125;PUT /product_db/_doc/12&#123;&quot;id&quot;:&quot;37&quot;,&quot;name&quot;:&quot;(华为)HUAWEI MateBook X Pro 2019款 13.9英寸3K触控全面屏 轻薄笔记本&quot;,&quot;keywords&quot;:&quot;轻薄笔记本华为 笔记本电脑&quot;,&quot;subTitle&quot;:&quot;轻薄华为笔记本 电脑&quot;,&quot;price&quot;:&quot;4769&quot;,&quot;promotionPrice&quot;:&quot;4469&quot;,&quot;originalPrice&quot;:&quot;4999&quot;,&quot;pic&quot;:&quot;http://tuling-mall.oss-cn-shenzhen.aliyuncs.com/tulingmall/images/20200317/800_800_1555752016264mp.png&quot;,&quot;sale&quot;:999,&quot;hasStock&quot;:true,&quot;salecount&quot;:699,&quot;putawayDate&quot;:&quot;2021-04-14&quot;,&quot;brandId&quot;:3,&quot;brandName&quot;:&quot;华为&quot;,&quot;brandImg&quot;:&quot;http://macro-oss.oss-cn-shenzhen.aliyuncs.com/mall/images/20190129/17f2dd9756d9d333bee8e60ce8c03e4c_222_222.jpg&quot;,&quot;categoryId&quot;:19,&quot;categoryName&quot;:&quot;手机通讯&quot;,&quot;attrs&quot;:[&#123;&quot;attrId&quot;:9,&quot;attrName&quot;:&quot;容量&quot;,&quot;attrValue&quot;:&quot;16G&quot;&#125;,&#123;&quot;attrId&quot;:10,&quot;attrName&quot;:&quot;网络&quot;,&quot;attrValue&quot;:&quot;4G&quot;&#125;]&#125;PUT /product_db/_doc/13&#123;&quot;id&quot;:&quot;38&quot;,&quot;name&quot;:&quot;华为nova6se 手机 绮境森林 全网通（8G+128G)&quot;,&quot;keywords&quot;:&quot;轻薄笔记本华为 手机&quot;,&quot;subTitle&quot;:&quot;华为nova6se 手机&quot;,&quot;price&quot;:&quot;6769&quot;,&quot;promotionPrice&quot;:&quot;6469&quot;,&quot;originalPrice&quot;:&quot;6999&quot;,&quot;pic&quot;:&quot;http://macro-oss.oss-cn-shenzhen.aliyuncs.com/mall/images/20180607/5ac1bf58Ndefaac16.jpg&quot;,&quot;sale&quot;:999,&quot;hasStock&quot;:true,&quot;salecount&quot;:899,&quot;putawayDate&quot;:&quot;2021-04-15&quot;,&quot;brandId&quot;:3,&quot;brandName&quot;:&quot;华为&quot;,&quot;brandImg&quot;:&quot;http://macro-oss.oss-cn-shenzhen.aliyuncs.com/mall/images/20190129/17f2dd9756d9d333bee8e60ce8c03e4c_222_222.jpg&quot;,&quot;categoryId&quot;:19,&quot;categoryName&quot;:&quot;手机通讯&quot;,&quot;attrs&quot;:[&#123;&quot;attrId&quot;:9,&quot;attrName&quot;:&quot;容量&quot;,&quot;attrValue&quot;:&quot;64G&quot;&#125;,&#123;&quot;attrId&quot;:10,&quot;attrName&quot;:&quot;网络&quot;,&quot;attrValue&quot;:&quot;5G&quot;&#125;]&#125;PUT /product_db/_doc/14&#123;&quot;id&quot;:&quot;39&quot;,&quot;name&quot;:&quot;iPhone7/6s/8钢化膜苹果8Plus全屏复盖抗蓝光防窥防偷看手机膜&quot;,&quot;keywords&quot;:&quot;手机膜&quot;,&quot;subTitle&quot;:&quot;iPhone7/6s/8钢化膜苹果8Plus全屏复盖抗蓝光防窥防偷看手机膜&quot;,&quot;price&quot;:&quot;29&quot;,&quot;promotionPrice&quot;:&quot;39&quot;,&quot;originalPrice&quot;:&quot;49&quot;,&quot;pic&quot;:&quot;http://tuling-mall.oss-cn-shenzhen.aliyuncs.com/tulingmall/images/20200311/6df99dab78bb2014.jpg&quot;,&quot;sale&quot;:999,&quot;hasStock&quot;:true,&quot;salecount&quot;:799,&quot;putawayDate&quot;:&quot;2021-04-16&quot;,&quot;brandId&quot;:51,&quot;brandName&quot;:&quot;苹果&quot;,&quot;brandImg&quot;:&quot;http://tuling-mall.oss-cn-shenzhen.aliyuncs.com/tulingmall/images/20200311/2b84746650fc122d67749a876c453619.png&quot;,&quot;categoryId&quot;:30,&quot;categoryName&quot;:&quot;手机配件&quot;,&quot;attrs&quot;:[&#123;&quot;attrId&quot;:11,&quot;attrName&quot;:&quot;手机膜-材料&quot;,&quot;attrValue&quot;:&quot;钢化&quot;&#125;,&#123;&quot;attrId&quot;:12,&quot;attrName&quot;:&quot;手机膜-颜色&quot;,&quot;attrValue&quot;:&quot;白色&quot;&#125;]&#125;PUT /product_db/_doc/15&#123;&quot;id&quot;:&quot;40&quot;,&quot;name&quot;:&quot;七匹狼短袖T恤男纯棉舒适春夏修身运动休闲短袖三条装 圆领3条装&quot;,&quot;keywords&quot;:&quot;七匹狼服装 衣服&quot;,&quot;subTitle&quot;:&quot;七匹狼短袖T恤男纯棉舒适春夏修身运动休闲短袖三条装 圆领3条装&quot;,&quot;price&quot;:&quot;129&quot;,&quot;promotionPrice&quot;:&quot;139&quot;,&quot;originalPrice&quot;:&quot;149&quot;,&quot;pic&quot;:&quot;http://tuling-mall.oss-cn-shenzhen.aliyuncs.com/tulingmall/images/20200311/19e846e727dff337.jpg&quot;,&quot;sale&quot;:999,&quot;hasStock&quot;:true,&quot;salecount&quot;:199,&quot;putawayDate&quot;:&quot;2021-04-20&quot;,&quot;brandId&quot;:49,&quot;brandName&quot;:&quot;七匹狼&quot;,&quot;brandImg&quot;:&quot;http://macro-oss.oss-cn-shenzhen.aliyuncs.com/mall/images/20190129/18d8bc3eb13533fab466d702a0d3fd1f40345bcd.jpg&quot;,&quot;categoryId&quot;:8,&quot;categoryName&quot;:&quot;T恤&quot;,&quot;attrs&quot;:[&#123;&quot;attrId&quot;:3,&quot;attrName&quot;:&quot;尺寸&quot;,&quot;attrValue&quot;:&quot;M&quot;&#125;,&#123;&quot;attrId&quot;:4,&quot;attrName&quot;:&quot;颜色&quot;,&quot;attrValue&quot;:&quot;白色&quot;&#125;]&#125;PUT /product_db/_doc/16&#123;&quot;id&quot;:&quot;41&quot;,&quot;name&quot;:&quot;华为P40 Pro手机&quot;,&quot;keywords&quot;:&quot;华为手机&quot;,&quot;subTitle&quot;:&quot;华为P40 Pro手机&quot;,&quot;price&quot;:&quot;2129&quot;,&quot;promotionPrice&quot;:&quot;2139&quot;,&quot;originalPrice&quot;:&quot;2149&quot;,&quot;pic&quot;:&quot;http://macro-oss.oss-cn-shenzhen.aliyuncs.com/mall/images/20180607/5ac1bf58Ndefaac16.jpg&quot;,&quot;sale&quot;:999,&quot;hasStock&quot;:true,&quot;salecount&quot;:199,&quot;putawayDate&quot;:&quot;2021-05-03&quot;,&quot;brandId&quot;:3,&quot;brandName&quot;:&quot;华为&quot;,&quot;brandImg&quot;:&quot;http://macro-oss.oss-cn-shenzhen.aliyuncs.com/mall/images/20190129/17f2dd9756d9d333bee8e60ce8c03e4c_222_222.jpg&quot;,&quot;categoryId&quot;:19,&quot;categoryName&quot;:&quot;手机通讯&quot;,&quot;attrs&quot;:[&#123;&quot;attrId&quot;:9,&quot;attrName&quot;:&quot;容量&quot;,&quot;attrValue&quot;:&quot;128G&quot;&#125;,&#123;&quot;attrId&quot;:10,&quot;attrName&quot;:&quot;网络&quot;,&quot;attrValue&quot;:&quot;5G&quot;&#125;]&#125;PUT /product_db/_doc/17&#123;&quot;id&quot;:&quot;42&quot;,&quot;name&quot;:&quot;朵唯智能手机 4G全网通 老人学生双卡双待手机&quot;,&quot;keywords&quot;:&quot;朵唯手机&quot;,&quot;subTitle&quot;:&quot;朵唯手机后置双摄，国产虎贲芯片！优化散热结构！浅薄机身！朵唯4月特惠！&quot;,&quot;price&quot;:&quot;3129&quot;,&quot;promotionPrice&quot;:&quot;3139&quot;,&quot;originalPrice&quot;:&quot;3249&quot;,&quot;pic&quot;:&quot;http://macro-oss.oss-cn-shenzhen.aliyuncs.com/mall/images/20180615/xiaomi.jpg&quot;,&quot;sale&quot;:999,&quot;hasStock&quot;:true,&quot;salecount&quot;:1199,&quot;putawayDate&quot;:&quot;2021-06-01&quot;,&quot;brandId&quot;:59,&quot;brandName&quot;:&quot;朵唯&quot;,&quot;brandImg&quot;:&quot;http://tuling-mall.oss-cn-shenzhen.aliyuncs.com/tulingmall/images/20200311/2b84746650fc122d67749a876c453619.png&quot;,&quot;categoryId&quot;:19,&quot;categoryName&quot;:&quot;手机通讯&quot;,&quot;attrs&quot;:[&#123;&quot;attrId&quot;:9,&quot;attrName&quot;:&quot;容量&quot;,&quot;attrValue&quot;:&quot;32G&quot;&#125;,&#123;&quot;attrId&quot;:10,&quot;attrName&quot;:&quot;网络&quot;,&quot;attrValue&quot;:&quot;4G&quot;&#125;]&#125; 检索DSL语句构建 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647POST /product_db/_doc/_search&#123; &quot;from&quot;: 0, &quot;size&quot;: 8, &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;: [&#123;&quot;match&quot;: &#123;&quot;name&quot;: &#123;&quot;query&quot;: &quot;手机&quot;&#125;&#125;&#125;], &quot;filter&quot;: [ &#123;&quot;term&quot;: &#123;&quot;hasStock&quot;: &#123;&quot;value&quot;: true&#125;&#125;&#125;, &#123;&quot;range&quot;: &#123;&quot;price&quot;: &#123;&quot;from&quot;: &quot;1&quot;,&quot;to&quot;: &quot;5000&quot;&#125;&#125;&#125; ] &#125; &#125;, &quot;sort&quot;: [&#123;&quot;salecount&quot;: &#123;&quot;order&quot;: &quot;asc&quot;&#125;&#125;], &quot;aggregations&quot;: &#123; &quot;brand_agg&quot;: &#123; &quot;terms&quot;: &#123;&quot;field&quot;: &quot;brandId&quot;,&quot;size&quot;: 50&#125;, &quot;aggregations&quot;: &#123; &quot;brand_name_agg&quot;: &#123;&quot;terms&quot;: &#123;&quot;field&quot;: &quot;brandName&quot;&#125;&#125;, &quot;brand_img_agg&quot;: &#123;&quot;terms&quot;: &#123;&quot;field&quot;: &quot;brandImg&quot;&#125;&#125; &#125; &#125;, &quot;category_agg&quot;: &#123; &quot;terms&quot;: &#123;&quot;field&quot;: &quot;categoryId&quot;,&quot;size&quot;: 50,&quot;min_doc_count&quot;: 1&#125;, &quot;aggregations&quot;: &#123; &quot;category_name_agg&quot;: &#123;&quot;terms&quot;: &#123;&quot;field&quot;: &quot;categoryName&quot;&#125;&#125; &#125; &#125;, &quot;attr_agg&quot;: &#123; &quot;nested&quot;: &#123;&quot;path&quot;: &quot;attrs&quot;&#125;, &quot;aggregations&quot;: &#123; &quot;attr_id_agg&quot;: &#123; &quot;terms&quot;: &#123;&quot;field&quot;: &quot;attrs.attrId&quot;&#125;, &quot;aggregations&quot;: &#123; &quot;attr_name_agg&quot;: &#123;&quot;terms&quot;: &#123;&quot;field&quot;: &quot;attrs.attrName&quot;&#125;&#125;, &quot;attr_value_agg&quot;: &#123;&quot;terms&quot;: &#123;&quot;field&quot;: &quot;attrs.attrValue&quot;&#125;&#125; &#125; &#125; &#125; &#125; &#125;, &quot;highlight&quot;: &#123; &quot;pre_tags&quot;: [&quot;&lt;b style=&#x27;color:red&#x27;&gt;&quot;], &quot;post_tags&quot;: [&quot;&lt;/b&gt;&quot;], &quot;fields&quot;: &#123;&quot;name&quot;: &#123;&#125;&#125; &#125;&#125; Java代码实现 1234567@ResponseBody@RequestMapping(value = &quot;/searchList&quot;)public CommonResult&lt;ESResponseResult&gt; listPage(ESRequestParam param, HttpServletRequest request) &#123; // 根据传递来的页面的查询参数，去es中检索商品 ESResponseResult searchResult = tulingMallSearchService.search(param); return CommonResult.success(searchResult);&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221@Overridepublic ESResponseResult search(ESRequestParam param) &#123; try &#123; // 构建检索对象-封装请求相关参数信息 SearchRequest searchRequest = startBuildRequestParam(param); // 进行检索操作 SearchResponse response = client.search(searchRequest, RequestOptions.DEFAULT); // 分析响应数据，封装成指定的格式 ESResponseResult responseResult = startBuildResponseResult(response, param); return responseResult; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; return null;&#125;/** * 封装请求参数信息，关键字查询、根据属性、分类、品牌、价格区间、是否有库存等进行过滤、分页、高亮、以及聚合统计品牌分类属性 */private SearchRequest startBuildRequestParam(ESRequestParam param) &#123; SearchSourceBuilder searchSourceBuilder = new SearchSourceBuilder(); // 关键字查询、根据属性、分类、品牌、价格区间、是否有库存等进行过滤、分页、高亮、以及聚合统计品牌分类属性 BoolQueryBuilder boolQueryBuilder = new BoolQueryBuilder(); if (!StringUtils.isEmpty(param.getKeyword())) &#123; //单字段查询 boolQueryBuilder.must(QueryBuilders.matchQuery(&quot;name&quot;, param.getKeyword())); //多字段查询 boolQueryBuilder.must(QueryBuilders.multiMatchQuery(param.getKeyword(),&quot;name&quot;,&quot;keywords&quot;,&quot;subTitle&quot;)); &#125; // 根据类目ID进行过滤 if (null != param.getCategoryId()) &#123; boolQueryBuilder.filter(QueryBuilders.termQuery(&quot;categoryId&quot;, param.getCategoryId())); &#125; // 根据品牌ID进行过滤 if (null != param.getBrandId() &amp;&amp; param.getBrandId().size() &gt; 0) &#123; boolQueryBuilder.filter(QueryBuilders.termsQuery(&quot;brandId&quot;, param.getBrandId())); &#125; // 根据属性进行相关过滤 if (param.getAttrs() != null &amp;&amp; param.getAttrs().size() &gt; 0) &#123; param.getAttrs().forEach(item -&gt; &#123; //attrs=1_白色&amp;2_4核 BoolQueryBuilder boolQuery = QueryBuilders.boolQuery(); //attrs=1_64G String[] s = item.split(&quot;_&quot;); String attrId = s[0]; String[] attrValues = s[1].split(&quot;:&quot;);//这个属性检索用的值 boolQuery.must(QueryBuilders.termQuery(&quot;attrs.attrId&quot;, attrId)); boolQuery.must(QueryBuilders.termsQuery(&quot;attrs.attrValue&quot;, attrValues)); NestedQueryBuilder nestedQueryBuilder = QueryBuilders.nestedQuery(&quot;attrs&quot;, boolQuery, ScoreMode.None); boolQueryBuilder.filter(nestedQueryBuilder); &#125;); &#125; // 是否有库存 if (null != param.getHasStock()) &#123; boolQueryBuilder.filter(QueryBuilders.termQuery(&quot;hasStock&quot;, param.getHasStock() == 1)); &#125; // 根据价格过滤 if (!StringUtils.isEmpty(param.getPrice())) &#123; // 价格的输入形式为：10-100（起始价格和最终价格）或-100（不指定起始价格）或10-（不限制最终价格） RangeQueryBuilder rangeQueryBuilder = QueryBuilders.rangeQuery(&quot;price&quot;); String[] price = param.getPrice().split(&quot;_&quot;); if (price.length == 2) &#123; rangeQueryBuilder.gte(price[0]).lte(price[1]); &#125; else if (price.length == 1) &#123; if (param.getPrice().startsWith(&quot;_&quot;)) &#123; rangeQueryBuilder.lte(price[1]); &#125; if (param.getPrice().endsWith(&quot;_&quot;)) &#123; rangeQueryBuilder.gte(price[0]); &#125; &#125; boolQueryBuilder.filter(rangeQueryBuilder); &#125; // 封装所有查询条件 searchSourceBuilder.query(boolQueryBuilder); //实现排序、高亮、分页操作，排序，页面传入的参数值形式 sort=price_asc/desc if (!StringUtils.isEmpty(param.getSort())) &#123; String sort = param.getSort(); String[] sortFileds = sort.split(&quot;_&quot;); System.out.println(&quot;sortFileds:&quot;+sortFileds.length); if(!StringUtils.isEmpty(sortFileds[0]))&#123; SortOrder sortOrder = &quot;asc&quot;.equalsIgnoreCase(sortFileds[1]) ? SortOrder.ASC : SortOrder.DESC; searchSourceBuilder.sort(sortFileds[0], sortOrder); &#125; &#125; // 分页查询 searchSourceBuilder.from((param.getPageNum() - 1) * SearchConstant.PAGE_SIZE); searchSourceBuilder.size(SearchConstant.PAGE_SIZE); // 高亮显示 if (!StringUtils.isEmpty(param.getKeyword())) &#123; HighlightBuilder highlightBuilder = new HighlightBuilder(); highlightBuilder.field(&quot;name&quot;); highlightBuilder.preTags(&quot;&lt;b style=&#x27;color:red&#x27;&gt;&quot;); highlightBuilder.postTags(&quot;&lt;/b&gt;&quot;); searchSourceBuilder.highlighter(highlightBuilder); &#125; // 对品牌、分类信息、属性信息进行聚合分析，按照品牌进行聚合 TermsAggregationBuilder brand_agg = AggregationBuilders.terms(&quot;brand_agg&quot;); brand_agg.field(&quot;brandId&quot;).size(50); // 品牌的子聚合-品牌名聚合 brand_agg.subAggregation(AggregationBuilders.terms(&quot;brand_name_agg&quot;).field(&quot;brandName&quot;).size(1)); // 品牌的子聚合-品牌图片聚合 brand_agg.subAggregation(AggregationBuilders.terms(&quot;brand_img_agg&quot;).field(&quot;brandImg&quot;).size(1)); searchSourceBuilder.aggregation(brand_agg); // 按照分类信息进行聚合 TermsAggregationBuilder category_agg = AggregationBuilders.terms(&quot;category_agg&quot;); category_agg.field(&quot;categoryId&quot;).size(50); category_agg.subAggregation(AggregationBuilders.terms(&quot;category_name_agg&quot;).field(&quot;categoryName&quot;).size(1)); searchSourceBuilder.aggregation(category_agg); // 按照属性信息进行聚合 NestedAggregationBuilder attr_agg = AggregationBuilders.nested(&quot;attr_agg&quot;, &quot;attrs&quot;); // 按照属性ID进行聚合 TermsAggregationBuilder attr_id_agg = AggregationBuilders.terms(&quot;attr_id_agg&quot;).field(&quot;attrs.attrId&quot;); attr_agg.subAggregation(attr_id_agg); // 在每个属性ID下，按照属性名进行聚合 attr_id_agg.subAggregation(AggregationBuilders.terms(&quot;attr_name_agg&quot;).field(&quot;attrs.attrName&quot;).size(1)); // 在每个属性ID下，按照属性值进行聚合 attr_id_agg.subAggregation(AggregationBuilders.terms(&quot;attr_value_agg&quot;).field(&quot;attrs.attrValue&quot;).size(50)); searchSourceBuilder.aggregation(attr_agg); System.out.println(&quot;构建的DSL语句 &#123;&#125;:&quot;+ searchSourceBuilder.toString()); SearchRequest searchRequest = new SearchRequest(new String[]&#123;SearchConstant.INDEX_NAME&#125;, searchSourceBuilder); return searchRequest;&#125;/** * 封装查询到的结果信息，关键字查询、根据属性、分类、品牌、价格区间、是否有库存等进行过滤、分页、高亮、以及聚合统计品牌分类属性 */private ESResponseResult startBuildResponseResult(SearchResponse response, ESRequestParam param) &#123; ESResponseResult result = new ESResponseResult(); // 获取查询到的商品信息 SearchHits hits = response.getHits(); List&lt;EsProduct&gt; esModels = new ArrayList&lt;&gt;(); // 遍历所有商品信息 if (hits.getHits() != null &amp;&amp; hits.getHits().length &gt; 0) &#123; for (SearchHit hit : hits.getHits()) &#123; String sourceAsString = hit.getSourceAsString(); EsProduct esModel = JSON.parseObject(sourceAsString, EsProduct.class); // 判断是否按关键字检索，若是就显示高亮，否则不显示 if (!StringUtils.isEmpty(param.getKeyword())) &#123; // 拿到高亮信息显示标题 HighlightField name = hit.getHighlightFields().get(&quot;name&quot;); // 判断name中是否含有查询的关键字(因为是多字段查询，因此可能不包含指定的关键字，假设不包含则显示原始name字段的信息) String nameValue = name!=null ? name.getFragments()[0].string() : esModel.getName(); esModel.setName(nameValue); &#125; esModels.add(esModel); &#125; &#125; result.setProducts(esModels); // 当前商品涉及到的所有品牌信息，小米手机和小米电脑都属于小米品牌，过滤重复品牌信息 Set&lt;ESResponseResult.BrandVo&gt; brandVos = new LinkedHashSet&lt;&gt;(); // 获取到品牌的聚合 ParsedLongTerms brandAgg = response.getAggregations().get(&quot;brand_agg&quot;); for (Terms.Bucket bucket : brandAgg.getBuckets()) &#123; ESResponseResult.BrandVo brandVo = new ESResponseResult.BrandVo(); // 获取品牌的id long brandId = bucket.getKeyAsNumber().longValue(); brandVo.setBrandId(brandId); // 获取品牌的名字 ParsedStringTerms brandNameAgg = bucket.getAggregations().get(&quot;brand_name_agg&quot;); String brandName = brandNameAgg.getBuckets().get(0).getKeyAsString(); brandVo.setBrandName(brandName); // 获取品牌的LOGO ParsedStringTerms brandImgAgg = bucket.getAggregations().get(&quot;brand_img_agg&quot;); String brandImg = brandImgAgg.getBuckets().get(0).getKeyAsString(); brandVo.setBrandImg(brandImg); System.out.println(&quot;brandId:&quot;+brandId+&quot;brandName:&quot;+brandName+&quot;brandImg&quot;); brandVos.add(brandVo); &#125; System.out.println(&quot;brandVos.size:&quot;+brandVos.size()); result.setBrands(brandVos); // 当前商品相关的所有类目信息，获取到分类的聚合 List&lt;ESResponseResult.categoryVo&gt; categoryVos = new ArrayList&lt;&gt;(); ParsedLongTerms categoryAgg = response.getAggregations().get(&quot;category_agg&quot;); for (Terms.Bucket bucket : categoryAgg.getBuckets()) &#123; ESResponseResult.categoryVo categoryVo = new ESResponseResult.categoryVo(); // 获取分类id String keyAsString = bucket.getKeyAsString(); categoryVo.setCategoryId(Long.parseLong(keyAsString)); // 获取分类名 ParsedStringTerms categoryNameAgg = bucket.getAggregations().get(&quot;category_name_agg&quot;); String categoryName = categoryNameAgg.getBuckets().get(0).getKeyAsString(); categoryVo.setCategoryName(categoryName); categoryVos.add(categoryVo); &#125; result.setCategorys(categoryVos); // 获取商品相关的所有属性信息 List&lt;ESResponseResult.AttrVo&gt; attrVos = new ArrayList&lt;&gt;(); // 获取属性信息的聚合 ParsedNested attrsAgg = response.getAggregations().get(&quot;attr_agg&quot;); ParsedLongTerms attrIdAgg = attrsAgg.getAggregations().get(&quot;attr_id_agg&quot;); for (Terms.Bucket bucket : attrIdAgg.getBuckets()) &#123; ESResponseResult.AttrVo attrVo = new ESResponseResult.AttrVo(); // 获取属性ID值 long attrId = bucket.getKeyAsNumber().longValue(); attrVo.setAttrId(attrId); // 获取属性的名字 ParsedStringTerms attrNameAgg = bucket.getAggregations().get(&quot;attr_name_agg&quot;); String attrName = attrNameAgg.getBuckets().get(0).getKeyAsString(); attrVo.setAttrName(attrName); // 获取属性的值 ParsedStringTerms attrValueAgg = bucket.getAggregations().get(&quot;attr_value_agg&quot;); List&lt;String&gt; attrValues = attrValueAgg.getBuckets().stream().map(item -&gt; item.getKeyAsString()).collect(Collectors.toList()); attrVo.setAttrValue(attrValues); attrVos.add(attrVo); &#125; result.setAttrs(attrVos); // 进行分页操作 result.setPageNum(param.getPageNum()); // 获取总记录数 long total = hits.getTotalHits().value; result.setTotal(total); // 计算总页码 int totalPages = (int) total % SearchConstant.PAGE_SIZE == 0 ? (int) total / SearchConstant.PAGE_SIZE : ((int) total / SearchConstant.PAGE_SIZE + 1); result.setTotalPages(totalPages); List&lt;Integer&gt; pageNavs = new ArrayList&lt;&gt;(); for (int i = 1; i &lt;= totalPages; i++) &#123; pageNavs.add(i); &#125; result.setPageNavs(pageNavs); return result;&#125;","tags":[{"name":"ElasticSearch","slug":"ElasticSearch","permalink":"http://example.com/tags/ElasticSearch/"}],"categories":[{"name":"工具和中间件","slug":"工具和中间件","permalink":"http://example.com/categories/%E5%B7%A5%E5%85%B7%E5%92%8C%E4%B8%AD%E9%97%B4%E4%BB%B6/"},{"name":"搜索引擎技术","slug":"工具和中间件/搜索引擎技术","permalink":"http://example.com/categories/%E5%B7%A5%E5%85%B7%E5%92%8C%E4%B8%AD%E9%97%B4%E4%BB%B6/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E6%8A%80%E6%9C%AF/"},{"name":"ElasticSearch","slug":"工具和中间件/搜索引擎技术/ElasticSearch","permalink":"http://example.com/categories/%E5%B7%A5%E5%85%B7%E5%92%8C%E4%B8%AD%E9%97%B4%E4%BB%B6/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E6%8A%80%E6%9C%AF/ElasticSearch/"}]},{"title":"ElasticSearch工具-Kibana","date":"2022-01-01T16:00:00.000Z","path":"blog/工具和中间件/搜索引擎技术/ElasticSearch/ElasticSearch工具-Kibana/","text":"Kibana 是什么Kibana 是一个针对 ElasticSearch 的开源分析及可视化平台，用来搜索、查看交互存储在 ElasticSearch 索引中的数据。使用 Kibana，可以通过各种图表进行高级数据分析及展示。 Kibana 是在ElasticSearch 有了相当多的数据之后，进行分析这些数据用的工具。 但是现在还么有数据呀，为什么就要介绍这个工具呢？ 因为Kibana 里面有一个叫做 Dev Tools的，可以很方便地以Restful 风格向 ElasticSearch 服务器提交请求。Kibana 让海量数据更容易理解。它操作简单，基于浏览器的用户界面可以快速创建仪表板（DashBoard）实时显示 ElasticSearch 查询动态。 官网下载(https://www.elastic.co/cn/kibana/)下载rar，并解压。 假设下载路径在：C:\\Users\\X7TI运行启动中的 kibana.bat 1C:\\Users\\X7TI\\Downloads\\kibana-6.2.2-windows-x86_64\\bin\\kibana.bat 验证启动浏览器输入 http://localhost:5601/app/kibana#/dev_tools/console?_g=(), 打开当前的开发工具 Dev Tools 界面 运行测试在控制台里输入 1GET /_cat/health?v 然后点击绿色箭头进行运行，就可以看到右侧出现查询结果GET &#x2F;_cat&#x2F;health?v 这个命令用来查看服务器状态（健康度）， green 表示一切OK。 索引概念索引相当于就是一个数据库服务器上的某个数据库，所以索引也可以看成是Elastic Search里的某个数据库 Restful 风格管理索引，管理无非就是增删改查，即 CRUD。在使用Restful风格之前，进行所以管理需要这样的访问地址： add,delete,update,get 等不同的访问地址来表示不同的业务请求。但是使用Restful 风格，就通过提交不同的method 来表示 CRUD： PUT 表示增加 GET 表示获取 DELETE 表示删除 POST表示更新 增加索引:在 kibana 控制台中输入如下命令：打开 kibana控制台： 1http://localhost:5601/app/kibana#/dev_tools/console?_g=() 运行如下命令： 1PUT /howToKibana?pretty 返回： 12345&#123; &quot;acknowledged&quot;: true, &quot;shards_acknowledged&quot;: true, &quot;index&quot;: &quot;howToKibana&quot;&#125; 表示创建成功了，索引名称是howToKibana注： 要运行kibana控制台，需要先安装kibana: 查询运行如下命令： 1GET /_cat/indices?v 可以观察到新建立的索引 删除运行如下命令：DELETE &#x2F;howToKibana?pretty再运行 1GET /_cat/indices?v 可以观察到索引howToKibana被删除了，右侧一个索引也看不到了 修改修改两种方式， 第一种还是用PUT，PUT本来用来做增加的，但是当输入的id已经存在的时候，就自动变成修改功能了; 第二种使用 POST，这才是正规的修改，其实和第一种效果一样的。 批量导入批量导入两条数据在 kibana 控制台中输入如下命令：打开 kibana控制台： 1http://localhost:5601/app/kibana#/dev_tools/console?_g=() 运行如下命令： 12345POST _bulk&#123;&quot;index&quot;:&#123;&quot;_index&quot;:&quot;howToKibana&quot;,&quot;_type&quot;:&quot;product&quot;,&quot;_id&quot;:10001&#125;&#125;&#123;&quot;code&quot;:&quot;540785126782&quot;,&quot;price&quot;:398,&quot;name&quot;:&quot;房屋卫士自流平美缝剂瓷砖地砖专用双组份真瓷胶防水填缝剂镏金色&quot;,&quot;place&quot;:&quot;上海&quot;,&quot;category&quot;:&quot;品质建材&quot;&#125;&#123;&quot;index&quot;:&#123;&quot;_index&quot;:&quot;howToKibana&quot;,&quot;_type&quot;:&quot;product&quot;,&quot;_id&quot;:10002&#125;&#125;&#123;&quot;code&quot;:&quot;24727352473&quot;,&quot;price&quot;:21.799999237060547,&quot;name&quot;:&quot;艾瑞泽手工大号小号调温热熔胶枪玻璃胶枪硅胶条热溶胶棒20W-100W&quot;,&quot;place&quot;:&quot;山东青岛&quot;,&quot;category&quot;:&quot;品质建材&quot;&#125; 注： 要运行kibana控制台，需要先安装kibana并启动 注： 其中的product在elastic search里是type的概念，相当于数据库里的表，这里就相当于向 product 表里插入了一条数据 验证插入的数据使用命令查询howToKibana 索引里所有的数据： 1234GET /howToKibana/_search&#123; &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;&#125; 可以看到刚刚批量插入的两条数据.","tags":[{"name":"ElasticSearch","slug":"ElasticSearch","permalink":"http://example.com/tags/ElasticSearch/"},{"name":"Kibana","slug":"Kibana","permalink":"http://example.com/tags/Kibana/"}],"categories":[{"name":"工具和中间件","slug":"工具和中间件","permalink":"http://example.com/categories/%E5%B7%A5%E5%85%B7%E5%92%8C%E4%B8%AD%E9%97%B4%E4%BB%B6/"},{"name":"搜索引擎技术","slug":"工具和中间件/搜索引擎技术","permalink":"http://example.com/categories/%E5%B7%A5%E5%85%B7%E5%92%8C%E4%B8%AD%E9%97%B4%E4%BB%B6/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E6%8A%80%E6%9C%AF/"},{"name":"ElasticSearch","slug":"工具和中间件/搜索引擎技术/ElasticSearch","permalink":"http://example.com/categories/%E5%B7%A5%E5%85%B7%E5%92%8C%E4%B8%AD%E9%97%B4%E4%BB%B6/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E6%8A%80%E6%9C%AF/ElasticSearch/"}]},{"title":"ElasticSearch进阶","date":"2022-01-01T16:00:00.000Z","path":"blog/工具和中间件/搜索引擎技术/ElasticSearch/ElasticSearch进阶/","text":"分值计算首先根据用户query条件,过滤出包含指定term的doc,Field-length norm即field长度越长相关度越弱。 123query &quot;hello world&quot; --&gt; hello / world / hello &amp; worldbool --&gt; must/must not/should --&gt; 过滤 --&gt; 包含 / 不包含 / 可能包含doc --&gt; 不打分数 --&gt; 正或反 true or false --&gt; 为了减少后续要计算的doc的数量,提升性能 relevance score算法：计算出一个 索引中文本 与 搜索文本 之间 关联匹配程度,ES使用 term frequency/inverse document frequency算法 简称为 TF/IDF 算法。 Term frequency 即 搜索文本 中 各个词条 在 field 文本中 出现次数,次数越多越相关 。Inverse document frequency 即 搜索文本 中 各个词条 在 整个索引所有文档 中 出现次数,出现次数越多越不相关 。 向量空间模型vector space model向量空间模型,多个term对一个doc的总分数,es会根据查询字符串在所有doc中的评分情况,计算出一个 query vector 即 query向量,会给每一个doc,拿每个term计算出一个分数来。每个doc vector计算出对 query vector 的 弧度,最后基于该弧度给出一个doc相对于query中多个term的总分数,弧度越大分数越低,弧度越小分数越高 。若是多个term,那么就是线性代数来计算,无法用图表示若查询条件字符串为hello world,hello这个term,给的基于所有doc的一个评分就是3,world这个term,给的基于所有doc的一个评分就是6,则 query向量 为 [3, 6],若3个doc一个包含hello,一个包含world,一个包含hello和world,doc向量分别为[3, 0]、[0, 6]、[3, 6] 分词器工作流程首先进行 normalization切分词语,将目标文本拆分成单个单词,同时对每个单词进行 normalization时态转换单复数转换、分词器recall、搜索时召回率、增加能搜索到的结果的数量 。分词器将文本进行各种处理,最后处理好的结果才会用来建立倒排索引 。 123character filter：在一段文本进行分词之前,先进行预处理,如过滤html标签（&lt;span&gt;hello&lt;span&gt; --&gt; hello）,&amp; --&gt; and (I&amp;you --&gt; I and you)tokenizer：分词,hello you and me --&gt; hello, you, and, metoken filter：lowercase,stop word,synonymom,liked --&gt; like,Tom --&gt; tom,a/the/an --&gt; 干掉,small --&gt; little 对于默认的 standard分词器 ： standard tokenizer： 以单词边界进行切分 standard token filter：什么都不做 lowercase token filter：将所有字母转换为小写 stop token filer：默认被禁用,移除停用词,比如a the it等等1234567891011121314151617181920212223242526272829303132333435363738394041424344454647POST _analyze&#123; &quot;analyzer&quot;: &quot;standard&quot;, &quot;text&quot;: &quot;Set the shape to semi-transparent by calling set_trans(5)&quot;&#125;PUT /my_index&#123; &quot;settings&quot;: &#123; &quot;analysis&quot;: &#123; &quot;analyzer&quot;: &#123; &quot;es_std&quot;: &#123; &quot;type&quot;: &quot;standard&quot;, &quot;stopwords&quot;: &quot;_english_&quot; // 启用english停用词token filter &#125; &#125; &#125; &#125;&#125;GET /my_index/_analyze&#123; &quot;analyzer&quot;: &quot;standard&quot;, &quot;text&quot;: &quot;a dog is in the house&quot;&#125;GET /my_index/_analyze&#123; &quot;analyzer&quot;: &quot;es_std&quot;, &quot;text&quot;:&quot;a dog is in the house&quot;&#125;PUT /my_index // 定制化分词器&#123; &quot;settings&quot;: &#123; &quot;analysis&quot;: &#123; &quot;char_filter&quot;: &#123;&quot;&amp;_to_and&quot;: &#123;&quot;type&quot;: &quot;mapping&quot;,&quot;mappings&quot;: [&quot;&amp;=&gt; and&quot;]&#125;&#125;, &quot;filter&quot;: &#123;&quot;my_stopwords&quot;: &#123;&quot;type&quot;: &quot;stop&quot;,&quot;stopwords&quot;: [&quot;the&quot;,&quot;a&quot;]&#125; &#125;, &quot;analyzer&quot;: &#123; &quot;my_analyzer&quot;: &#123;&quot;type&quot;: &quot;custom&quot;,&quot;char_filter&quot;: [&quot;html_strip&quot;,&quot;&amp;_to_and&quot;],&quot;tokenizer&quot;: &quot;standard&quot;,&quot;filter&quot;: [&quot;lowercase&quot;,&quot;my_stopwords&quot;]&#125; &#125; &#125; &#125;&#125;GET /my_index/_analyze&#123; &quot;text&quot;: &quot;tom&amp;jerry are a friend in the house, &lt;a&gt;, HAHA!!&quot;, &quot;analyzer&quot;: &quot;my_analyzer&quot;&#125; IK分词器IK分词器配置文件地址为 es/plugins/ik/config,ik原生最重要的是 main.dic 和 stopword.dic 两个配置文件 IKAnalyzer.cfg.xml：用来配置自定义词库 main.dic：ik原生内置中文词库,总共有27万多条,会按照该文件中的词语去分词 quantifier.dic：单位相关的词 suffix.dic：后缀相关的词 surname.dic：中国姓氏 stopword.dic：英文停用词,停用词会在分词时被干掉,不会建立在倒排索引中 可通过在 IKAnalyzer.cfg.xml 配置文件中通过修改 &lt;entry key=&quot;ext_dict&quot;&gt;&lt;/entry&gt; 配置内容 扩展自己的词库,需重启es才能生效,还可以通过修改 &lt;entry key=&quot;ext_stopwords&quot;&gt;&lt;/entry&gt; 配置扩展停用词 。 每次在es扩展词典中,手动添加新词语,添加完都要重启es才能生效,非常麻烦,且es是分布式的,可能有数百个节点,不能每次都一个一个节点上面去修改。 IKAnalyzer.cfg.xml 配置文件中可通过 &lt;entry key=&quot;remote_ext_dict&quot;&gt;words_location&lt;/entry&gt; 和 &lt;entry key=&quot;remote_ext_stopwords&quot;&gt;words_location&lt;/entry&gt; 配置支持 远程扩展字典 。 高亮显示搜索中经常需要对搜索关键字做高亮显示,ES默认通过添加 &lt;em&gt;&lt;/em&gt;标签,在HTML中会变成红色,指定的field中若包含了搜索词,就会在那个field文本中,对搜索词进行红色高亮显示。highlight中的field必须跟 query 中field一一对齐 默认的 highlight 为 plain highlight 即 lucene highlight,在 mapping 中设置 index_options 为 offsets 使用 posting highlight 。在 mapping 中设置 term_vector 为 term_vector 使用 fast verctor highlight,对 大于1mb的field性能更高 。也可通过在查询时强制使用某种highlighter 。 一般情况下用 plain highlight 也就足够了,不需要做其他额外设置,若对高亮性能要求很高,可尝试启用 posting highlight,若 field值特别大超过了1M,则可用 fast vector highlight 。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455PUT /news_website&#123; &quot;mappings&quot;: &#123; &quot;properties&quot;: &#123; &quot;title&quot;: &#123;&quot;type&quot;: &quot;text&quot;,&quot;analyzer&quot;: &quot;ik_max_word&quot;&#125;, &quot;content&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;ik_max_word&quot;, &quot;index_options&quot;: &quot;offsets&quot; &#125; &#125; &#125;&#125;PUT /news_website&#123; &quot;mappings&quot;: &#123; &quot;properties&quot;: &#123; &quot;title&quot;: &#123;&quot;type&quot;: &quot;text&quot;,&quot;analyzer&quot;: &quot;ik_max_word&quot;&#125;, &quot;content&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;ik_max_word&quot;, &quot;term_vector&quot;: &quot;with_positions_offsets&quot; &#125; &#125; &#125;&#125;GET /news_website/_doc/_search&#123; &quot;query&quot;: &#123;&quot;match&quot;: &#123;&quot;content&quot;: &quot;文章&quot;&#125;&#125;, &quot;highlight&quot;: &#123; &quot;fields&quot;: &#123;&quot;content&quot;: &#123;&quot;type&quot;: &quot;plain&quot;&#125;&#125; &#125;&#125;GET /news_website/_doc/_search&#123; &quot;query&quot;: &#123;&quot;match&quot;: &#123;&quot;content&quot;: &quot;文章&quot;&#125;&#125;, &quot;highlight&quot;: &#123; // 设置高亮html标签,默认是&lt;em&gt;标签 &quot;pre_tags&quot;: [&quot;&lt;span color=&#x27;red&#x27;&gt;&quot;], &quot;post_tags&quot;: [&quot;&lt;/span&gt;&quot;], &quot;fields&quot;: &#123;&quot;content&quot;: &#123;&quot;type&quot;: &quot;plain&quot;&#125;&#125; &#125;&#125;GET /_search&#123; &quot;query&quot;: &#123;&quot;match&quot;: &#123;&quot;content&quot;: &quot;文章&quot;&#125;&#125;, &quot;highlight&quot;: &#123; &quot;fields&quot;: &#123; &quot;content&quot;: &#123; &quot;fragment_size&quot;: 150, // 设置要显示出来的fragment文本长度,默认100 &quot;number_of_fragments&quot;: 3 // 指定显示高亮fragment文本片段个数 &#125; &#125; &#125;&#125;用一个大家容易理解的SQL语法来解释,如：select count(*) from table group by column。那么group by column分组后的每组数据就是bucket。对每个分组执行的count(*)就是metric。 聚合搜索bucket就是一个聚合搜索时的数据分组,metric就是对一个bucket数据执行的统计分析,metric有求和,最大值,最小值,平均值等多种统计 。如 select count(*) from table group by column 其中 group by column 分组后的 每组数据就是bucket,每个分组执行的 count(*) 就是 metric 。 1234567891011121314151617181920PUT /cars&#123;&quot;mappings&quot;:&#123;&quot;properties&quot;:&#123;&quot;price&quot;:&#123;&quot;type&quot;:&quot;long&quot;&#125;,&quot;color&quot;:&#123;&quot;type&quot;:&quot;keyword&quot;&#125;,&quot;brand&quot;:&#123;&quot;type&quot;:&quot;keyword&quot;&#125;,&quot;model&quot;:&#123;&quot;type&quot;:&quot;keyword&quot;&#125;,&quot;sold_date&quot;:&#123;&quot;type&quot;:&quot;date&quot;&#125;,&quot;remark&quot;:&#123;&quot;type&quot;:&quot;text&quot;,&quot;analyzer&quot;:&quot;ik_max_word&quot;&#125;&#125;&#125;&#125;POST /cars/_bulk&#123;&quot;index&quot;:&#123;&#125;&#125;&#123;&quot;price&quot;:258000,&quot;color&quot;:&quot;金色&quot;,&quot;brand&quot;:&quot;大众&quot;,&quot;model&quot;:&quot;大众迈腾&quot;,&quot;sold_date&quot;:&quot;2021-10-28&quot;,&quot;remark&quot;:&quot;大众中档车&quot;&#125;&#123;&quot;index&quot;:&#123;&#125;&#125;&#123;&quot;price&quot;:123000,&quot;color&quot;:&quot;金色&quot;,&quot;brand&quot;:&quot;大众&quot;,&quot;model&quot;:&quot;大众速腾&quot;,&quot;sold_date&quot;:&quot;2021-11-05&quot;,&quot;remark&quot;:&quot;大众神车&quot;&#125;&#123;&quot;index&quot;:&#123;&#125;&#125;&#123;&quot;price&quot;:239800,&quot;color&quot;:&quot;白色&quot;,&quot;brand&quot;:&quot;标志&quot;,&quot;model&quot;:&quot;标志508&quot;,&quot;sold_date&quot;:&quot;2021-05-18&quot;,&quot;remark&quot;:&quot;标志品牌全球上市车型&quot;&#125;&#123;&quot;index&quot;:&#123;&#125;&#125;&#123;&quot;price&quot;:148800,&quot;color&quot;:&quot;白色&quot;,&quot;brand&quot;:&quot;标志&quot;,&quot;model&quot;:&quot;标志408&quot;,&quot;sold_date&quot;:&quot;2021-07-02&quot;,&quot;remark&quot;:&quot;比较大的紧凑型车&quot;&#125;&#123;&quot;index&quot;:&#123;&#125;&#125;&#123;&quot;price&quot;:1998000,&quot;color&quot;:&quot;黑色&quot;,&quot;brand&quot;:&quot;大众&quot;,&quot;model&quot;:&quot;大众辉腾&quot;,&quot;sold_date&quot;:&quot;2021-08-19&quot;,&quot;remark&quot;:&quot;大众最让人肝疼的车&quot;&#125;&#123;&quot;index&quot;:&#123;&#125;&#125;&#123;&quot;price&quot;:218000,&quot;color&quot;:&quot;红色&quot;,&quot;brand&quot;:&quot;奥迪&quot;,&quot;model&quot;:&quot;奥迪A4&quot;,&quot;sold_date&quot;:&quot;2021-11-05&quot;,&quot;remark&quot;:&quot;小资车型&quot;&#125;&#123;&quot;index&quot;:&#123;&#125;&#125;&#123;&quot;price&quot;:489000,&quot;color&quot;:&quot;黑色&quot;,&quot;brand&quot;:&quot;奥迪&quot;,&quot;model&quot;:&quot;奥迪A6&quot;,&quot;sold_date&quot;:&quot;2022-01-01&quot;,&quot;remark&quot;:&quot;政府专用？&quot;&#125;&#123;&quot;index&quot;:&#123;&#125;&#125;&#123;&quot;price&quot;:1899000,&quot;color&quot;:&quot;黑色&quot;,&quot;brand&quot;:&quot;奥迪&quot;,&quot;model&quot;:&quot;奥迪A 8&quot;,&quot;sold_date&quot;:&quot;2022-02-12&quot;,&quot;remark&quot;:&quot;很贵的大A6。。。&quot;&#125; 根据color 分组统计销售数量,只执行聚合分组,ES中 最基础的聚合 为 terms,相当于SQL中的count,ES中默认为分组数据做排序,使用的是 doc_count 数据执行 降序排列 。可使用 _key 元数据根据分组后的 字段数据 执行不同的排序方案,也可根据 _count 元数据,根据分组后的统计值执行不同的排序方案 。 1234567891011GET /cars/_search&#123; &quot;aggs&quot;: &#123; &quot;group_by_color&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;color&quot;, &quot;order&quot;: &#123;&quot;_count&quot;: &quot;desc&quot;&#125; &#125; &#125; &#125;&#125; 先根据color执行聚合分组,在此分组的基础上,对组内数据执行聚合统计,组内数据的聚合统计就是 metric,同样可执行排序,因为组内有聚合统计,且对统计数据给予了命名avg_by_price,所以可根据该聚合统计数据字段名执行排序逻辑。 size可设置为0,表示不返回文档只返回聚合之后的数据,提高查询速度,若需要这些文档也可按照实际情况进行设置。对聚合统计数据进行排序,若有多层 aggs 执行 下钻聚合 时也可 根据最内层聚合数据执行排序 。 1234567891011121314151617181920212223242526272829GET /cars/_search&#123; &quot;aggs&quot;: &#123; &quot;group_by_color&quot;: &#123; &quot;terms&quot;: &#123; &quot;field&quot;: &quot;color&quot;, &quot;order&quot;: &#123;&quot;avg_by_price&quot;: &quot;asc&quot;&#125; &#125;, &quot;aggs&quot;: &#123; &quot;avg_by_price&quot;: &#123;&quot;avg&quot;: &#123;&quot;field&quot;: &quot;price&quot;&#125;&#125; &#125; &#125; &#125;&#125;GET /cars/_search&#123; &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;group_by_color&quot;: &#123; &quot;terms&quot;: &#123;&quot;field&quot;: &quot;color&quot;&#125;, &quot;aggs&quot;: &#123; &quot;group_by_brand&quot;: &#123; &quot;terms&quot;: &#123;&quot;field&quot;: &quot;brand&quot;,&quot;order&quot;: &#123;&quot;avg_by_price&quot;: &quot;desc&quot;&#125;&#125;, &quot;aggs&quot;: &#123;&quot;avg_by_price&quot;: &#123;&quot;avg&quot;: &#123;&quot;field&quot;: &quot;price&quot;&#125;&#125;&#125; &#125; &#125; &#125; &#125;&#125; 先根据color聚合分组,在组内根据brand再次聚合分组,这种操作可称为下钻分析,aggs若定义比较多,则会感觉语法格式混乱,aggs语法格式有一个相对固定的结构,aggs可嵌套定义也可水平定义 。嵌套定义称为下钻分析,水平定义就是平铺多个分组方式 123456789101112131415GET /index_name/type_name/_search&#123; &quot;aggs&quot;: &#123; &quot;定义分组名称&quot;: &#123; &quot;分组策略如：terms、avg、sum&quot;: &#123; &quot;field&quot;: &quot;根据哪一个字段分组&quot;, &quot;其他参数&quot;: &quot;&quot; &#125;, &quot;aggs&quot;: &#123; &quot;分组名称1&quot;: &#123;&#125;, &quot;分组名称2&quot;: &#123;&#125; &#125; &#125; &#125;&#125; 统计不同color中的最大和最小价格、总价,聚合分析最常用的种类就是统计数量,最大,最小,平均,总计等 12345678910111213141516171819202122232425262728293031GET /cars/_search&#123; &quot;aggs&quot;: &#123; &quot;group_by_color&quot;: &#123; &quot;terms&quot;: &#123;&quot;field&quot;: &quot;color&quot;&#125;, &quot;aggs&quot;: &#123; &quot;max_price&quot;: &#123;&quot;max&quot;: &#123;&quot;field&quot;: &quot;price&quot;&#125;&#125;, &quot;min_price&quot;: &#123;&quot;min&quot;: &#123;&quot;field&quot;: &quot;price&quot;&#125;&#125;, &quot;sum_price&quot;: &#123;&quot;sum&quot;: &#123;&quot;field&quot;: &quot;price&quot;&#125;&#125; &#125; &#125; &#125;&#125;GET cars/_search // 统计不同品牌汽车中价格排名最高的车型&#123; &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;group_by_brand&quot;: &#123; &quot;terms&quot;: &#123;&quot;field&quot;: &quot;brand&quot;&#125;, &quot;aggs&quot;: &#123; &quot;top_car&quot;: &#123; &quot;top_hits&quot;: &#123; &quot;size&quot;: 1, // 取组内多少条数据,默认为10 &quot;sort&quot;: [&#123;&quot;price&quot;: &#123;&quot;order&quot;: &quot;desc&quot;&#125;&#125;], // 组内使用什么字段什么规则排序,默认使用_doc的asc规则排序 &quot;_source&quot;: &#123;&quot;includes&quot;: [&quot;model&quot;,&quot;price&quot;]&#125; // 结果中包含document中的哪些字段,默认包含全部字段 &#125; &#125; &#125; &#125; &#125;&#125; histogram区间统计 类似 terms, 也是用于 bucket分组操作, 是根据一个field实现数据区间分组 。如以100万为一个范围,统计不同范围内车辆销售量和平均价格。使用 histogram聚合 时field指定价格字段price,区间范围是100万,此时ES会将price价格区间划分为： [0, 1000000), [1000000, 2000000), [2000000, 3000000)等依次类推。在划分区间同时 histogram 会类似 terms 进行 数据数量统计, 可通过嵌套aggs对聚合分组后的组内数据做再次聚合分析 。 123456789101112GET /cars/_search&#123; &quot;aggs&quot;: &#123; &quot;histogram_by_price&quot;: &#123; &quot;histogram&quot;: &#123; &quot;field&quot;: &quot;price&quot;, &quot;interval&quot;: 1000000 &#125;, &quot;aggs&quot;: &#123;&quot;avg_by_price&quot;: &#123;&quot;avg&quot;: &#123;&quot;field&quot;: &quot;price&quot;&#125;&#125;&#125; &#125; &#125;&#125; date_histogram区间分组可对date类型的field执行区间聚合分组,若以月为单位,统计不同月份汽车销售数量及销售总金额。此时可使用 date_histogram 实现聚合分组,其中field来指定用于聚合分组的字段,interval指定 区间范围,可选值有 year、quarter、month、week、day、hour、minute、second,format指定日期格式化,min_doc_count指定每个区间最少document,若不指定默认为0,当区间范围内没有document时,也会显示bucket分组,extended_bounds指定起始时间和结束时间,若不指定默认使用字段中日期最小值和最大值作为起始和结束时间。 123456789101112131415GET /cars/_search&#123; &quot;aggs&quot;: &#123; &quot;histogram_by_date&quot;: &#123; &quot;date_histogram&quot;: &#123; &quot;field&quot;: &quot;sold_date&quot;, &quot;interval&quot;: &quot;month&quot;, // 7.X之后使用calendar_interval,指定区间范围 &quot;format&quot;: &quot;yyyy-MM-dd&quot;, // 指定日期格式化 &quot;min_doc_count&quot;: 1, &quot;extended_bounds&quot;: &#123;&quot;min&quot;: &quot;2021-01-01&quot;,&quot;max&quot;: &quot;2022-12-31&quot;&#125; &#125;, &quot;aggs&quot;: &#123;&quot;sum_by_price&quot;: &#123;&quot;sum&quot;: &#123;&quot;field&quot;: &quot;price&quot;&#125;&#125;&#125; &#125; &#125;&#125; 聚合统计数据时,有时需要对比部分数据和总体数据,如统计某品牌车辆平均价格和所有车辆平均价格。 global 是用于定义一个全局bucket,该 bucket 会 忽略query 的条件,检索所有document进行对应的聚合统计。 123456789101112GET /cars/_search&#123; &quot;size&quot;: 0, &quot;query&quot;: &#123;&quot;match&quot;: &#123;&quot;brand&quot;: &quot;大众&quot;&#125;&#125;, &quot;aggs&quot;: &#123; &quot;volkswagen_of_avg_price&quot;: &#123;&quot;avg&quot;: &#123;&quot;field&quot;: &quot;price&quot;&#125;&#125;, // 统计某品牌车辆平均价格 &quot;all_avg_price&quot;: &#123; // 所有车辆平均价格 &quot;global&quot;: &#123;&#125;, &quot;aggs&quot;: &#123;&quot;all_of_price&quot;: &#123;&quot;avg&quot;: &#123;&quot;field&quot;: &quot;price&quot;&#125;&#125;&#125; &#125; &#125;&#125; filter也可和aggs组合使用,实现相对复杂的过滤聚合分析,filter的范围决定了其过滤的范围,将filter放在aggs内部,代表该过滤器只对query搜索得到的结果执行filter过滤 。若filter放在aggs外部,过滤器则会过滤所有数据。 12345678910111213141516171819GET /cars/_search // filter和aggs组合使用,实现相对复杂的过滤聚合分析&#123; &quot;query&quot;: &#123; &quot;constant_score&quot;: &#123; &quot;filter&quot;: &#123;&quot;range&quot;: &#123;&quot;price&quot;: &#123;&quot;gte&quot;: 100000,&quot;lte&quot;: 500000&#125;&#125;&#125; &#125; &#125;, &quot;aggs&quot;: &#123;&quot;avg_by_price&quot;: &#123;&quot;avg&quot;: &#123;&quot;field&quot;: &quot;price&quot;&#125;&#125;&#125;&#125;GET /cars/_search&#123; &quot;query&quot;: &#123;&quot;match&quot;: &#123;&quot;brand&quot;: &quot;大众&quot;&#125;&#125;, &quot;aggs&quot;: &#123; &quot;count_last_year&quot;: &#123; // 12M/M表示12个月,1y/y表示1年,d表示天 &quot;filter&quot;: &#123;&quot;range&quot;: &#123;&quot;sold_date&quot;: &#123;&quot;gte&quot;: &quot;now-12M&quot;&#125;&#125;&#125;, &quot;aggs&quot;: &#123;&quot;sum_of_price_last_year&quot;: &#123;&quot;sum&quot;: &#123;&quot;field&quot;: &quot;price&quot;&#125;&#125;&#125; &#125; &#125;&#125; 数据建模如下设计一个用户document数据类型,其中包含一个地址数据的数组,该设计方式相对复杂,但在管理数据时更加的灵活。但也有明显的缺陷,针对地址数据做数据搜索时,经常会搜索出不必要的数据 。 123456789101112131415161718192021222324252627282930313233343536373839404142434445PUT /user_index&#123; &quot;mappings&quot;: &#123; &quot;properties&quot;: &#123; &quot;login_name&quot;: &#123;&quot;type&quot;: &quot;keyword&quot;&#125;, &quot;age &quot;: &#123;&quot;type&quot;: &quot;short&quot;&#125;, &quot;address&quot;: &#123; &quot;properties&quot;: &#123; &quot;province&quot;: &#123;&quot;type&quot;: &quot;keyword&quot;&#125;, &quot;city&quot;: &#123;&quot;type&quot;: &quot;keyword&quot;&#125;, &quot;street&quot;: &#123;&quot;type&quot;: &quot;keyword&quot;&#125; &#125; &#125; &#125; &#125;&#125;PUT /user_index/_doc/1&#123; &quot;login_name&quot;: &quot;jack&quot;, &quot;age&quot;: 25, &quot;address&quot;: [ &#123;&quot;province&quot;: &quot;北京&quot;,&quot;city&quot;: &quot;北京&quot;,&quot;street&quot;: &quot;枫林三路&quot;&#125;, &#123;&quot;province&quot;: &quot;天津&quot;,&quot;city&quot;: &quot;天津&quot;,&quot;street&quot;: &quot;华夏路&quot;&#125; ]&#125;PUT /user_index/_doc/2&#123; &quot;login_name&quot;: &quot;rose&quot;, &quot;age&quot;: 21, &quot;address&quot;: [ &#123;&quot;province&quot;: &quot;河北&quot;,&quot;city&quot;: &quot;廊坊&quot;,&quot;street&quot;: &quot;燕郊经济开发区&quot;&#125;, &#123;&quot;province&quot;: &quot;天津&quot;,&quot;city&quot;: &quot;天津&quot;,&quot;street&quot;: &quot;华夏路&quot;&#125; ]&#125;GET /user_index/_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;: [ &#123;&quot;match&quot;: &#123;&quot;address.province&quot;: &quot;北京&quot;&#125;&#125;, &#123;&quot;match&quot;: &#123;&quot;address.city&quot;: &quot;天津&quot;&#125;&#125; ] &#125; &#125;&#125; 可使用 nested object 作为 地址数组 的集体类型可解决上述问题,且搜索时需要 使用nested对应的搜索语法 123456789101112131415161718192021222324252627282930PUT /user_index&#123; &quot;mappings&quot;: &#123; &quot;properties&quot;: &#123; &quot;login_name&quot;: &#123;&quot;type&quot;: &quot;keyword&quot;&#125;, &quot;age &quot;: &#123;&quot;type&quot;: &quot;short&quot;&#125;, &quot;address&quot;: &#123; &quot;type&quot;: &quot;nested&quot;, &quot;properties&quot;: &#123; &quot;province&quot;: &#123;&quot;type&quot;: &quot;keyword&quot;&#125;, &quot;city&quot;: &#123;&quot;type&quot;: &quot;keyword&quot;&#125;, &quot;street&quot;: &#123;&quot;type&quot;: &quot;keyword&quot;&#125; &#125; &#125; &#125; &#125;&#125;GET /user_index/_search&#123; &quot;query&quot;: &#123;&quot;bool&quot;: &#123;&quot;must&quot;: [ &#123;&quot;nested&quot;: &#123;&quot;path&quot;: &quot;address&quot;, &quot;query&quot;: &#123; &quot;bool&quot;: &#123;&quot;must&quot;: [ &#123;&quot;match&quot;: &#123;&quot;address.province&quot;: &quot;北京&quot;&#125;&#125;, &#123;&quot;match&quot;: &#123;&quot;address.city&quot;: &quot;北京&quot;&#125;&#125; ]&#125; &#125;&#125; &#125;] &#125;&#125;&#125; 普通数组数据在ES中会被扁平化处理,nested object数据类型ES在保存时不会扁平化处理 1234567891011121314151617181920&#123; // 普通数组 &quot;login_name&quot; : &quot;jack&quot;, &quot;address.province&quot; : [ &quot;北京&quot;, &quot;天津&quot; ], &quot;address.city&quot; : [ &quot;北京&quot;, &quot;天津&quot; ] &quot;address.street&quot; : [ &quot;枫林三路&quot;, &quot;华夏路&quot; ]&#125;// nested数据&#123; &quot;login_name&quot; : &quot;jack&quot;&#125;&#123; &quot;address.province&quot; : &quot;北京&quot;, &quot;address.city&quot; : &quot;北京&quot;, &quot;address.street&quot; : &quot;枫林三路&quot;&#125;&#123; &quot;address.province&quot; : &quot;天津&quot;, &quot;address.city&quot; : &quot;天津&quot;, &quot;address.street&quot; : &quot;华夏路&quot;,&#125; nested object建模缺点是采取的是类似冗余数据的方式,将多个数据放在一起,维护成本比较高,每次更新需要重新索引整个对象,包括根对象和嵌套对象。ES提供 类似关系型数据库 中 Join 的实现,使用Join数据类型实现父子关系,从而分离两个文档对象。 更新父文档无需重新索引整个子文档,子文档被新增,更改和删除也不会影响到父文档和其他子文档,父子关系元数据映射,用于确保查询时高性能,但是有一个限制父子数据包括映射其关联关系的元数据必须存在于一个shard中,搜索父子关系数据时,不用跨分片性能高。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495PUT my_blogs&#123; &quot;mappings&quot;: &#123; &quot;properties&quot;: &#123; &quot;blog_comments_relation&quot;: &#123; &quot;type&quot;: &quot;join&quot;, // 指明join类型 &quot;relations&quot;: &#123; // 声明父子关系 &quot;blog&quot;: &quot;comment&quot; // blog为父文档名称,comment为子文档名称 &#125; &#125;, &quot;content&quot;: &#123;&quot;type&quot;: &quot;text&quot;&#125;, &quot;title&quot;: &#123;&quot;type&quot;: &quot;keyword&quot;&#125; &#125; &#125;&#125;PUT my_blogs/_doc/blog1 // blog1为父文档id&#123; &quot;title&quot;: &quot;Learning Elasticsearch&quot;, &quot;content&quot;: &quot;learning ELK is happy&quot;, &quot;blog_comments_relation&quot;: &#123; // 声明文档类型 &quot;name&quot;: &quot;blog&quot; &#125;&#125;PUT my_blogs/_doc/blog2 // blog2为父文档id&#123; &quot;title&quot;: &quot;Learning Hadoop&quot;, &quot;content&quot;: &quot;learning Hadoop&quot;, &quot;blog_comments_relation&quot;: &#123; // 声明文档类型 &quot;name&quot;: &quot;blog&quot; &#125;&#125;// 父文档和子文档必须存在相同的分片上, 当指定文档时候,必须指定它的父文档IDPUT my_blogs/_doc/comment1?routing=blog1 // 使用route参数来保证,分配到相同分片&#123; &quot;comment&quot;: &quot;I am learning ELK&quot;, &quot;username&quot;: &quot;Jack&quot;, &quot;blog_comments_relation&quot;: &#123; &quot;name&quot;: &quot;comment&quot;, &quot;parent&quot;: &quot;blog1&quot; &#125;&#125;PUT my_blogs/_doc/comment2?routing=blog2 // comment2为子文档id,blog2为父文档id&#123; &quot;comment&quot;: &quot;I like Hadoop!!!!!&quot;, &quot;username&quot;: &quot;Jack&quot;, &quot;blog_comments_relation&quot;: &#123; &quot;name&quot;: &quot;comment&quot;, &quot;parent&quot;: &quot;blog2&quot; &#125;&#125;PUT my_blogs/_doc/comment3?routing=blog2&#123; &quot;comment&quot;: &quot;Hello Hadoop&quot;, &quot;username&quot;: &quot;Bob&quot;, &quot;blog_comments_relation&quot;: &#123; &quot;name&quot;: &quot;comment&quot;, &quot;parent&quot;: &quot;blog2&quot; &#125;&#125;POST my_blogs/_search // 查询所有文档&#123;&#125;GET my_blogs/_doc/blog2 // 根据父文档ID查看POST my_blogs/_search // parent_id查询,返回所有相关子文档&#123; &quot;query&quot;: &#123; &quot;parent_id&quot;: &#123;&quot;type&quot;: &quot;comment&quot;,&quot;id&quot;: &quot;blog2&quot;&#125; &#125;&#125;POST my_blogs/_search // has_child查询,返回父文档&#123; &quot;query&quot;: &#123; &quot;has_child&quot;: &#123; &quot;type&quot;: &quot;comment&quot;, &quot;query&quot;: &#123;&quot;match&quot;: &#123;&quot;username&quot;: &quot;Jack&quot;&#125;&#125; &#125; &#125;&#125;POST my_blogs/_search // has_parent查询,返回相关的子文档&#123; &quot;query&quot;: &#123; &quot;has_parent&quot;: &#123; &quot;parent_type&quot;: &quot;blog&quot;, &quot;query&quot;: &#123;&quot;match&quot;: &#123;&quot;title&quot;: &quot;Learning Hadoop&quot;&#125;&#125; &#125; &#125;&#125;PUT my_blogs/_doc/comment3?routing=blog2 //更新子文档不会影响到父文档&#123; &quot;comment&quot;: &quot;Hello Hadoop??&quot;, &quot;blog_comments_relation&quot;: &#123; &quot;name&quot;: &quot;comment&quot;, &quot;parent&quot;: &quot;blog2&quot; &#125;&#125; 文件系统数据若需要使用 文件路径搜索 内容,只需要为其中的字段定义一个特殊的 path_hierarchy 分词器 123456789101112131415161718192021222324252627282930313233343536373839404142PUT /codes&#123; &quot;settings&quot;: &#123;&quot;analysis&quot;: &#123;&quot;analyzer&quot;: &#123;&quot;path_analyzer&quot;: &#123;&quot;tokenizer&quot;: &quot;path_hierarchy&quot;&#125;&#125;&#125;&#125;, &quot;mappings&quot;: &#123; &quot;properties&quot;: &#123; &quot;fileName&quot;: &#123;&quot;type&quot;: &quot;keyword&quot;&#125;, &quot;path&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;analyzer&quot;: &quot;path_analyzer&quot;, &quot;fields&quot;: &#123;&quot;keyword&quot;: &#123;&quot;type&quot;: &quot;text&quot;,&quot;analyzer&quot;: &quot;standard&quot;&#125;&#125; &#125;, &quot;content&quot;: &#123;&quot;type&quot;: &quot;text&quot;,&quot;analyzer&quot;: &quot;standard&quot;&#125; &#125; &#125;&#125;PUT /codes/_doc/1&#123; &quot;fileName&quot;: &quot;HelloWorld.java&quot;, &quot;path&quot;: &quot;/com/eleven/first&quot;, &quot;content&quot;: &quot;package com.eleven.first; public class HelloWorld &#123; // some code... &#125;&quot;&#125;GET /codes/_search&#123; &quot;query&quot;: &#123;&quot;match&quot;: &#123;&quot;path&quot;: &quot;/com&quot;&#125;&#125;&#125;GET /codes/_analyze&#123; &quot;text&quot;: &quot;/a/b/c/d&quot;, &quot;field&quot;: &quot;path&quot;&#125;GET /codes/_search&#123; &quot;query&quot;: &#123;&quot;match&quot;: &#123;&quot;path.keyword&quot;: &quot;/com&quot;&#125;&#125;&#125;GET /codes/_search&#123; &quot;query&quot;: &#123;&quot;bool&quot;: &#123;&quot;should&quot;: [ &#123;&quot;match&quot;: &#123;&quot;path&quot;: &quot;/com&quot;&#125;&#125;, &#123;&quot;match&quot;: &#123;&quot;path.keyword&quot;: &quot;/com/eleven&quot;&#125;&#125; ]&#125;&#125;&#125; Scroll分页使用 from 和 size 方式查询1W以内的数据都OK,但若数据比较多时会出现性能问题。ES做了一个限制 不允许查询1W条以后的数据 。若要查询1W条以后的数据,可使用ES中提供的 scroll游标 来查询。 在进行大量分页时,每次分页都需要将要查询数据进行重新排序,这样非常浪费性能。使用 scroll游标 是 将要用的数据一次性排序好, 然后 分批取出 。性能要比from + size好得多,使用scroll查询后,排序后的数据会保持一定的时间, 后续分页查询都从该快照取数据。响应结果中会返回_scroll_id,第二次查询直接使用_scroll_id来查询。 1234567891011121314GET /es_db/_search?scroll=1m // 让排序的数据保持1分钟&#123; &quot;query&quot;: &#123; &quot;multi_match&quot;: &#123; &quot;query&quot;: &quot;广州长沙张三&quot;, &quot;fields&quot;: [&quot;address&quot;,&quot;name&quot;] &#125; &#125;, &quot;size&quot;: 100&#125;GET _search/scroll?scroll=1m&#123; &quot;scroll_id&quot;: &quot;FGluY2x1ZGVfY29udGV4dF91dWlkDXF1ZXJ5QW5kRmV0Y2gBFnJKUnZmX1pIVGVpM05TWDBQX0JJeXcAAAAAAAaeghZDUkdZN1FJNVIwYUJhYUxvNWVxd1Rn&quot;&#125; SQL支持ES SQL允许执行类SQL查询,REST接口、命令行或 JDBC 等都可使用 SQL 来进行 数据检索 和 数据聚合 。特点： 本地集成：ES SQL是专门为ES构建的,每个SQL查询都根据底层存储对相关节点有效执行 无额外要求：不依赖其他硬件、进程、运行时库,可直接运行在ES集群上 轻量且高效：像SQL那样简洁、高效地完成查询1234567891011121314GET /es_db/_search?scroll=1m // 让排序的数据保持1分钟&#123; &quot;query&quot;: &#123; &quot;multi_match&quot;: &#123; &quot;query&quot;: &quot;广州长沙张三&quot;, &quot;fields&quot;: [&quot;address&quot;,&quot;name&quot;] &#125; &#125;, &quot;size&quot;: 100&#125;GET _search/scroll?scroll=1m&#123; &quot;scroll_id&quot;: &quot;FGluY2x1ZGVfY29udGV4dF91dWlkDXF1ZXJ5QW5kRmV0Y2gBFnJKUnZmX1pIVGVpM05TWDBQX0JJeXcAAAAAAAaeghZDUkdZN1FJNVIwYUJhYUxvNWVxd1Rn&quot;&#125; 目前 FROM只支持单表, 不支持JOIN、不支持较复杂的子查询,format 表示 指定返回数据类型, 支持的类型有 逗号分隔csv、json、制表符分隔符tsv、txt、yaml 。 12345678910111213141516GET /_sql?format=json&#123; &quot;query&quot;: &quot;SELECT * FROM es_db limit 1&quot;&#125;GET /_sql/translate // 将SQL转换为DSL&#123; &quot;query&quot;: &quot;SELECT * FROM es_db limit 1&quot;&#125;GET /_sql?format=json // field_exp匹配字段,constant_exp匹配常量表达式,&#123; // 检索address包含广州和name中包含张三的用户 &quot;query&quot;: &quot;select * from es_db where MATCH(address, &#x27;广州&#x27;) or MATCH(name, &#x27;张三&#x27;) limit 10&quot;&#125;GET /_sql?format=txt // 统计分组&#123; &quot;query&quot;: &quot;select age, count(*) as age_cnt from es_db group by age&quot;&#125; 模板模板搜索可将一些搜索进行模板化,每次执行该搜索就直接调用模板,传入一些参数即可。 123456789101112131415161718192021222324252627282930GET /cars/_search/template // 简单定义参数并传递&#123; &quot;source&quot;: &#123; &quot;query&quot;: &#123;&quot;match&quot;: &#123;&quot;remark&quot;: &quot;&#123;&#123;kw&#125;&#125;&quot;&#125;&#125;, &quot;size&quot;: &quot;&#123;&#123;size&#125;&#125;&quot; &#125;, &quot;params&quot;: &#123;&quot;kw&quot;: &quot;大众&quot;,&quot;size&quot;: 2&#125;&#125;GET cars/_search/template // toJson方式传递参数&#123; &quot;source&quot;: &quot;&quot;&quot;&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123;&#123;#toJson&#125;&#125;parameter&#123;&#123;/toJson&#125;&#125; &#125;&#125;&quot;&quot;&quot;, &quot;params&quot;: &#123; &quot;parameter&quot;: &#123;&quot;remark&quot;: &quot;大众&quot;&#125; &#125;&#125;GET cars/_search/template // json方式传递参数&#123; &quot;source&quot;: &#123;&quot;query&quot;: &#123;&quot;match&quot;: &#123; &quot;remark&quot;: &quot;&#123;&#123;#join delimiter=&#x27; &#x27;&#125;&#125;kw&#123;&#123;/join delimiter=&#x27; &#x27;&#125;&#125;&quot; &#125;&#125;&#125;, &quot;params&quot;: &#123;&quot;kw&quot;: [&quot;大众&quot;,&quot;标致&quot;]&#125;&#125;GET cars/_search/template&#123; &quot;source&quot;: &#123;&quot;query&quot;: &#123;&quot;range&quot;: &#123;&quot;price&quot;: &#123; &quot;gte&quot;: &quot;&#123;&#123;start&#125;&#125;&quot;, &quot;lte&quot;: &quot;&#123;&#123;end&#125;&#125;&#123;&#123;^end&#125;&#125;200000&#123;&#123;/end&#125;&#125;&quot; // 默认值定义 &#125;&#125;&#125;&#125;, &quot;params&quot;: &#123;&quot;start&quot;: 100000,&quot;end&quot;: 140000&#125;&#125; 记录template实现重复调用 可使用 Mustache 语言作为 搜索请求预处理, 它提供模板 通过键值对 来替换模板中的变量。把 脚本存储在本地磁盘中, 默认位置为 elasticsearch\\config\\scripts, 通过引用脚本名称进行使用 12345678910111213POST _scripts/test // test为脚本id&#123; &quot;script&quot;: &#123; &quot;lang&quot;: &quot;mustache&quot;, // 指定mustache语言 &quot;source&quot;: &#123;&quot;query&quot;: &#123;&quot;match&quot;: &#123;&quot;remark&quot;: &quot;&#123;&#123;kw&#125;&#125;&quot;&#125;&#125;&#125; &#125;&#125;GET cars/_search/template&#123; &quot;id&quot;: &quot;test&quot;, // 指定调用脚本的id &quot;params&quot;: &#123;&quot;kw&quot;: &quot;大众&quot;&#125;&#125;DELETE _scripts/test // 删除脚本id为test的脚本 suggest searchsuggest search(completion suggest) 即 建议搜索 或 搜索建议, 也可叫做自动完成,类似百度中搜索联想提示功能。ES实现 suggest时 性能非常高, 其构建的不是倒排索引也不是正排索引,是纯粹用于前缀搜索的一种特殊数据结构,且会全部放在内存中,所以suggest search进行前缀搜索提示性能是非常高。需要使用suggest时候,必须在定义index时为其mapping指定开启suggest 。 12345678910111213141516171819202122232425262728293031323334PUT /movie&#123; &quot;mappings&quot;: &#123; &quot;properties&quot;: &#123;&quot;title&quot;: &#123;&quot;type&quot;: &quot;text&quot;,&quot;analyzer&quot;: &quot;ik_max_word&quot;, &quot;fields&quot;: &#123;&quot;suggest&quot;: &#123;&quot;type&quot;: &quot;completion&quot;,&quot;analyzer&quot;: &quot;ik_max_word&quot;&#125;&#125; &#125;, &quot;content&quot;: &#123;&quot;type&quot;: &quot;text&quot;,&quot;analyzer&quot;: &quot;ik_max_word&quot;&#125; &#125; &#125;&#125;PUT /movie/_doc/1&#123; &quot;title&quot;: &quot;西游记电影系列&quot;, &quot;content&quot;: &quot;西游记之月光宝盒将与2021年进行......&quot;&#125;PUT /movie/_doc/2&#123; &quot;title&quot;: &quot;西游记文学系列&quot;, &quot;content&quot;: &quot;某知名网络小说作家已经完成了大话西游同名小说的出版&quot;&#125;PUT /movie/_doc/3&#123; &quot;title&quot;: &quot;西游记之大话西游手游&quot;, &quot;content&quot;: &quot;网易游戏近日出品了大话西游经典IP的手游,正在火爆内测中&quot;&#125;GET /movie/_search&#123; &quot;suggest&quot;: &#123; &quot;my-suggest&quot;: &#123; &quot;prefix&quot;: &quot;西游记&quot;, &quot;completion&quot;: &#123;&quot;field&quot;: &quot;title.suggest&quot;&#125; &#125; &#125;&#125; 地理位置搜索ES支持 地理位置搜索 和 聚合分析, 可实现 在指定区域内搜索数据、搜索指定地点附近的数据、聚合分析指定地点附近的数据 等操作。ES中若使用地理位置搜索,必须提供一个特殊的字段类型 geo_point, 用于指定地理位置坐标点。 新增一个基于 geo_point 类型数据,可使用多种方式。多种类型描述 geo_point 类型字段时,在 搜索数据时显示格式和录入格式是统一的 。 任何数据描述 的 geo_point 类型字段, 都适用地理位置搜索 。 数据范围要求 纬度范围 是-90~90之间, 经度范围 是-180~180之间,经纬度数据都是 浮点数 或 数字串, 最大精度为 小数点后7位 。 latitude ： 纬度 、 longitude ： 经度 。 123456789101112131415161718192021222324PUT /hotel_app&#123; &quot;mappings&quot;: &#123; &quot;properties&quot;: &#123; &quot;pin&quot;: &#123;&quot;type&quot;: &quot;geo_point&quot;&#125;, &quot;name&quot;: &#123;&quot;type&quot;: &quot;text&quot;,&quot;analyzer&quot;: &quot;ik_max_word&quot;&#125; &#125; &#125;&#125;PUT /hotel_app/_doc/1&#123; &quot;name&quot;: &quot;七天连锁酒店&quot;, &quot;pin&quot;: &#123;&quot;lat&quot;: 40.12,&quot;lon&quot;: -71.34&#125;&#125;PUT /hotel_app/_doc/2&#123; &quot;name&quot;: &quot;维多利亚大酒店&quot;, &quot;pin&quot;: &quot;40.99, -70.81&quot;&#125;PUT /hotel_app/_doc/3&#123; &quot;name&quot;: &quot; 红树林宾馆&quot;, &quot;pin&quot;: [40,-73.81] // 基于数组：依次定义经度、纬度,不推荐使用&#125; 矩形范围搜索 传入 top_left 和 bottom_right 坐标点是有固定要求的, top_left 即 从西北向东南, Bottom_right 即从东南向西北, 且 top_left纬度应大于bottom_right, top_left经度应小于bottom_right 。多边形范围搜索 对传入若干点坐标顺序没有任何要求,只要传入若干地理位置坐标点,即可形成多边形。 12345678910111213141516171819202122232425GET /hotel_app/_doc/_search // 矩形搜索&#123; &quot;query&quot;: &#123; &quot;geo_bounding_box&quot;: &#123; &quot;pin&quot;: &#123; &quot;top_left&quot;: &#123;&quot;lat&quot;: 41.73,&quot;lon&quot;: -74.1&#125;, &quot;bottom_right&quot;: &#123;&quot;lat&quot;: 40.01,&quot;lon&quot;: -70.12&#125; &#125; &#125; &#125;&#125;GET /hotel_app/_doc/_search // 多边形搜索&#123; &quot;query&quot;: &#123; &quot;geo_polygon&quot;: &#123; &quot;pin&quot;: &#123; &quot;points&quot;: [ &#123;&quot;lat&quot;: 40.73,&quot;lon&quot;: -74.1&#125;, &#123;&quot;lat&quot;: 40.01,&quot;lon&quot;: -71.12&#125;, &#123;&quot;lat&quot;: 50.56,&quot;lon&quot;: -90.58&#125; ] &#125; &#125; &#125;&#125; Distance距离的单位,常用米m和千米km,建议使用 filter 来过滤 geo_point 数据,因为 geo_point 数据相关度评分计算比较耗时。使用query来搜索geo_point数据效率相对会慢一些。 12345678910111213141516171819GET /hotel_app/_doc/_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;filter&quot;: &#123; &quot;geo_distance&quot;: &#123;&quot;distance&quot;: &quot;200km&quot;,&quot;pin&quot;: &#123;&quot;lat&quot;: 40,&quot;lon&quot;: -70&#125;&#125; &#125; &#125; &#125;&#125;GET hotel_app/_search&#123; &quot;query&quot;: &#123; &quot;geo_distance&quot;: &#123; &quot;distance&quot;: &quot;90km&quot;, &quot;pin&quot;: &#123;&quot;lat&quot;: 40.55,&quot;lon&quot;: -71.12&#125; &#125; &#125;&#125; 聚合统计某位置附近区域内的数据,unit是距离单位,常用单位有米m,千米km,英里mi,distance_type是统计算法：sloppy_arc默认算法、 arc最高精度 、 plane最高效率 。 12345678910111213141516171819GET /hotel_app/_doc/_search&#123; &quot;size&quot;: 0, &quot;aggs&quot;: &#123; &quot;agg_by_pin&quot;: &#123; &quot;geo_distance&quot;: &#123; &quot;distance_type&quot;: &quot;arc&quot;, &quot;field&quot;: &quot;pin&quot;, &quot;origin&quot;: &#123;&quot;lat&quot;: 40,&quot;lon&quot;: -70&#125;, &quot;unit&quot;: &quot;mi&quot;, &quot;ranges&quot;: [ // 聚合统计分别距离某位置80英里,300英里,1000英里范围内的数据数量 &#123;&quot;to&quot;: 80&#125;, &#123;&quot;from&quot;: 80,&quot;to&quot;: 300&#125;, &#123;&quot;from&quot;: 300,&quot;to&quot;: 1000&#125; ] &#125; &#125; &#125;&#125; ElasticSearch: 进一步学习Elasticsearch 官方学习文档：https://www.elastic.co/guide/en/elasticsearch/reference/current/index.htmlJava API:https://www.elastic.co/guide/en/elasticsearch/client/java-api/current/java-api.html","tags":[{"name":"ElasticSearch","slug":"ElasticSearch","permalink":"http://example.com/tags/ElasticSearch/"}],"categories":[{"name":"工具和中间件","slug":"工具和中间件","permalink":"http://example.com/categories/%E5%B7%A5%E5%85%B7%E5%92%8C%E4%B8%AD%E9%97%B4%E4%BB%B6/"},{"name":"搜索引擎技术","slug":"工具和中间件/搜索引擎技术","permalink":"http://example.com/categories/%E5%B7%A5%E5%85%B7%E5%92%8C%E4%B8%AD%E9%97%B4%E4%BB%B6/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E6%8A%80%E6%9C%AF/"},{"name":"ElasticSearch","slug":"工具和中间件/搜索引擎技术/ElasticSearch","permalink":"http://example.com/categories/%E5%B7%A5%E5%85%B7%E5%92%8C%E4%B8%AD%E9%97%B4%E4%BB%B6/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E6%8A%80%E6%9C%AF/ElasticSearch/"}]},{"title":"ElasticSearch基础","date":"2022-01-01T16:00:00.000Z","path":"blog/工具和中间件/搜索引擎技术/ElasticSearch/ElasticSearch基础/","text":"Elasticsearch是什么？ Elasticsearch 是用 Java开发 的当前 最流行 的 开源 的企业级搜索引擎,能够达到 实时搜索, 稳定, 可靠, 快速,安装使用方便。 和Solr一样的,Elasticsearch 是基于 Lucene 进行了封装, 提供了更为便利的访问和调用, Lucene可被认为是迄今为止最先进、性能最好、功能最全的搜索引擎框架。 ES与Solr对比：单纯对已有数据进行搜索时 Solr更快,当实时建立索引时Solr会产生 IO阻塞,查询性能较差,该情况下 Elasticsearch 具有明显优势。 Solr利用Zookeeper进行分布式管理,而 Elasticsearch自带分布式协调管理功能 Solr支持更多格式数据,如JSON、XML、CSV,而 Elasticsearch仅支持JSON文件格式 Solr在传统搜索应用中表现好于Elasticsearch,但在处理实时搜索应用时效率明显低于Elasticsearch Solr是传统搜索应用的有力解决方案,但Elasticsearch更适用于新兴实时搜索应用。 ES与关系型数据库: 关系型数据库 Database数据库 Table表 ROW行 Column列 Elasticsearch Index索引库 Type类型 Document文档 Field字段 ES核心概念: 索引index:一个索引就是一个拥有几分相似特征的文档集合,相当于关系型数据库中的database,一个索引由一个名字来标识, 必须全部是小写字母,且当要对对应于该索引中的文档进行索引搜索、更新和删除时,都要使用该名字。 Mapping映射:ElasticSearch中的Mapping映射用来定义一个文档,Mapping是处理数据的方式和规则方面做一些限制,如某个字段的数据类型、默认值、分词器、是否被索引等,这都是映射里面可设置的。 Field字段:相当于是数据表的字段或列。 Type字段类型：每个字段都应该有一个对应的类型,如 Text、Keyword、Byte 等。 Document文档：一个文档是一个可被索引的基础信息单元,类似一条记录, 文档以JSON格式来表示。 Cluster集群：一个集群由一个或多个节点组织在一起, 共同持有整个数据,并一起提供索引和搜索功能 Node节点：一个节点即集群中一个服务器,作为集群的一部分,它存储数据,参与集群的索引和搜索功能,一个节点可通过配置集群名称的方式来加入一个指定的集群。默认每个节点都会被安排加入到一个叫做 elasticsearch的集群中。一个集群中可拥有任意多个节点,且若当前网络中没有运行任何Elasticsearch节点,这时启动一个节点,会默认创建并加入一个叫做 elasticsearch 的集群。 分片：一个索引可存储超出单个结点硬件限制的大量数据,如一个具有10亿文档的索引占据1TB磁盘空间,而任一节点都没有这样大的磁盘空间,或者单个节点处理搜索请求,响应太慢,为了解决这个问题,Elasticsearch提供了将索引划分成多份的能力,每一份就是一个分片。当创建索引时可指定分片数量, 每个分片本身也是一个功能完善且独立的索引,该分片可被放置到集群中任何节点上, 分片允许水平分割扩展内容容量,允许在分片之上进行分布式并行操作,进而提高性能和吞吐量,每个分片怎样分布, 文档怎样聚合回搜索请求,完全由Elasticsearch管理,对于用户透明 副本：在一个网络环境中,失败随时都可能发生,在某个分片或节点处于离线状态,或由于任何原因消失,该情况下有一个故障转移机制是非常有用且强烈推荐。为此 Elasticsearch允许创建分片的一份或多份拷贝,这些拷贝叫做副本分片或直接叫副本。扩展搜索量和吞吐量,搜索可在所有的副本上并行运行, 每个索引可被分成多个分片, 一个索引有零个或者多个副本, 一旦设置了副本,每个索引就有了主分片和副本分片, 分片和副本数量可在索引创建时指定,在索引创建后, 可在任何时候动态地改变副本数量,但不能改变分片数量。 ES安装注： ES不能使用root用户来启动,必须使用普通用户来安装启动。 12345678910groupadd elasticsearch # 创建elasticsearch用户组useradd eleven # 创建eleven用户passwd eleven # 给eleven用户设置密码为elevenusermod -G elasticsearch eleven # 将用户eleven添加到elasticsearch用户组mkdir -p /usr/local/es # 创建es文件夹chown -R eleven /usr/local/es/elasticsearch-7.6.1 # 修改owner为eleven用户visudo # 使用root用户执行visudo命令然后为es用户添加权限eleven ALL=(ALL) ALL # 在root ALL=(ALL) ALL 一行下面添加eleven用户 修改elasticsearch.yml,可通过修改jvm.options配置文件调整JVM参数: 123456789101112cluster.name: eleven-es # 集群名称node.name: node1 # 节点名称path.data: /usr/local/es/elasticsearch-7.6.1/data # 数据目录path.logs: /usr/local/es/elasticsearch-7.6.1/log # 日志目录network.host: 0.0.0.0http.port: 9200discovery.seed_hosts: [&quot;IP1&quot;, &quot;IP2&quot;, &quot;IP3&quot;]cluster.initial_master_nodes: [&quot;节点1名称&quot;, &quot;节点2名称&quot;, &quot;节点3名称&quot;]bootstrap.system_call_filter: falsebootstrap.memory_lock: falsehttp.cors.enabled: truehttp.cors.allow-origin: &quot;*&quot; ES需要大量创建索引文件,需要大量打开系统文件,所以需要解除linux系统当中打开文件最大数目限制,不然ES启动会抛错：max file descriptors [4096] for elasticsearch process likely too low, increase to at least [65536] 12345sudo vim /etc/security/limits.conf* soft nofile 65536* hard nofile 131072* soft nproc 2048* hard nproc 4096 若出现max number of threads [1024] for user [es] likely too low, increase to at least [4096] 错误信息,是由于普通用户启动线程数限制最大可创建线程数太小,无法创建本地线程问题。 安装IK分词器使用ElasticSearch来进行中文分词,需要单独给Elasticsearch安装IK分词器插件, 123mkdir -p /usr/local/es/elasticsearch-7.6.1/plugins/ikcd /usr/local/es/elasticsearch-7.6.1/plugins/ikunzip elasticsearch-analysis-ik-7.6.1.zip ES的默认分词设置是 standard单字拆分,可使用 IK分词器 的 ik_smart 和 ik_max_word 分词方式, ik_smart 会做 最粗粒度拆分, ik_max_word会将文本做最细粒度拆分。修改默认分词方法,修改 eleven_index索引的默认分词为 ik_max_word 1234567891011121314151617181920212223PUT /school_index&#123; &quot;settings&quot;: &#123; &quot;index&quot;: &#123; &quot;analysis.analyzer.default.type&quot;: &quot;ik_max_word&quot; &#125; &#125;&#125;POST _analyze&#123; &quot;analyzer&quot;: &quot;standard&quot;, &quot;text&quot;: &quot;中华人民共和国&quot;&#125;POST _analyze&#123; &quot;analyzer&quot;: &quot;ik_smart&quot;, &quot;text&quot;: &quot;中华人民共和国&quot;&#125;POST _analyze&#123; &quot;analyzer&quot;: &quot;ik_max_word&quot;, &quot;text&quot;: &quot;中华人民共和国&quot;&#125; ES基础ES是面向文档Document的, 使用JSON作为文档序列化格式,这其可存储整个对象或文档Document,不仅仅是存储,还会索引index每个文档内容使之可被搜索。ES中可对文档而非成行成列的数据进行索引、搜索、排序、过滤。 条件查询：GET /索引名称/类型/_search?q=字段1:字段值,字段2:字段值,条件之间是通过逗号分隔多个条件,如分页、排序、输出指定字段等通过 &amp;符号分隔 123456789101112131415161718192021222324252627GET _cat/nodes?v // 查看集群节点状态GET _cat/health?v // 查看集群健康状态GET /es_db // 查询索引：GET /索引名称PUT /es_db // 创建索引：PUT /索引名称DELETE /es_db // 删除索引：DELETE /索引名称PUT /es_db/_doc/1 // 添加文档：PUT /索引名称/类型/id&#123; &quot;name&quot;: &quot;张三&quot;, &quot;sex&quot;: 1, &quot;age&quot;: 25, &quot;address&quot;: &quot;广州天河公园&quot;, &quot;remark&quot;: &quot;java developer&quot;&#125;GET /es_db/_doc/1 // 查询文档：GET /索引名称/类型/idDELETE /es_db/_doc/1 // 删除文档：DELETE /索引名称/类型/idGET /es_db/_doc/_search // 查询当前类型中的所有文档：GET /索引名称/类型/_searchGET /es_db/_doc/_search?q=age:28 // 条件查询：GET /索引名称/类型/_search?q=*:***GET /es_db/_doc/_search?q=age[25 TO 26] // 范围查询：GET /索引名称/类型/_search?q=***[** TO **]GET /es_db/_doc/_mget // 根据多个ID进行批量查询：GET /索引名称/类型/_mget&#123;&quot;ids&quot;:[&quot;1&quot;,&quot;2&quot;]&#125;GET /es_db/_doc/_search?q=age:&lt;=28 // 查询小于等于：GET /索引名称/类型/_search?q=age:&lt;=**GET /es_db/_doc/_search?q=age:&gt;=28 // 查询大于等于：GET /索引名称/类型/_search?q=age:&gt;=**GET /es_db/_doc/_search?q=age[25 TO 26]&amp;from=0&amp;size=1 // 分页查询：from=*&amp;size=*GET /es_db/_doc/_search?_source=name,age // 对查询结果只输出某些字段：_search?_source=字段,字段GET /es_db/_doc/_search?q=age[25 TO 26],sex:0 // 多条件查询GET /es_db/_doc/_search?sort=age:desc // 对查询结果排序sort=字段:desc/asc ES是基于Restful API和所有客户端交互都是使用JSON格式数据,其他所有程序语言都可使用RESTful API,通过9200端口的与ES进行通信,GET查询、PUT添加、POST修改、DELETE删除, POST和PUT都能起到创建&#x2F;更新的作用： PUT需要对一个具体的资源进行操作,也就是要确定id才能进行更新&#x2F;创建,而 POST可针对整个资源集合进行操作,若不写id则由ES生成一个唯一id进行创建新文档,过填了id则针对该id文档进行创建&#x2F;更新 PUT会将JSON数据都进行替换,POST只会更新相同字段的值 PUT与DELETE都是幂等性操作,不论操作多少次结果都一样 文档批量操作通过 _mget 的API来实现 批量操作多个文档,可通过 _id 批量获取 不同index和type的数据,若查询的是同一个文档可将index和type放到URL上。且可 通过_source指定查询字段。 1234567891011121314151617181920212223242526GET _mget&#123; &quot;docs&quot;: [ &#123; &quot;_index&quot;: &quot;es_db_1&quot;, &quot;_type&quot;: &quot;_doc&quot;, &quot;_id&quot;: 1, &#125;, &#123; &quot;_index&quot;: &quot;es_db&quot;, &quot;_type&quot;: &quot;_doc&quot;, &quot;_id&quot;: 2 &#125; ]&#125;GET /es_db/_doc/_mget?_source=age,name&#123; &quot;docs&quot;: [ &#123; &quot;_id&quot;: 1 &#125;, &#123; &quot;_id&quot;: 2 &#125; ]&#125; 批量 对文档进行 写操作 是通过 _bulk 的API来实现的,通过 _bulk 写操作文档,一般至少有两行参数,第一行参数为指定 操作的类型 及 操作的对象 如index、type、id,第二行参数为 操作的数据. actionName 表示 操作类型, 主要有 create、index、delete、update 。 1234567891011&#123; &quot;actionName&quot;: &#123; &quot;_index&quot;: &quot;indexName&quot;, &quot;_type&quot;: &quot;typeName&quot;, &quot;_id&quot;: &quot;id&quot; &#125;&#125;&#123; &quot;field1&quot;: &quot;value1&quot;, &quot;field2&quot;: &quot;value2&quot;&#125; 1234567891011&#123; &quot;actionName&quot;: &#123; &quot;_index&quot;: &quot;indexName&quot;, &quot;_type&quot;: &quot;typeName&quot;, &quot;_id&quot;: &quot;id&quot; &#125;&#125;&#123; &quot;field1&quot;: &quot;value1&quot;, &quot;field2&quot;: &quot;value2&quot;&#125; 乐观并发控制在数据库领域中,有悲观并发控制和乐观并发控制两种方法来确保并发更新不丢失数据, 悲观并发控制被关系型数据库广泛使用,阻塞访问资源以防止冲突；ES使用乐观并发控制,若源数据在读写当中被修改,更新将会失败。 12345678PUT /db_index/_doc/1?if_seq_no=1&amp;if_primary_term=1&#123; &quot;name&quot;: &quot;Jack&quot;, &quot;sex&quot;: 1, &quot;age&quot;: 25, &quot;book&quot;: &quot;Spring Boot 入门到精通2&quot;, &quot;remark&quot;: &quot;hello world2&quot;&#125; ES老版本是使用 version字段来乐观并发控制,新版本7.x使用if_seq_no&#x3D;文档版本号&amp;if_primary_term&#x3D;文档位置来乐观并发控制。 每当Primary Shard发生重新分配时如 重启, Primary选举 等, _primary_term会递增1, _primary_term 主要是用来 恢复数据时 处理当多个文档的 _seq_no一样 时的冲突. 如当一个shard宕机了,raplica需要用到最新的数据,就会根据_primary_term和_seq_no两个值来拿到最新的document。 文档映射ES中映射可以分为动态映射和静态映射,在关系数据库中,需要事先在数据库下创建数据表,并创建表字段、类型、长度、主键等,最后才能基于表插入数据。而Elasticsearch中不需要定义Mapping映射,在文档写入ES时,会根据文档字段自动识别类型,该机制为动态映射；也可事先定义好映射,包含文档的各字段类型、分词器等,该方式为静态映射 字符串： string类型包含text和keyword text： 该类型被用来索引长文本,创建索引前会将文本进行分词,转化为词的组合,建立索引；允许es来检索这些词, 不能用来排序和聚合 keyword： 该类型不能分词,可被用来检索过滤、排序和聚合, 不可用text进行分词模糊检索 数值型： long、integer、short、byte、double、float 日期型： date 布尔型： boolean 123456789101112131415161718192021222324252627282930313233343536GET /es_db/_mapping // 获取文档映射 PUT /es_db2 // 创建索引且设置文档映射&#123; &quot;mappings&quot;: &#123; &quot;properties&quot;: &#123; &quot;name&quot;: &#123; &quot;type&quot;: &quot;keyword&quot;, &quot;index&quot;: true, &quot;store&quot;: true &#125;, &quot;sex&quot;: &#123; &quot;type&quot;: &quot;integer&quot;, &quot;index&quot;: true, &quot;store&quot;: true &#125;, &quot;age&quot;: &#123; &quot;type&quot;: &quot;integer&quot;, &quot;index&quot;: true, &quot;store&quot;: true &#125;, &quot;book&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;index&quot;: true, &quot;store&quot;: true, &quot;analyzer&quot;: &quot;ik_smart&quot;, // 指定text类型的ik分词器 &quot;search_analyzer&quot;: &quot;ik_smart&quot; // 指定text类型的ik分词器 &#125;, &quot;address&quot;: &#123; &quot;type&quot;: &quot;text&quot;, &quot;index&quot;: true, &quot;store&quot;: true &#125; &#125; &#125;&#125; 若要推倒现有的映射,得重新建立一个静态索引,然后把之前索引里的数据导入到新的索引里, 删除原创建的索引, 为新索引起个别名,为原索引名。 1234567891011POST _reindex // 把之前索引里的数据导入到新的索引里&#123; &quot;source&quot;: &#123; &quot;index&quot;: &quot;db_index&quot; &#125;, &quot;dest&quot;: &#123; &quot;index&quot;: &quot;db_index_2&quot; &#125;&#125;DELETE /db_index // 删除原创建的索引PUT /db_index_2/_alias/db_index // 为新索引起个别名, 为原索引名 若要推倒现有的映射,得重新建立一个静态索引,然后把之前索引里的数据导入到新的索引里, 删除原创建的索引, 为新索引起个别名,为原索引名。 1234567891011POST _reindex // 把之前索引里的数据导入到新的索引里&#123; &quot;source&quot;: &#123; &quot;index&quot;: &quot;db_index&quot; &#125;, &quot;dest&quot;: &#123; &quot;index&quot;: &quot;db_index_2&quot; &#125;&#125;DELETE /db_index // 删除原创建的索引PUT /db_index_2/_alias/db_index // 为新索引起个别名, 为原索引名 DSL高级查询Domain Specific Language领域专用语言,由叶子查询子句和复合查询子句两种子句组成。DSL查询语言又分为查询DSL和过滤DSL。ES中索引的数据都会存储一个 _score分值, 分值越高就代表越匹配, 查询上下文中不仅要判断查询条件与文档是否匹配,且还要关心相关度即 _score分值,需要根据分值排序；过滤器上下文中值关心查询条件与文档是否匹配,不计算 _score分值, 不关心排序问题,经常使用过滤器,ES会自动缓存过滤器内容。 12GET /es_db/_doc/_search // 无查询条件是查询所有,默认查询所有,或使用match_all表示所有&#123;&quot;query&quot;:&#123;&quot;match_all&quot;:&#123;&#125;&#125;&#125; 叶子查询模糊匹配模糊匹配主要是针对文本类型的字段,文本类型的字段会对内容进行分词, 查询时也会对搜索条件进行分词,然后通过倒排索引查找到匹配数据,模糊匹配主要通过 match 等参数来实现 match：通过match关键词模糊匹配条件内容, 需指定字段名, 会进行分词 query：指定匹配的值 operator：匹配条件类型 and：条件分词后都要匹配 or：条件分词后有一个匹配即可,默认为or minmum_should_match：指定最小匹配数量 query_string：和match类似, 可不指定字段即所有字段中搜索,范围更广泛 match_phase：会对输入做分词,但结果中也包含所有分词,且顺序一样 prefix：前缀匹配 regexp：通过正则表达式来匹配数据 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566POST /es_db/_doc/_search&#123; &quot;from&quot;: 0, &quot;size&quot;: 2, &quot;query&quot;: &#123; &quot;match&quot;: &#123; // match会根据该字段的分词器,进行分词查询 &quot;address&quot;: &quot;广州&quot; &#125; &#125;&#125;POST /es_db/_doc/_search&#123; &quot;query&quot;: &#123; &quot;multi_match&quot;: &#123; // 多字段模糊匹配查询 &quot;query&quot;: &quot;长沙&quot;, &quot;fields&quot;: [&quot;address&quot;, &quot;name&quot;] // address或name字段中匹配到“长沙” &#125; &#125;&#125;POST /es_db/_doc/_search&#123; &quot;query&quot;: &#123; &quot;query_string&quot;: &#123; // 未指定字段条件查询query_string, 含AND与OR条件 &quot;query&quot;: &quot;广州 OR 长沙&quot; // 所有的字段中只要包含“广州”或“长沙” &#125; &#125;&#125;POST /es_db/_doc/_search&#123; &quot;query&quot;: &#123; &quot;query_string&quot;: &#123; // 指定字段条件查询query_string &quot;query&quot;: &quot;admin AND 长沙&quot;, &quot;fields&quot;: [&quot;name&quot;, &quot;address&quot;] // name或address匹配admin和长沙 &#125; &#125;&#125;GET /es_db/_search&#123; &quot;query&quot;: &#123; // ES执行搜索时,默认operator为or &quot;match&quot;: &#123; // remark字段包含java或developer词组,则符合搜索条件。 &quot;remark&quot;: &quot;java developer&quot; &#125; &#125;&#125;GET /es_db/_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;remark&quot;: &#123; // remark字段包含java和developer词组 &quot;query&quot;: &quot;java developer&quot;, &quot;operator&quot;: &quot;and&quot; &#125; &#125; &#125;&#125;GET /es_db/_search&#123; &quot;query&quot;: &#123; &quot;match&quot;: &#123; &quot;remark&quot;: &#123; // 需要remark字段中包含多个搜索词条中的一定比例 &quot;query&quot;: &quot;java architect assistant&quot;, &quot;minimum_should_match&quot;: &quot;50%&quot; // minimum_should_match可使用百分比或固定数字 &#125; &#125; &#125;&#125; match_phrase短语搜索,使用短语搜索时和match类似,首先对搜索条件进行分词,ES在做分词时除了将数据切分外,还会保留一个词在整个数据中的下标position。当ES执行match phrase短语搜索时,首先将搜索条件分词,然后在倒排索引中检索数据,若搜索条件分词数据在某个document某个field出现时,则检查匹配到的单词的position是否连续,若不连续则匹配失败。 ES对match phrase短语搜索提供了 slop参数,可实现数据在所有匹配结果中,多个单词距离越近相关度评分越高排序越靠前,若当 slop 移动次数使用完毕还没有匹配成功则无搜索结果。 12345678910111213141516171819GET _search&#123; &quot;query&quot;: &#123; &quot;match_phrase&quot;: &#123; // 短语搜索,搜索条件不分词 &quot;remark&quot;: &quot;java assistant&quot; &#125; &#125;&#125;GET /es_db/_search&#123; &quot;query&quot;: &#123; &quot;match_phrase&quot;: &#123; &quot;remark&quot;: &#123; &quot;query&quot;: &quot;java assistant&quot;, &quot;slop&quot;: 1 &#125; &#125; &#125;&#125; 前缀搜索通常针对 keyword 类型字段即不分词字段, keyword类型字段数据大小写敏感, 前缀搜索效率比较低,且不计算相关度分数, 前缀越短效率越低。若使用前缀搜索,建议使用长前缀,因为前缀搜索需要扫描完整索引内容,所以前缀越长相对效率越高。 12345678GET /test_a/_search&#123; &quot;query&quot;: &#123; &quot;prefix&quot;: &#123; &quot;f.keyword&quot;: &#123;&quot;value&quot;: &quot;Jav&quot;&#125; &#125; &#125;&#125; 通配符搜索通配符可在倒排索引中使用,也可在 keyword类型字段中使用。?问号匹配一个任意字符, *星号匹配0到n个任意字符。性能也很低,也需要扫描完整索引。 12345678GET /test_a/_search&#123; &quot;query&quot;: &#123; &quot;wildcard&quot;: &#123; &quot;f.keyword&quot;: &#123; &quot;value&quot;: &quot;?e*o*&quot; &#125; &#125; &#125;&#125; 正则搜索可在 倒排索引 或 keyword 类型字段中使用, 性能很低需要扫描完整索引。 123456GET /test_a/_search&#123; &quot;query&quot;: &#123; &quot;regexp&quot;: &#123;&quot;f.keyword&quot;: &quot;[A-z].+&quot;&#125; &#125;&#125; 搜索推荐其原理和 match phrase类似,先使用match匹配term数据即示例中的java,然后在指定 slop移动次数范围内前缀匹配示例数据sp, max_expansions是用于指定prefix最多匹配多少个term,超过该数量就不再匹配了。该语法限制只有最后一个term会执行前缀搜索。执行性能很差, 最后一个term 需要 扫描所有符合slop要求的倒排索引的term. 若必须使用一定要使用参数 max_expansions. 12345678GET /test_a/_search&#123; &quot;query&quot;: &#123; &quot;match_phrase_prefix&quot;: &#123; &quot;f&quot;: &#123;&quot;query&quot;: &quot;java sp&quot;,&quot;slop&quot;: 10,&quot;max_expansions&quot;: 10&#125; &#125; &#125;&#125; 模糊搜索搜索时可能搜索条件文本输入错误,fuzzy技术就是用于解决错误拼写的,英文中很有效但中文中几乎无效,其中 fuzziness 代表 value值word可修改多少个字母来进行拼写错误纠正,修改字母数量包含字母变更,增加或减少字母. 12345678GET /test_a/_search&#123; &quot;query&quot;: &#123; &quot;fuzzy&quot;: &#123; &quot;f&quot;: &#123;&quot;value&quot;: &quot;word&quot;,&quot;fuzziness&quot;: 2&#125; &#125; &#125;&#125; 精确匹配 term： 单个条件相等,查询字段映射类型属于为 keyword, 不会被分词 terms： 单个字段属于某个值数组内的值 range： 字段属于某个范围内的值 gte： 大于等于 lte： 小于等于 gt： 大于 lt： 小于 now： 当前时间 exists： 某个字段的值是否存在 ids： 通过ID批量查询 12345678910111213141516171819202122232425262728POST /es_db/_doc/_search&#123; &quot;query&quot;: &#123; &quot;term&quot;: &#123; // term查询不会对字段进行分词查询,会采用精确匹配 &quot;name&quot;: &quot;admin&quot; &#125; &#125;&#125;POST /es_db/_doc/_search&#123; &quot;query&quot;: &#123; &quot;range&quot;: &#123; // 范围查询 &quot;age&quot;: &#123;&quot;gte&quot;: 25,&quot;lte&quot;: 28&#125; &#125; &#125;&#125;POST /es_db/_doc/_search // 范围、分页、输出字段、综合查询&#123; &quot;query&quot;: &#123; &quot;range&quot;: &#123; // 范围查询 &quot;age&quot;: &#123;&quot;gte&quot;: 25,&quot;lte&quot;: 28&#125; &#125; &#125;, &quot;from&quot;: 0, // 分页 &quot;size&quot;: 2, &quot;_source&quot;: [&quot;name&quot;, &quot;age&quot;, &quot;book&quot;], // 指定输出字段 &quot;sort&quot;: &#123;&quot;age&quot;: &quot;desc&quot;&#125;// 排序&#125; 组合查询组合条件查询是将叶子条件查询语句进行组合而形成的一个完整的查询条件, must、filter、shoud、must_not等子条件是通过 term、terms、range、ids、exists、match等叶子条件为参数,当只有一个搜索条件时,must等对应的是一个对象,当多个条件时,对应的是一个数组。 bool： 各条件之间有 and, or 或 not 关系 must： 各个条件都必须满足,即各条件是 and 关系 should： 各个条件有一个满足即可,即各条件是 or 关系 must_not： 不满足所有条件,即各条件是 not 关系 filter： 不计算相关度评分,即不计算_score, 不对结果排序,效率更高, 查询结果可被缓存 constant_score： 不计算相关度评分 1234567891011121314151617181920212223POST /es_db/_doc/_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;filter&quot;: &#123; // 对数据进行过滤 &quot;term&quot;: &#123;&quot;age&quot;: 25&#125; &#125; &#125; &#125;&#125; GET /es_db/_search&#123; &quot;query&quot;: &#123; // 使用should+bool搜索,控制搜索条件的匹配度 &quot;bool&quot;: &#123; &quot;should&quot;: [ // 必须匹配java、developer、assistant三个词条中的至少2个 &#123;&quot;match&quot;: &#123;&quot;remark&quot;: &quot;java&quot;&#125;&#125;, &#123;&quot;match&quot;: &#123;&quot;remark&quot;: &quot;developer&quot;&#125;&#125;, &#123;&quot;match&quot;: &#123;&quot;remark&quot;: &quot;assistant&quot;&#125;&#125; ], &quot;minimum_should_match&quot;: 2 // 控制搜索条件的匹配度 &#125; &#125;&#125; ES中执行 match搜索 时,ES底层通常会对搜索条件进行底层转换,来实现最终的搜索结果,若不怕麻烦, 尽量使用转换后的语法执行搜索, 效率更高。 123456789101112131415GET /es_db/_search // 转换前&#123;&quot;query&quot;:&#123;&quot;match&quot;:&#123;&quot;remark&quot;:&quot;java developer&quot;&#125;&#125;&#125;GET /es_db/_search // 转换后&#123;&quot;query&quot;:&#123;&quot;bool&quot;:&#123;&quot;should&quot;:[&#123;&quot;term&quot;:&#123;&quot;remark&quot;:&quot;java&quot;&#125;&#125;,&#123;&quot;term&quot;:&#123;&quot;remark&quot;:&#123;&quot;value&quot;:&quot;developer&quot;&#125;&#125;&#125;]&#125;&#125;&#125;GET /es_db/_search // 转换前&#123;&quot;query&quot;:&#123;&quot;match&quot;:&#123;&quot;remark&quot;:&#123;&quot;query&quot;:&quot;java developer&quot;,&quot;operator&quot;:&quot;and&quot;&#125;&#125;&#125;&#125;GET /es_db/_search // 转换后&#123;&quot;query&quot;:&#123;&quot;bool&quot;:&#123;&quot;must&quot;:[&#123;&quot;term&quot;:&#123;&quot;remark&quot;:&quot;java&quot;&#125;&#125;,&#123;&quot;term&quot;:&#123;&quot;remark&quot;:&#123;&quot;value&quot;:&quot;developer&quot;&#125;&#125;&#125;]&#125;&#125;&#125;GET /es_db/_search // 转换前&#123;&quot;query&quot;:&#123;&quot;match&quot;:&#123;&quot;remark&quot;:&#123;&quot;query&quot;:&quot;java architect assistant&quot;,&quot;minimum_should_match&quot;:&quot;68%&quot;&#125;&#125;&#125;&#125;GET /es_db/_search // 转换后&#123;&quot;query&quot;:&#123;&quot;bool&quot;:&#123;&quot;should&quot;:[&#123;&quot;term&quot;:&#123;&quot;remark&quot;:&quot;java&quot;&#125;&#125;,&#123;&quot;term&quot;:&#123;&quot;remark&quot;:&quot;architect&quot;&#125;&#125;,&#123;&quot;term&quot;:&#123;&quot;remark&quot;:&quot;assistant&quot;&#125;&#125;],&quot;minimum_should_match&quot;:2&#125;&#125;&#125; boost权重控制boost权重控制一般用于搜索时相关度排序使用,将某字段数据匹配时相关度分数增加 123456789101112GET /es_db/_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;: [&#123;&quot;match&quot;: &#123;&quot;remark&quot;: &quot;java&quot;&#125;&#125;], &quot;should&quot;: [ &#123;&quot;match&quot;: &#123;&quot;remark&quot;: &#123;&quot;query&quot;: &quot;developer&quot;,&quot;boost&quot;: 3&#125;&#125;&#125;, &#123;&quot;match&quot;: &#123;&quot;remark&quot;: &#123;&quot;query&quot;: &quot;architect&quot;,&quot;boost&quot;: 1&#125;&#125;&#125; ] &#125; &#125;&#125; dis_maxdis_max 语法是直接 获取搜索多条件 中 单条件query相关度分数最高 的数据,以该数据做 相关度排序。基于dis_max 实现 best fields策略 进行 多字段搜索, best fields策略是搜索document中某个field, 尽可能多的匹配搜索条件。与之相反的是 most fields策略 即 尽可能多的字段匹配到搜索条件 。 best fields策略优点是精确匹配的数据可尽可能排列在最前端,且可通过 minimum_should_match 去除 长尾数据,避免长尾数据字段对排序结果的影响。缺点相对排序不均匀。 1234567891011GET /es_db/_search&#123; &quot;query&quot;: &#123; &quot;dis_max&quot;: &#123; // 找name字段中rod匹配相关度分数或remark字段中java developer匹配相关度分数,哪个高就使用哪个相关度分数进行结果排序 &quot;queries&quot;: [ &#123;&quot;match&quot;: &#123;&quot;name&quot;: &quot;rod&quot;&#125;&#125;, &#123;&quot;match&quot;: &#123;&quot;remark&quot;: &quot;java developer&quot;&#125;&#125; ] &#125; &#125;&#125; dis_max 是将 多个 搜索query条件中 相关度分数最高 的用于结果排序, 忽略其他query分数,在某些情况下 需要其他query条件中相关度介入最终结果排序,此时可 使用tie_breaker参数来优化dis_max搜索。 tie_breaker 参数表示 将其他query搜索条件相关度分数乘以参数值再参与结果排序。若不定义 tie_breaker 参数相当于 参数值为0,故其他query条件的相关度分数被忽略。 123456789101112GET /es_db/_search&#123; &quot;query&quot;: &#123; &quot;dis_max&quot;: &#123; // 找name字段中rod匹配相关度分数或remark字段中java developer匹配相关度分数,哪个高就使用哪个相关度分数进行结果排序 &quot;queries&quot;: [ &#123;&quot;match&quot;: &#123;&quot;name&quot;: &quot;rod&quot;&#125;&#125;, &#123;&quot;match&quot;: &#123;&quot;remark&quot;: &quot;java developer&quot;&#125;&#125; ], &quot;tie_breaker&quot;: 0.5 &#125; &#125;&#125; 使用multi_match简化dis_max+tie_breaker,ES中相同结果搜索也可使用不同语法语句来实现。 123456789101112131415161718192021222324252627GET /es_db/_search&#123; &quot;query&quot;: &#123; &quot;dis_max&quot;: &#123; &quot;queries&quot;: [ &#123;&quot;match&quot;: &#123;&quot;name&quot;: &quot;rod&quot;&#125;&#125;, &#123;&quot;match&quot;: &#123;&quot;remark&quot;: &#123;&quot;query&quot;: &quot;java assistant&quot;,&quot;boost&quot;: 2&#125;&#125;&#125; ], &quot;tie_breaker&quot;: 0.5 &#125; &#125;&#125;GET /es_db/_search&#123; &quot;query&quot;: &#123; &quot;multi_match&quot;: &#123; &quot;query&quot;: &quot;rod java developer&quot;, &quot;fields&quot;: [ &quot;name&quot;, &quot;remark^2&quot; // ^n代表权重,相当于&quot;boost&quot;:n ], &quot;type&quot;: &quot;best_fields&quot;, // 其中type常用的有best_fields和most_fields &quot;tie_breaker&quot;: 0.5, &quot;minimum_should_match&quot;: &quot;50%&quot; &#125; &#125;&#125; cross fields 是一个 唯一标识,且分布在 多个fields 中, 使用该唯一标识搜索数据即cross fields搜索。如人名可分为姓和名,地址可分为省、市、区县、街道等。使用人名或地址来搜索document,就称为cross fields搜索。 实现这种搜索,一般都是使用 most fields搜索策略,因为这就是 多个field 问题。 Cross fields 搜索策略是 从多个字段中搜索条件数据, 默认和most fields搜索逻辑一致 但 计算相关度分数和best fields策略一致。一般若使用cross fields搜索策略,都会携带 operator 额外参数,用来标记搜索条件如何在多个字段中匹配。 1234567891011GET /es_db/_search&#123; &quot;query&quot;: &#123; &quot;multi_match&quot;: &#123; // 搜索条件中java必须在name或remark字段中匹配,developer也必须在name或remark字段中匹配 &quot;query&quot;: &quot;java developer&quot;, &quot;fields&quot;: [&quot;name&quot;, &quot;remark&quot;], &quot;type&quot;: &quot;cross_fields&quot;, &quot;operator&quot;: &quot;and&quot; &#125; &#125;&#125; most fields策略 是尽可能匹配更多字段,会导致 精确搜索结果排序问题 ,又因为cross fields搜索,不能使用 minimum_should_match 来去除长尾数据。故在使用 most fields 和 cross fields 策略搜索数据时,都有不同缺陷,商业项目开发中都 推荐使用best fields策略 实现搜索。 可通过 copy_to 解决 cross fields搜索问题, copy_to 就是将 多个字段复制到一个字段 中实现一个 多字段组合,在商业项目中,也用于 解决搜索条件默认字段问题。若需要使用copy_to语法,则需要在定义 index 时手工指定 mapping映射策略。 123456789PUT /es_db/_mapping&#123; &quot;properties&quot;: &#123; &quot;provice&quot;: &#123;&quot;type&quot;: &quot;text&quot;,&quot;analyzer&quot;: &quot;standard&quot;,&quot;copy_to&quot;: &quot;address&quot;&#125;, &quot;city&quot;: &#123;&quot;type&quot;: &quot;text&quot;,&quot;copy_to&quot;: &quot;address&quot;&#125;, &quot;street&quot;: &#123;&quot;type&quot;: &quot;text&quot;,&quot;analyzer&quot;: &quot;standard&quot;,&quot;copy_to&quot;: &quot;address&quot;&#125;, &quot;address&quot;: &#123;&quot;type&quot;: &quot;text&quot;,&quot;analyzer&quot;: &quot;standard&quot;&#125; &#125;&#125; 在mapping定义中新增provice、city、street、address等字段,其中provice、city、street三个字段值会自动复制到address字段中,实现一个字段组合。在搜索地址时可在address字段中做条件匹配,从而避免most fields策略导致的问题。在维护数据时不需对address字段特殊维护,ES会自动维护组合字段。在存储时物理上不一定存在但逻辑上存在,因为address由3个物理存在属性province、city、street组成。 使用 match 和 proximity search 实现 召回率 和 精准度平衡 ,若搜索时只使用match phrase语法,会导致 召回率低下,若只使用match语法,会导致 精准度低下,因为搜索结果排序是根据相关度分数算法计算得到。若需要在结果中 兼顾召回率 和 精准度,就需要将 match 和 proximity search 混合使用。 召回率：搜索结果比率,如索引A中有100个document,搜索时返回多少个document 精准度：搜索结果准确率,如搜索条件为hello java,搜索结果中尽可能让短语匹配和hello java离的近的结果排序靠前1234567891011121314151617181920POST /test_a/_doc/3&#123;&quot;f&quot;:&quot;hello, java is very good, spark is also very good&quot;&#125;POST /test_a/_doc/4&#123;&quot;f&quot;:&quot;java and spark, development language &quot;&#125;POST /test_a/_doc/5&#123;&quot;f&quot;:&quot;Java Spark is a fast and general-purpose cluster computing system. It provides high-level APIs in Java, Scala, Python and R, and an optimized engine that supports general execution graphs.&quot;&#125;POST /test_a/_doc/6&#123;&quot;f&quot;:&quot;java spark and, development language &quot;&#125;GET /test_a/_search&#123;&quot;query&quot;:&#123;&quot;match&quot;:&#123;&quot;f&quot;:&quot;java spark&quot;&#125;&#125;&#125;GET /test_a/_search&#123; &quot;query&quot;: &#123; &quot;bool&quot;: &#123; &quot;must&quot;: [&#123;&quot;match&quot;: &#123;&quot;f&quot;: &quot;java spark&quot;&#125;&#125;], &quot;should&quot;: [&#123;&quot;match_phrase&quot;: &#123;&quot;f&quot;: &#123;&quot;query&quot;: &quot;java spark&quot;,&quot;slop&quot;: 50&#125;&#125;&#125;] &#125; &#125;&#125; 连接查询 父子 文档查询： parent/child 嵌套 文档查询： nested ES架构原理在ES中主要分成 Master 和 DataNode 两类节点,ES启动时会选举出一个Master节点,当某个节点启动后,使用 Zen Discovery机制 找到集群中的其他节点并 建立连接,并 从候选主节点中选举出一个主节点。一个ES集群中只有一个Master节点,但会有 N个DataNode 节点,在生产环境中内存可相对小一点但机器要稳定。 Master：管理索引即创建、删除索引, 分配分片, 维护元数据, 管理集群节点状态, 不负责数据写入和查询,比较轻量级 DataNode：数据写入, 数据检索,大部分ES压力都在DataNode节点上 分片ShardES是一个分布式搜索引擎,索引数据也分成若干部分,分布在不同服务器节点中,分布在不同服务器节点中的索引数据,就是Shard分片。Elasticsearch会自动管理分片,若发现分片分布不均衡,会自动迁移一个索引index由多个shard分片组成, 分片是分布在不同的服务器上。 副本为了对ES分片进行容错,假设某个节点不可用,会导致整个索引库都将不可用。故需要对分片进行副本容错, 每个分片都会有对应的副本。默认创建索引为1个分片、每个分片有 1个主分片 和 1个副本分片。 每个分片都会有一个Primary Shard主分片,也会有若干个Replica Shard副本分片, Primary Shard和Replica Shard不在同一个节点上。 12345678910111213141516PUT /job_idx_shard_temp // 创建指定分片数量、副本数量的索引&#123; &quot;mappings&quot;: &#123; &quot;properties&quot;: &#123; &quot;id&quot;: &#123;&quot;type&quot;: &quot;long&quot;,&quot;store&quot;: true&#125;, &quot;area&quot;: &#123;&quot;type&quot;: &quot;keyword&quot;,&quot;store&quot;: true&#125;, &quot;edu&quot;: &#123;&quot;type&quot;: &quot;keyword&quot;,&quot;store&quot;: true&#125; &#125; &#125;, &quot;settings&quot;: &#123; &quot;number_of_shards&quot;: 3, // 指定分片数量 &quot;number_of_replicas&quot;: 2 // 指定副本数量 &#125;&#125;GET /_cat/indices?v // 查看分片、主分片、副本分片 文档写入原理 选择 任意一个DataNode发送请求 如node2,此时node2就成为一个 coordinating node协调节点,通过协调节点 计算得到文档要写入的分片shard = hash(routing) % number_of_primary_shards,其中 routing 是一个 可变值, 默认为文档_id,然后 协调节点会进行路由,将请求 转发给对应 primary shard主分片所在的 DataNode,假设primary shard主分片在node1、replica shard副分片在node2,node1节点上的Primary Shard处理请求,写入数据到索引库中,并将数据同步到Replica shard副分片,Primary Shard和Replica Shard都保存好了文档则返回Client。 检索原理 Client发起查询请求某个 DataNode 接收到请求后,该 DataNode 就成为 Coordinating Node协调节点, 协调节点将查询请求广播到每一个数据节点,这些 数据节点 的 分片 会处理该查询请求, 每个分片进行数据查询,将符合条件的数据放在一个 优先队列 中,并将这些数据的 文档ID 、 节点信息 、 分片信息 返回给 协调节点 , 协调节点将所有结果进行汇总并全局排序,协调节点向包含这些 文档ID 的 分片 发送 get请求,对应的分片将文档数据返回给协调节点,最后协调节点将数据返回给客户端。 准实时索引 当数据写入到ES分片时会 首先写入到内存中,然后通过 内存buffer 生成一个 Segment,并刷到 文件系统缓存 中而 不是直接刷到磁盘,数据可被检索,ES中 默认1秒refresh一次。数据在 写入内存的同时,也会 记录Translog日志,若 在refresh期间出现异常,会 根据Translog 来进行 数据恢复,等到 文件系统缓存 中的 Segment 数据 都刷到磁盘中,则 清空Translog文件,ES 默认每隔30分钟 会将 文件系统缓存 的数据 刷入到磁盘. Segment太多 时ES 定期 会将多个 Segment合并 成为大的Segment, 减少索引查询时IO开销,此阶段ES会真正的 物理删除 之前 执行过delete的数据。","tags":[{"name":"ElasticSearch","slug":"ElasticSearch","permalink":"http://example.com/tags/ElasticSearch/"}],"categories":[{"name":"工具和中间件","slug":"工具和中间件","permalink":"http://example.com/categories/%E5%B7%A5%E5%85%B7%E5%92%8C%E4%B8%AD%E9%97%B4%E4%BB%B6/"},{"name":"搜索引擎技术","slug":"工具和中间件/搜索引擎技术","permalink":"http://example.com/categories/%E5%B7%A5%E5%85%B7%E5%92%8C%E4%B8%AD%E9%97%B4%E4%BB%B6/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E6%8A%80%E6%9C%AF/"},{"name":"ElasticSearch","slug":"工具和中间件/搜索引擎技术/ElasticSearch","permalink":"http://example.com/categories/%E5%B7%A5%E5%85%B7%E5%92%8C%E4%B8%AD%E9%97%B4%E4%BB%B6/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E6%8A%80%E6%9C%AF/ElasticSearch/"}]},{"title":"SpringCloud介绍","date":"2021-10-05T16:00:00.000Z","path":"blog/Cloud/SpringCloud/SpringCloud/","text":"基础知识介绍：单体架构系统：单体架构就是所有功能，都放在一个应用里。 好处：便于开发，测试，部署也很方便，直接打成一个 jar 或者 war, 就什么都好了。 弊端：要体现在高访问，高并发的上限是固定的。 比如一个单体架构，能够承受 1000次访问&#x2F;秒。 但是访问量达到 2000次&#x2F;秒的时候，就会非常卡顿，严重影响业务，并且仅仅依靠单体架构本身，很难突破这个瓶颈了。 集群和分布式：既然单体架构会有性能上的瓶颈，那么总要解决呀。 解决办法通常就是采用集群和分布式来做。 集群：指一组相互独立的计算机，通过高速的网络组成一个计算机系统。服务器集群就是指将很多服务器集中起来一起进行同一种服务，在客户端看来就像是只有一个服务器。 集群的特点和优势: 高性能 性价比 可伸缩性集群的分类 负载均衡集群（Load balancing clusters）简称LBC 高可用性集群（High-availability clusters）简称HAC 高性能计算集群（High-perfomance clusters）简称HPC 分布式：指将不同的业务分布在不同的地方，而集群指的是将几台服务器集中在一起，实现同一业务。分布式中的每一个节点，都可以做集群，而集群并不一定就是分布式的。 分布式一致性：分布式系统中，一个问题是负载均衡，另外一个问题就是数据的一致性。 在分布式集群中，很难保障数据的一致性。在以往的单节点服务中，通常使用锁来实现，当发生并发冲突时 通过对锁的持有获得对象的操作权，从而保证数据在同一时刻只允许被一个请求操作。但是在集群中，若同样采用锁的机制，那么需要一台节点用来管理分配锁，当其他节点进行请求前，首先去获取锁从而获得执行权。但是这样会产生单节点问题，即若管理锁的节点down掉，那么整个集群将无法工作。同时，由于锁的机制会使整个集群变成串行化单节点的形式，失去了集群的意义。 分布式和集群的关系： 根据分布式的介绍看出，其主要的功能是用了将我们的系统模块化，将系统进行解耦的，方便我们的维护和开发的，但是其并不能解决我们的并发问题，也无法保证我们的系统在服务器宕机后的正常运转。 集群就恰好弥补了分布式的缺陷，集群就是多个服务器处理相同的业务，这在一方面可以解决或者说改善我们系统的并发问题，一方面可以解决我们服务器如果出现一定数量的宕机后，系统仍然可以正常运转。 SpringCloud介绍：SpringCloud 就是一套工具。 Spring Cloud 并不是一个项目，而是一组项目的集合。包含了很多的子项目，每一个子项目都是一种微服务开发过程中遇到的问题的一种解决方案。它利用 Spring Boot的开发便利性巧妙地简化了分布式系统基础设施的开发，如服务发现注册、配置中心、消息总线、负载均衡、断路器、数据监控等，都可以用 Spring Boot的开发风格做到一键启动和部署。Spring Cloud并没有重复制造轮子，它只是将目前各家公司开发的比较成熟、经得起实际考验的服务框架组合起来，通过Spring Boot风格进行再封装屏蔽掉了复杂的配置和实现原理，最终给开发者留出了一套简单易懂、易部署和易维护的分布式系统开发工具包。 子项目介绍: Spring Cloud Config：集中配置管理工具，分布式系统中统一的外部配置管理，可以支持客户端配置的刷新及加密、解密操作, 可以让你把配置放到远程服务器，目前支持本地存储、Git 以及 Subversion。 Spring Cloud Netflix：针对多种 Netflix 组件提供的开发工具包，其中包括 Eureka、Hystrix、Ribbon、Feign、Zuul、Archaius 等组件, 如下: Eureka：服务治理组件，包括服务端的注册中心和客户端的服务发现机制； Hystrix：服务容错组件，实现了断路器模式，为依赖服务的出错和延迟提供了容错能力； Ribbon：负载均衡的服务调用组件，具有多种负载均衡调用策略； Feign：基于Ribbon和Hystrix的声明式服务调用组件； Zuul：API网关组件，对请求提供路由及过滤功能； Archaius：基于java的配置管理类库，主要用于多配置存储的动态获取。 Spring Cloud Bus：事件、消息总线，用于在集群（例如，配置变化事件）中传播状态变化，可与 Spring Cloud Config 联合实现热部署。 Spring Cloud Consul：封装了 Consul 操作，consul 是一个服务发现与配置工具，与 Docker 容器可以无缝集成。 Spring Cloud Security ：安全工具包，对Zuul代理中的负载均衡OAuth2客户端及登录认证进行支持。 Spring Cloud Sleuth：日志收集工具包，封装了 Dapper，Zipkin 和 HTrace 操作. Spring Cloud 应用的分布式跟踪实现。 Spring Cloud Stream：数据流操作开发包，封装了与 Redis，Rabbit、Kafka 等发送接收消息，实现的消息微服务。 Spring Cloud Task：用于快速构建短暂、有限数据处理任务的微服务框架，用于向应用中添加功能性和非功能性的特性。 Spring Cloud Zookeeper：基于 ZooKeeper 的服务发现与配置管理组件。 Spring Cloud Gateway：API网关组件，对请求提供路由及过滤功能, Spring Cloud 网关相关的整合实现。 Spring Cloud Aws：用于简化整合 Amazon Web Service 的组件。 Spring Cloud Cli：基于 Spring Boot CLI，可以让你以命令行方式快速建立云组件。 Spring Cloud Commons：服务发现、负载均衡、熔断机制这种模式为 Spring Cloud 客户端提供了一个通用的抽象层。 Spring Cloud Contract：Spring Cloud Contract是一个总体项目，其中包含帮助用户成功实施消费者驱动合同方法的解决方案(契约测试)。 Spring Cloud Cloudfoundry：通过 Oauth2 协议绑定服务到 CloudFoundry，CloudFoundry 是 VMware 推出的开源 PaaS 云平台。 Spring Cloud OpenFeign：基于Ribbon和Hystrix的声明式服务调用组件，可以动态创建基于Spring MVC注解的接口实现用于服务调用，在Spring Cloud 2.0中已经取代Feign成为了一等公民。","tags":[],"categories":[{"name":"Cloud","slug":"Cloud","permalink":"http://example.com/categories/Cloud/"},{"name":"SpringCloud","slug":"Cloud/SpringCloud","permalink":"http://example.com/categories/Cloud/SpringCloud/"}]},{"title":"SpringCloud系列10-断路器Hystrix","date":"2021-10-05T16:00:00.000Z","path":"blog/Cloud/SpringCloud/SpringCloud系列10-断路器Hystrix/","text":"问题:视图微服务是依赖于数据微服务的。那么当数据微服务不可用的时候，会怎么样呢？我们主动把 ProductDataServiceApplication 关闭，然后再访问：http://localhost:8012/products 就会抛出异常。客户也看不懂这个是什么。为了解决这个问题，我们就会引入断路器的概念。 断路器:断路器: 就是当被访问的微服务无法使用的时候，当前服务能够感知这个现象，并且提供一个备用的方案出来。 改造:pom.xml:增加 jar spring-cloud-starter-netflix-hystrix 以支持断路器。 12345&lt;!--增加 jar spring-cloud-starter-netflix-hystrix 以支持断路器--&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-hystrix&lt;/artifactId&gt;&lt;/dependency&gt; ProductClientFeign.java注解由原来的 @FeignClient(value = &quot;PRODUCT-DATA-SERVICE&quot;)修改为 @FeignClient(value = &quot;PRODUCT-DATA-SERVICE&quot;,fallback = ProductClientFeignHystrix.class)。 123456789101112131415package cn.peach.client;import cn.peach.pojo.Product;import org.springframework.cloud.openfeign.FeignClient;import org.springframework.web.bind.annotation.GetMapping;import java.util.List;@FeignClient(value = &quot;PRODUCT-DATA-SERVICE&quot;,fallback = ProductClientFeignHystrix.class)public interface ProductClientFeign &#123; @GetMapping(&quot;/products&quot;) public List&lt;Product&gt; listProdcuts();&#125; ProductClientFeignHystrix.javaProductClientFeignHystrix 实现了 ProductClientFeign 接口，并提供了 listProdcuts() 方法。这个方法就会固定返回包含一条信息的集合。 1234567891011121314151617181920package cn.peach.client;/* * Create By Tao on 2022/4/24. * * */import cn.peach.pojo.Product;import org.springframework.stereotype.Component;import java.util.ArrayList;import java.util.List;@Componentpublic class ProductClientFeignHystrix implements ProductClientFeign&#123; public List&lt;Product&gt; listProdcuts()&#123; List&lt;Product&gt; result = new ArrayList&lt;&gt;(); result.add(new Product(0,&quot;产品数据微服务现在不可用&quot;,0)); return result; &#125;&#125; application.yml在配置文件里开启断路器: 1feign.hystrix.enabled: true 启动:挨个启动： EurekaServerApplication, ConfigServerApplication, ProductViewServiceFeignApplication。注意: 数据服务是没有启动的。然后访问地址：http://127.0.0.1:8012/products会发现，依然可以打开，并且得到提示信息： 产品数据微服务不可用。","tags":[],"categories":[{"name":"Cloud","slug":"Cloud","permalink":"http://example.com/categories/Cloud/"},{"name":"SpringCloud","slug":"Cloud/SpringCloud","permalink":"http://example.com/categories/Cloud/SpringCloud/"}]},{"title":"SpringCloud系列12-断路器聚合监控","date":"2021-10-05T16:00:00.000Z","path":"blog/Cloud/SpringCloud/SpringCloud系列12-断路器聚合监控/","text":"需求:前面是针对一个微服务的断路器监控，但是微服务通常会是多个实例组成的一个集群。 倘若集群里的实例比较多，难道要挨个挨个去监控这些实例吗？ 何况有时候，根据集群的需要，会动态增加或者减少实例，监控起来就更麻烦了。 为了方便监控集群里的多个实例，springCloud 提供了一个 turbine 项目，它的作用是把一个集群里的多个实例汇聚在一个 turbine里，这个然后再在 断路器监控里查看这个 turbine, 这样就能够在集群层面进行监控了。 创建子项目： turbinepom.xml:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;parent&gt; &lt;artifactId&gt;spring-cloud-parent&lt;/artifactId&gt; &lt;groupId&gt;cn.peach&lt;/groupId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;relativePath&gt;../spring-cloud-parent/pom.xml&lt;/relativePath&gt; &lt;/parent&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;artifactId&gt;turbine&lt;/artifactId&gt; &lt;name&gt;turbine&lt;/name&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.11&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-client&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-hystrix&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-hystrix-dashboard&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-turbine&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt; ProductServiceTurbineApplication.java1234567891011121314151617181920package cn.peach;import cn.hutool.core.util.NetUtil;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.boot.builder.SpringApplicationBuilder;import org.springframework.cloud.netflix.turbine.EnableTurbine;@SpringBootApplication@EnableTurbinepublic class ProductServiceTurbineApplication &#123; public static void main(String[] args) &#123; int port = 8021; if(!NetUtil.isUsableLocalPort(port)) &#123; System.err.printf(&quot;端口%d被占用了，无法启动%n&quot;, port ); System.exit(1); &#125; new SpringApplicationBuilder(ProductServiceTurbineApplication.class).properties(&quot;server.port=&quot; + port).run(args); &#125;&#125; application.yml配置信息，主要是：appConfig: product-view-service-feign, 这就表示它会把所有微服务名称是product-view-service-feign 的实例信息都收集起来。 1234567891011121314spring: application.name: turbineturbine: aggregator: clusterConfig: default # 指定聚合哪些集群，多个使用&quot;,&quot;分割，默认为default。可使用http://.../turbine.stream?cluster=&#123;clusterConfig之一&#125;访问 appConfig: product-view-service-feign ### 配置Eureka中的serviceId列表，表明监控哪些服务 clusterNameExpression: new String(&quot;default&quot;) # 1. clusterNameExpression指定集群名称，默认表达式appName；此时：turbine.aggregator.clusterConfig需要配置想要监控的应用名称 # 2. 当clusterNameExpression:default时，turbine.aggregator.clusterConfig可以不写，因为默认就是default # 3. 当clusterNameExpression:metadata[&#x27;cluster&#x27;]时，假设想要监控的应用配置了eureka.instance.metadata-map.cluster: ABC，则需要配置，同时turbine.aggregator.clusterConfig: ABCeureka: client: serviceUrl: defaultZone: http://localhost:8761/eureka/","tags":[],"categories":[{"name":"Cloud","slug":"Cloud","permalink":"http://example.com/categories/Cloud/"},{"name":"SpringCloud","slug":"Cloud/SpringCloud","permalink":"http://example.com/categories/Cloud/SpringCloud/"}]},{"title":"SpringCloud系列11-断路器监控","date":"2021-10-05T16:00:00.000Z","path":"blog/Cloud/SpringCloud/SpringCloud系列11-断路器监控/","text":"需求:断路器，是当数据服务不可用的时候， 断路器就会发挥作用。那么数据服务什么时候可用，什么时候不可用，如何监控这个事情呢？ 我们就要用到 断路器监控 来可视化掌控这个情况了。 创建子项目：hystrix-dashboardpom.xml文件1234567891011121314151617181920212223242526272829303132333435363738394041424344&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;parent&gt; &lt;artifactId&gt;spring-cloud-parent&lt;/artifactId&gt; &lt;groupId&gt;cn.peach&lt;/groupId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;relativePath&gt;../spring-cloud-parent/pom.xml&lt;/relativePath&gt; &lt;/parent&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;artifactId&gt;hystrix-dashboard&lt;/artifactId&gt; &lt;name&gt;hystrix-dashboard&lt;/name&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.11&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-client&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-hystrix&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-hystrix-dashboard&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt; ProductServiceHystrixDashboardApplication.java断路器监控启动类，主要就是@EnableHystrixDashboard 1234567891011121314151617181920package cn.peach;import cn.hutool.core.util.NetUtil;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.boot.builder.SpringApplicationBuilder;import org.springframework.cloud.netflix.hystrix.dashboard.EnableHystrixDashboard;@SpringBootApplication@EnableHystrixDashboardpublic class ProductServiceHystrixDashboardApplication&#123; public static void main(String[] args) &#123; int port = 8020; if(!NetUtil.isUsableLocalPort(port)) &#123; System.err.printf(&quot;端口%d被占用了，无法启动%n&quot;, port ); System.exit(1); &#125; new SpringApplicationBuilder(ProductServiceHystrixDashboardApplication.class).properties(&quot;server.port=&quot; + port).run(args); &#125;&#125; application.yml:123spring: application: name: hystrix-dashboard ProductViewServiceFeignApplication.java 修改视图微服务项目，以使得它可以把信息共享给监控中心。 修改ProductViewServiceFeignApplication， 增加 @EnableCircuitBreaker 1234567891011121314151617181920212223242526272829303132333435package cn.peach;import brave.sampler.Sampler;import cn.hutool.core.util.NetUtil;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.cloud.client.circuitbreaker.EnableCircuitBreaker;import org.springframework.cloud.client.discovery.EnableDiscoveryClient;import org.springframework.cloud.netflix.eureka.EnableEurekaClient;import org.springframework.cloud.openfeign.EnableFeignClients;import org.springframework.context.annotation.Bean;@SpringBootApplication@EnableEurekaClient@EnableDiscoveryClient@EnableFeignClients@EnableCircuitBreaker //把信息共享给监控中心public class ProductViewServiceFeignApplication &#123; public static void main(String[] args) &#123; // 判断 rabiitMQ 是否启动 int rabbitMQPort = 5672; if(NetUtil.isUsableLocalPort(rabbitMQPort)) &#123; System.err.printf(&quot;未在端口%d 发现 rabbitMQ服务，请检查rabbitMQ 是否启动&quot;, rabbitMQPort ); System.exit(1); &#125; // 推荐 8012 、 8013 或者 8014 SpringApplication.run(ProductViewServiceFeignApplication.class, args); &#125; @Bean public Sampler defaultSampler() &#123; return Sampler.ALWAYS_SAMPLE; &#125;&#125; AccessViewService.java:准备一个不停访问服务的类： AccessViewService。 这样可以不断地访问服务，才便于在监控那里观察现象。 1234567891011121314151617181920212223242526package cn.peach.util;import cn.hutool.core.thread.ThreadUtil;import cn.hutool.http.HttpUtil;public class AccessViewService &#123; public static void main(String[] args) &#123; while(true) &#123; ThreadUtil.sleep(1000); access(8012); access(8013); &#125; &#125; public static void access(int port) &#123; try &#123; String html= HttpUtil.get(String.format(&quot;http://127.0.0.1:%d/products&quot;,port)); System.out.printf(&quot;%d 地址的视图服务访问成功，返回大小是 %d%n&quot; ,port, html.length()); &#125; catch(Exception e) &#123; System.err.printf(&quot;%d 地址的视图服务无法访问%n&quot;,port); &#125; &#125;&#125; 启动： 首先挨个运行 EurekaServerApplication, ConfigServerApplication, ProductDataServiceApplication， ProductViewServiceFeignApplication，ProductServiceHystrixDashboardApplication; 运行视图微服务里的 AccessViewService 来周期性地访问 http://127.0.0.1:8012/products 。 因为只有访问了，监控里才能看到数据; 打开监控地址 http://localhost:8020/hystrix; 如图所示，在最上面输入http://localhost:8012/actuator/hystrix.stream : 点击 Monitor Stream 就可以看到监控信息了。","tags":[],"categories":[{"name":"Cloud","slug":"Cloud","permalink":"http://example.com/categories/Cloud/"},{"name":"SpringCloud","slug":"Cloud/SpringCloud","permalink":"http://example.com/categories/Cloud/SpringCloud/"}]},{"title":"SpringCloud系列13-网关Zuul","date":"2021-10-05T16:00:00.000Z","path":"blog/Cloud/SpringCloud/SpringCloud系列13-网关Zuul/","text":"问题：为何要用网关? 我们现在有两种微服务，分别是数据微服务和视图微服务。它们有可能放在不同的 ip 地址上，有可能是不同的端口。 为了访问他们，就需要记录这些地址和端口。 而地址和端口都可能会变化，这就增加了访问者的负担。这个时候，我们就可以用网关来解决这个问题。 如图所示，我们只需要记住网关的地址和端口号就行了： 如果要访问数据服务，访问地址 http://ip:port/api-data/products 即可。 如果要访问视图服务，访问地址 http://ip:port/api-view/products 即可 创建子项目： zuulpom.xml：12345678910111213141516171819202122232425262728293031&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;parent&gt; &lt;artifactId&gt;spring-cloud-parent&lt;/artifactId&gt; &lt;groupId&gt;cn.peach&lt;/groupId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;relativePath&gt;../spring-cloud-parent/pom.xml&lt;/relativePath&gt; &lt;/parent&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;artifactId&gt;productServiceZuul&lt;/artifactId&gt; &lt;name&gt;productServiceZuul&lt;/name&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-client&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-zuul&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt; ProductServiceZuulApplication.java:12345678910111213141516171819202122232425package cn.peach;import cn.hutool.core.util.NetUtil;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.boot.builder.SpringApplicationBuilder;import org.springframework.cloud.client.discovery.EnableDiscoveryClient;import org.springframework.cloud.netflix.eureka.EnableEurekaClient;import org.springframework.cloud.netflix.zuul.EnableZuulProxy;@SpringBootApplication@EnableZuulProxy@EnableEurekaClient@EnableDiscoveryClientpublic class ProductServiceZuulApplication&#123; public static void main( String[] args ) &#123; int port = 8040; if(!NetUtil.isUsableLocalPort(port)) &#123; System.err.printf(&quot;端口%d被占用了，无法启动%n&quot;, port ); System.exit(1); &#125; new SpringApplicationBuilder(ProductServiceZuulApplication.class).properties(&quot;server.port=&quot; + port).run(args); &#125;&#125; application.yml:配置文件，进行了路由映射 12345678zuul: routes: api-a: path: /api-data/** serviceId: PRODUCT-DATA-SERVICE api-b: path: /api-view/** serviceId: PRODUCT-VIEW-SERVICE-FEIGN 完整代码： 123456789101112131415eureka: client: serviceUrl: defaultZone: http://localhost:8761/eureka/spring: application: name: product-service-zuulzuul: routes: api-a: path: /api-data/** serviceId: PRODUCT-DATA-SERVICE api-b: path: /api-view/** serviceId: PRODUCT-VIEW-SERVICE-FEIGN 启动： 首先挨个运行 EurekaServerApplication, ConfigServerApplication, ProductDataServiceApplication， ProductViewServiceFeignApplication。 然后启动 ProductServiceZuulApplication 接着访问地址: http://localhost:8040/api-data/products http://localhost:8040/api-view/products 这样就可以访问数据微服务和视微服务集群了，并且无需去记住那么多ip地址和端口号了。","tags":[],"categories":[{"name":"Cloud","slug":"Cloud","permalink":"http://example.com/categories/Cloud/"},{"name":"SpringCloud","slug":"Cloud/SpringCloud","permalink":"http://example.com/categories/Cloud/SpringCloud/"}]},{"title":"SpringCloud系列1-父子(聚合)项目","date":"2021-10-05T16:00:00.000Z","path":"blog/Cloud/SpringCloud/SpringCloud系列1-父子(聚合)项目/","text":"SpringCloud代码结构 创建父项目: spring-cloud-parent修改pom： 1&lt;packaging&gt;pom&lt;/packaging&gt; 注意： 父项目只有pom.xml文件， packaging值为pom. 完整代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;packaging&gt;pom&lt;/packaging&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.0.6.RELEASE&lt;/version&gt; &lt;!-- 踩坑:版本不对会导致Feign连接不上，亲测其它版本, 2.3.3.RELEASE version. --&gt; &lt;relativePath/&gt; &lt;/parent&gt; &lt;groupId&gt;cn.peach&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-parent&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;name&gt;spring-cloud-parent&lt;/name&gt; &lt;description&gt;spring-cloud-parent project for Spring Boot&lt;/description&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;spring-cloud.version&gt;Finchley.SR2&lt;/spring-cloud.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt; &lt;version&gt;$&#123;spring-cloud.version&#125;&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/dependencyManagement&gt;&lt;/project&gt; 创建子项目: product-data-service修改pom： 123456789&lt;parent&gt; &lt;artifactId&gt;spring-cloud-parent&lt;/artifactId&gt; &lt;groupId&gt;cn.peach&lt;/groupId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;relativePath&gt;../spring-cloud-parent/pom.xml&lt;/relativePath&gt;&lt;/parent&gt;&lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;&lt;artifactId&gt;product-data-service&lt;/artifactId&gt;&lt;name&gt;product-data-service&lt;/name&gt;","tags":[],"categories":[{"name":"Cloud","slug":"Cloud","permalink":"http://example.com/categories/Cloud/"},{"name":"SpringCloud","slug":"Cloud/SpringCloud","permalink":"http://example.com/categories/Cloud/SpringCloud/"}]},{"title":"SpringCloud系列3-注册数据微服务","date":"2021-10-05T16:00:00.000Z","path":"blog/Cloud/SpringCloud/SpringCloud系列3-注册数据微服务/","text":"创建子项目: product-data-service注意：若前面父子(聚合)项目创建了数据微服务，可直接更新此微服务。修改 pom.xml 为如下： spring-cloud-starter-netflix-eureka-client 表示这是个 eureka 客户端。 spring-boot-starter-web: 表示这是个web服务，会提供控制层1234567891011121314151617181920212223242526&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;parent&gt; &lt;artifactId&gt;spring-cloud-parent&lt;/artifactId&gt; &lt;groupId&gt;cn.peach&lt;/groupId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;relativePath&gt;../spring-cloud-parent/pom.xml&lt;/relativePath&gt; &lt;/parent&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;artifactId&gt;product-data-service&lt;/artifactId&gt; &lt;name&gt;product-data-service&lt;/name&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-client&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt; 启动类ProductDataServiceApplication 启动类， 考虑到要做集群。 自己输入端口，推荐 8001，8002，8003. 注意：这里忽略controller、 service、 repository, html层的代码，只列出重要部分代码，如需了解详情可参阅以下地址获取代码：https://github.com/taoliu-hub/spring-cloud-angular11/tree/main/spring-cloud. 123456789101112131415package cn.peach;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.cloud.netflix.eureka.EnableEurekaClient;@SpringBootApplication@EnableEurekaClientpublic class ProductDataServiceApplication&#123; public static void main( String[] args ) &#123; SpringApplication.run(ProductDataServiceApplication.class, args); &#125;&#125; 配置application.yml 设置微服务的名称： product-data-service 设置注册中心的地址： http://localhost:8761/eureka/, 与eureka-server中的配置 application.yml一致。 12345678910# server:# port: 因为会启动多个 product-data-service, 所以端口号由用户自动设置，推荐 8001,8002,8003 spring: application: name: product-data-serviceeureka: client: serviceUrl: defaultZone: http://localhost:8761/eureka/ 启动并访问注册中心Eureka:刷新访问：http://127.0.0.1:8761/。 补充(上图红色信息)： EMERGENCY! EUREKA MAY BE INCORRECTLY CLAIMING INSTANCES ARE UP WHEN THEY’RE NOT. RENEWALS ARE LESSER THAN THRESHOLD AND HENCE THE INSTANCES ARE NOT BEING EXPIRED JUST TO BE SAFE. 上面这句话意思是，Eureka可能会声明已经不存在的实例。刷新数小于阈值时，为了安全起见不会剔除过期实例。 Eureka的默认阈值为：85% 比如目前有10个微服务，只有8个有心跳反应时，（8&#x2F;10&#x3D;80%&lt;85%）Eureka就会开启保护机制，过期的实例不会立马剔除。并且出这个紧急警告，在搭建Eureka Server时，比如我们搭建了2个Eureka Server，并且禁止自注册，Eureka Server自身算一个服务，那么其中任意一个Eureka，只能获得一个心跳，1&#x2F;2&#x3D;50%。那么也会出现这个警告。 当不想有这个红色警告是，本机自测可以关闭Eureka保护配置。生产环境下不要关。在application.yml文件中配置：12server: enable-self-preservation: false","tags":[],"categories":[{"name":"Cloud","slug":"Cloud","permalink":"http://example.com/categories/Cloud/"},{"name":"SpringCloud","slug":"Cloud/SpringCloud","permalink":"http://example.com/categories/Cloud/SpringCloud/"}]},{"title":"SpringCloud系列14-总结","date":"2021-10-05T16:00:00.000Z","path":"blog/Cloud/SpringCloud/SpringCloud系列14-总结/","text":"多个微服务:12345678910&lt;modules&gt; &lt;module&gt;eureka-server&lt;/module&gt; &lt;module&gt;product-data-service&lt;/module&gt; &lt;module&gt;product-view-service-ribbon&lt;/module&gt; &lt;module&gt;product-view-service-feign&lt;/module&gt; &lt;module&gt;config-server&lt;/module&gt; &lt;module&gt;hystrix-dashboard&lt;/module&gt; &lt;module&gt;turbine&lt;/module&gt; &lt;module&gt;productServiceZuul&lt;/module&gt;&lt;/modules&gt; 端口号总结:微服务： eureka-server: 8761 product-data-service: 8001,8002,8003 product-view-service-ribbon: 8010 product-view-service-feign: 8012, 8013, 8014 hystrix-dashboard: 8020 turbine: 8021 config-server: 8030 zuul: 8040 第三方: zipkin:9411 rabbitMQ: 5672","tags":[],"categories":[{"name":"Cloud","slug":"Cloud","permalink":"http://example.com/categories/Cloud/"},{"name":"SpringCloud","slug":"Cloud/SpringCloud","permalink":"http://example.com/categories/Cloud/SpringCloud/"}]},{"title":"SpringCloud系列2-服务注册中心","date":"2021-10-05T16:00:00.000Z","path":"blog/Cloud/SpringCloud/SpringCloud系列2-服务注册中心/","text":"创建子项目: eureka-serverpom.xml ，增加 spring-cloud-starter-netflix-eureka-server jar 包 12345678910111213141516171819202122&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;parent&gt; &lt;artifactId&gt;spring-cloud-parent&lt;/artifactId&gt; &lt;groupId&gt;cn.peach&lt;/groupId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;relativePath&gt;../spring-cloud-parent/pom.xml&lt;/relativePath&gt; &lt;/parent&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;artifactId&gt;eureka-server&lt;/artifactId&gt; &lt;name&gt;eureka-server&lt;/name&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-server&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt; 启动类: EurekaServerApplication EurekaServer，它扮演的角色是注册中心，用于注册各种微服务，以便于其他微服务找到和访问。 EurekaServer 本身就是个 Springboot 微服务, 所以它有 @SpringBootApplication 注解。 @EnableEurekaServer 表示这是个 EurekaServer 。完整代码：123456789101112131415161718192021package cn.peach;import cn.hutool.core.util.NetUtil;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.boot.builder.SpringApplicationBuilder;import org.springframework.cloud.netflix.eureka.server.EnableEurekaServer;@SpringBootApplication@EnableEurekaServerpublic class EurekaServerApplication&#123; public static void main(String[] args) &#123; //8761 这个端口是默认的，就不要修改了，后面的子项目，都会访问这个端口。 int port = 8761; if(!NetUtil.isUsableLocalPort(port)) &#123; System.err.printf(&quot;端口%d被占用了，无法启动%n&quot;, port ); System.exit(1); &#125; new SpringApplicationBuilder(EurekaServerApplication.class).properties(&quot;server.port=&quot; + port).run(args); &#125;&#125; 配置application.yml 设置微服务的名称： eureka-server hostname: localhost 表示主机名称。 registerWithEureka：false. 表示是否注册到服务器。 因为它本身就是服务器，所以就无需把自己注册到服务器了。 fetchRegistry: false. 表示是否获取服务器的注册信息，和上面同理，这里也设置为 false。 defaultZone： http:&#x2F;&#x2F;${eureka.instance.hostname}:${server.port}&#x2F;eureka&#x2F; 自己作为服务器，公布出来的地址。 比如后续某个微服务要把自己注册到 eureka server, 那么就要使用这个地址： http://localhost:8761/eureka/ 123456789101112eureka: instance: hostname: localhost client: registerWithEureka: false fetchRegistry: false serviceUrl: defaultZone: http://$&#123;eureka.instance.hostname&#125;:$&#123;server.port&#125;/eureka/ spring: application: name: eureka-server 启动并访问注册中心Eureka:运行 EurekaServerApplication，并访问：http://127.0.0.1:8761/，默认端口号为：8761。","tags":[],"categories":[{"name":"Cloud","slug":"Cloud","permalink":"http://example.com/categories/Cloud/"},{"name":"SpringCloud","slug":"Cloud/SpringCloud","permalink":"http://example.com/categories/Cloud/SpringCloud/"}]},{"title":"SpringCloud系列5-视图微服务-Feign","date":"2021-10-05T16:00:00.000Z","path":"blog/Cloud/SpringCloud/SpringCloud系列5-视图微服务-Feign/","text":"Feign 概念:是对Ribbon的封装，调用起来更简单。 代码片段的区别 Ribbon方式：123456@AutowiredRestTemplate restTemplate;public List&lt;Product&gt; listProdcuts() &#123; return restTemplate.getForObject(&quot;http://PRODUCT-DATA-SERVICE/products&quot;,List.class);&#125; Feign方式：123456@FeignClient(value = &quot;PRODUCT-DATA-SERVICE&quot;)public interface ProductClientFeign &#123; @GetMapping(&quot;/products&quot;) public List&lt;Product&gt; listProdcuts();&#125; 创建子项目 product-view-service-feignpom.xml:123456789101112131415161718192021222324252627282930313233&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;parent&gt; &lt;artifactId&gt;spring-cloud-parent&lt;/artifactId&gt; &lt;groupId&gt;cn.peach&lt;/groupId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;relativePath&gt;../spring-cloud-parent/pom.xml&lt;/relativePath&gt; &lt;/parent&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;artifactId&gt;product-view-service-feign&lt;/artifactId&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-client&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-openfeign&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-thymeleaf&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt; Feign 客户端:Feign 客户端， 通过 注解方式 访问 访问PRODUCT-DATA-SERVICE服务的 products路径， product-data-service 既不是域名也不是ip地址，而是 数据服务在 eureka 注册中心的名称。 注意: 这里只是指定了要访问的 微服务名称，但是并没有指定端口号到底是 8001, 还是 8002. Feign方式：1234567891011121314package cn.peach.client;import cn.peach.pojo.Product;import org.springframework.cloud.openfeign.FeignClient;import org.springframework.web.bind.annotation.GetMapping;import java.util.List;@FeignClient(value = &quot;PRODUCT-DATA-SERVICE&quot;)public interface ProductClientFeign &#123; @GetMapping(&quot;/products&quot;) public List&lt;Product&gt; listProdcuts();&#125; Java code: 注意：这里忽略controller、 service、 repository, html层的代码，只列出重要部分代码，如需了解详情可参阅以下地址获取代码：https://github.com/taoliu-hub/spring-cloud-angular11/tree/main/spring-cloud. 服务类: ProductServiceImpl 123456789101112131415161718192021package cn.peach.service.impl;import cn.peach.client.ProductClientFeign;import cn.peach.pojo.Product;import cn.peach.service.ProductService;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Service;import java.util.List;@Servicepublic class ProductServiceImpl implements ProductService &#123; @Autowired ProductClientFeign productClientFeign; @Override public List&lt;Product&gt; listProducts() &#123; return productClientFeign.listProdcuts(); &#125;&#125; 启动类，注解多了个 @EnableFeignClients， 表示用于使用 Feign 方式。 1234567891011121314151617181920package cn.peach;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.cloud.netflix.eureka.EnableEurekaClient;import org.springframework.cloud.client.discovery.EnableDiscoveryClient;import org.springframework.cloud.openfeign.EnableFeignClients;@SpringBootApplication@EnableEurekaClient@EnableDiscoveryClient@EnableFeignClientspublic class ProductViewServiceFeignApplication &#123; public static void main(String[] args) &#123; // 推荐 8012 、 8013 或者 8014 SpringApplication.run(ProductViewServiceFeignApplication.class, args); &#125;&#125; 配置application.yml配置类，指定了 eureka server 的地址，以及自己的名称。 另外是一些 thymeleaf 的默认配置。 1234567891011121314eureka: client: serviceUrl: defaultZone: http://localhost:8761/eureka/spring: application: name: product-view-service-feign thymeleaf: cache: false prefix: classpath:/templates/ suffix: .html encoding: UTF-8 content-type: text/html mode: HTML5 启动并访问注册中心Eureka:刷新访问：http://127.0.0.1:8761/。 启动并访问视图微服务product-view-service-feign:刷新访问：http://127.0.0.1:8012/products。 调用图：如图所示： 首先数据微服务和视图微服务都被 eureka 管理起来了。 数据服务是由两个实例的集群组成的，端口分别是 8001 ， 8002 视图微服务通过 注册中心调用微服务， 然后负载均衡到 8001 或者 8002 端口的应用上。","tags":[],"categories":[{"name":"Cloud","slug":"Cloud","permalink":"http://example.com/categories/Cloud/"},{"name":"SpringCloud","slug":"Cloud/SpringCloud","permalink":"http://example.com/categories/Cloud/SpringCloud/"}]},{"title":"SpringCloud系列4-视图微服务-RIBBON","date":"2021-10-05T16:00:00.000Z","path":"blog/Cloud/SpringCloud/SpringCloud系列4-视图微服务-RIBBON/","text":"Ribbon 概念访问前面注册好的数据微服务, springcloud 提供了两种方式: Ribbon: 是使用 restTemplate 进行调用，并进行客户端负载均衡。 Feign: 是对 Ribbon的封装，调用起来更简单。 什么是客户端负载均衡:在前面注册数据微服务里，注册了8001和8002两个微服务， Ribbon会从注册中心获知这个信息，然后由 Ribbon 这个客户端自己决定是调用哪个，这个就叫做客户端负载均衡。 创建子项目: product-view-service-ribbonpom.xml包含以下jar: spring-cloud-starter-netflix-eureka-client: eureka 客户端 spring-boot-starter-web： springmvc spring-boot-starter-thymeleaf： thymeleaf 做服务端渲染，(前后端分离项目不用配置)。 12345678910111213141516171819202122232425262728293031&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;parent&gt; &lt;artifactId&gt;spring-cloud-parent&lt;/artifactId&gt; &lt;groupId&gt;cn.peach&lt;/groupId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;relativePath&gt;../spring-cloud-parent/pom.xml&lt;/relativePath&gt; &lt;/parent&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;artifactId&gt;product-view-service-ribbon&lt;/artifactId&gt; &lt;name&gt;product-view-service-ribbon&lt;/name&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-client&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-thymeleaf&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt; Ribbon 客户端Ribbon客户端: 通过restTemplate 访问 http://PRODUCT-DATA-SERVICE/products,而 product-data-service既不是域名也不是ip地址，而是数据服务在eureka 注册中心的名称. 注意: 这里只是指定了要访问的 微服务名称，但是并没有指定端口号到底是8001, 还是8002. 12345678910111213141516171819package cn.peach.client;import cn.peach.pojo.Product;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Component;import org.springframework.web.client.RestTemplate;import java.util.List;@Componentpublic class ProductClientRibbon &#123; @Autowired RestTemplate restTemplate; public List&lt;Product&gt; listProdcuts() &#123; return restTemplate.getForObject(&quot;http://PRODUCT-DATA-SERVICE/products&quot;,List.class); &#125;&#125; Java code: 注意：这里忽略controller、 service、 repository, html层的代码，只列出重要部分代码，如需了解详情可参阅以下地址获取代码：https://github.com/taoliu-hub/spring-cloud-angular11/tree/main/spring-cloud. 服务类: ProductServiceImpl 12345678910111213141516171819202122package cn.peach.service.impl;import cn.peach.client.ProductClientRibbon;import cn.peach.pojo.Product;import cn.peach.service.ProductService;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Service;import java.util.List;@Servicepublic class ProductServiceImpl implements ProductService &#123; @Autowired ProductClientRibbon productClientRibbon; @Override public List&lt;Product&gt; listProducts() &#123; return productClientRibbon.listProdcuts(); &#125;&#125; 启动类，注解多了个 @EnableDiscoveryClient，表示用于发现eureka 注册中心的微服务, 启动类，多了个 RestTemplate，就表示用 restTemplate 这个工具来做负载均衡, 考虑到要做集群。 自己输入端口，推荐 80010，8002，8003. 1234567891011121314151617181920212223import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.cloud.netflix.eureka.EnableEurekaClient;import org.springframework.cloud.client.discovery.EnableDiscoveryClient;import org.springframework.context.annotation.Bean;import org.springframework.cloud.client.loadbalancer.LoadBalanced;import org.springframework.web.client.RestTemplate;@SpringBootApplication@EnableEurekaClient@EnableDiscoveryClientpublic class ProductViewServiceRibbonApplication &#123; public static void main( String[] args ) &#123; SpringApplication.run(ProductDataServiceApplication.class, args); &#125; @Bean @LoadBalanced RestTemplate restTemplate() &#123; return new RestTemplate(); &#125;&#125; 配置application.yml配置类，指定了 eureka server 的地址，以及自己的名称。 另外是一些 thymeleaf 的默认配置。 1234567891011121314eureka: client: serviceUrl: defaultZone: http://localhost:8761/eureka/spring: application: name: product-view-service-ribbon thymeleaf: cache: false prefix: classpath:/templates/ suffix: .html encoding: UTF-8 content-type: text/html mode: HTML5 启动并访问注册中心Eureka:刷新访问：http://127.0.0.1:8761/。 启动并访问视图微服务product-view-service-ribbon:刷新访问：http://127.0.0.1:8010/products。 调用图：如图所示： 首先数据微服务和视图微服务都被 eureka 管理起来了。 数据服务是由两个实例的集群组成的，端口分别是 8001 ， 8002 视图微服务通过 注册中心调用微服务， 然后负载均衡到 8001 或者 8002 端口的应用上。","tags":[],"categories":[{"name":"Cloud","slug":"Cloud","permalink":"http://example.com/categories/Cloud/"},{"name":"SpringCloud","slug":"Cloud/SpringCloud","permalink":"http://example.com/categories/Cloud/SpringCloud/"}]},{"title":"SpringCloud系列8-配置客户端","date":"2021-10-05T16:00:00.000Z","path":"blog/Cloud/SpringCloud/SpringCloud系列8-配置客户端/","text":"配置客户端把现成的 视图微服务-Feign 改造成配置客户端，使得其可以从配置服务器上获取版本信息。 pom.xml增加一个 spring-cloud-starter-config 用于访问配置服务器。 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-config&lt;/artifactId&gt;&lt;/dependency&gt; bootstrap.yml 作为配置客户端，它需要在 bootstrap.yml 里配置 config-server 的信息，而不是像以前那样在 application.yml 里进行配置。 bootstrap.yml 和 application.yml 的区别，简单说就是前者先启动，并且一些系统方面的配置需要在 bootstrap.yml 里进行配置。 在 bootstrap.yml 里配置提供了 serviceId: config-server, 这个是配置服务器在 eureka server 里的服务名称，这样就可以定位 config-server了。123456789101112131415spring: cloud: config: label: develop profile: dev discovery: enabled: true serviceId: config-server bus: enabled: true trace: enabled: true client: serviceUrl: defaultZone: http://localhost:8761/eureka/ application.yml把 eureka 地址信息移动到了 bootstrap.yml 里。 123456789101112spring: application: name: product-view-service-feign thymeleaf: cache: false prefix: classpath:/templates/ suffix: .html encoding: UTF-8 content-type: text/html mode: HTML5 zipkin: base-url: http://localhost:9411 ProductController.java增加这个属性，就可以从 config-server 去获取 version 信息了。 12@Value(&quot;$&#123;version&#125;&quot;)String version; 然后把这个信息放在 Model里 1m.addAttribute(&quot;version&quot;, version); 完整代码： 12345678910111213141516@Controller@RefreshScopepublic class ProductController &#123; @Autowired ProductService productService; @Value(&quot;$&#123;version&#125;&quot;) String version; @RequestMapping(&quot;/products&quot;) public Object products(Model m) &#123; List&lt;Product&gt; ps = productService.listProducts(); m.addAttribute(&quot;version&quot;, version); m.addAttribute(&quot;ps&quot;, ps); return &quot;products&quot;; &#125;&#125; products.html:12345&lt;tr&gt; &lt;td align=&quot;center&quot; colspan=&quot;3&quot;&gt; &lt;p th:text=&quot;$&#123;version&#125;&quot; &gt;how2j springcloud version unknown&lt;/p&gt; &lt;/td&gt;&lt;/tr&gt; 启动:挨个启动 EurekaServerApplication, ConfigServerApplication, ProductDataServiceApplication, ProductViewServiceFeignApplication, 然后访问如下地址：http://localhost:8012/products","tags":[],"categories":[{"name":"Cloud","slug":"Cloud","permalink":"http://example.com/categories/Cloud/"},{"name":"SpringCloud","slug":"Cloud/SpringCloud","permalink":"http://example.com/categories/Cloud/SpringCloud/"}]},{"title":"SpringCloud系列9-消息总线Bus","date":"2021-10-05T16:00:00.000Z","path":"blog/Cloud/SpringCloud/SpringCloud系列9-消息总线Bus/","text":"问题：虽然配置了config-server, 也把视图服务改造成了配置客户端，但是当需要刷新配置信息的时候，不得不既重启 config-server, 又重启微服务。 这样的体验当然是不太好的。 我们当然是希望一旦 git 上的配置信息修改之后，就可以自动地刷新到微服务里，而不是需要手动重启才可以。 RabbitMQ： springCloud 通过 rabbitMQ 来进行消息广播，以达到有配置信息发生改变的时候，广播给多个微服务的效果。 需要先安装 rabbitMQ 服务器。 改造:pom.xml:product-view-service-feign: 新增spring-boot-starter-actuator 用于访问路径：&#x2F;actuator&#x2F;bus-refresh 新增spring-cloud-starter-bus-amqp 用于支持 rabbitmq 12345678910&lt;!--多了spring-boot-starter-actuator 用于访问路径：/actuator/bus-refresh--&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;&lt;/dependency&gt;&lt;!--多了spring-cloud-starter-bus-amqp 用于支持 rabbitmq--&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-bus-amqp&lt;/artifactId&gt;&lt;/dependency&gt; bootstrap.yml:新增 but总线配置: 123456spring: cloud: bus: enabled: true trace: enabled: true 新增 rabbitMQ 配置: 12345rabbitmq: host: localhost port: 5672 username: guest password: guest 完整代码： 123456789101112131415161718192021spring: cloud: config: label: develop profile: dev discovery: enabled: true serviceId: config-server bus: enabled: true trace: enabled: true client: serviceUrl: defaultZone: http://localhost:8761/eureka/rabbitmq: host: localhost port: 5672 username: guest password: guest application.yml:新增路径访问允许,这样才能访问 &#x2F;actuator&#x2F;bus-refresh: 12345678management: endpoints: web: exposure: include: &quot;*&quot; cors: allowed-origins: &quot;*&quot; allowed-methods: &quot;*&quot; FreshConfigUtil.java使用 post 的方式访问 http://localhost:8012/actuator/bus-refresh 地址，之所以要专门做一个 FreshConfigUtil 类，就是为了可以使用 post 访问，因为它不支持 get 方式访问，直接把这个地址放在浏览器里，是会抛出 405错误的。 12345678910111213141516171819202122package cn.peach.util;/* * Create By Tao on 2022/4/24. * * */import cn.hutool.http.HttpUtil;import java.util.HashMap;public class FreshConfigUtil &#123; public static void main(String[] args) &#123; HashMap&lt;String,String&gt; headers =new HashMap&lt;&gt;(); headers.put(&quot;Content-Type&quot;, &quot;application/json; charset=utf-8&quot;); System.out.println(&quot;因为要去git获取，还要刷新config-server, 会比较卡，所以一般会要好几秒才能完成，请耐心等待&quot;); String result = HttpUtil.createPost(&quot;http://localhost:8012/actuator/bus-refresh&quot;).addHeaders(headers).execute().body(); System.out.println(&quot;result:&quot;+result); System.out.println(&quot;refresh 完成&quot;); &#125;&#125; 对服务链路追踪的影响因为视图服务进行了改造，支持了 rabbitMQ, 那么在默认情况下，它的信息就不会进入 Zipkin了。 在Zipkin 里看不到视图服务的资料了。为了解决这个问题，在启动 Zipkin 的时候 带一个参数就好了：–zipkin.collector.rabbitmq.addresses&#x3D;localhost即改成了： 1java -jar zipkin-server-2.10.1-exec.jar --zipkin.collector.rabbitmq.addresses=localhost 注： 重启 zipkin 后，要再访问业务地址才可以看到依赖关系。","tags":[],"categories":[{"name":"Cloud","slug":"Cloud","permalink":"http://example.com/categories/Cloud/"},{"name":"SpringCloud","slug":"Cloud/SpringCloud","permalink":"http://example.com/categories/Cloud/SpringCloud/"}]},{"title":"SpringCloud系列6-服务链路追踪","date":"2021-10-05T16:00:00.000Z","path":"blog/Cloud/SpringCloud/SpringCloud系列6-服务链路追踪/","text":"什么是服务链路我们有两个微服务，分别是数据服务和视图服务，随着业务的增加，就会有越来越多的微服务存在，他们之间也会有更加复杂的调用关系。这个调用关系，仅仅通过观察代码，会越来越难以识别，所以就需要通过 zipkin 服务链路追踪服务器 这个东西来用图片进行识别了 改造 eureka-server 不需要做改造; product-data-service和product-view-service 需要进行改造以使其可以被追踪到。 pom.xml都加上以下依赖：12345&lt;!--服务链路追踪--&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-zipkin&lt;/artifactId&gt;&lt;/dependency&gt; application.yml都加上以下配置信息：123spring: zipkin: base-url: http://localhost:9411 启动类都加上以下配置信息：ProductDataServiceApplication.java 和 ProductViewServiceFeignApplication.java： 1234@Beanpublic Sampler defaultSampler() &#123; return Sampler.ALWAYS_SAMPLE;&#125; 启动测试： 需要启动链路追踪服务器：zipkin-server-2.10.1-exec.jar, 命令启动:1java -jar zipkin-server-2.10.1-exec.jar 启动 eureka-server, 改造后的 product-data-service 和 product-view-service-feign; 访问一次 http://127.0.0.1:8012/products 通过 视图微服务去访问数据微服务，这样链路追踪服务器才知道有这事儿发生 然后打开链路追踪服务器 http://localhost:9411/zipkin/dependency/ 就可以看到如图所示的 视图微服务调用数据微服务 的图形了","tags":[],"categories":[{"name":"Cloud","slug":"Cloud","permalink":"http://example.com/categories/Cloud/"},{"name":"SpringCloud","slug":"Cloud/SpringCloud","permalink":"http://example.com/categories/Cloud/SpringCloud/"}]},{"title":"SpringCloud系列7-配置服务器","date":"2021-10-05T16:00:00.000Z","path":"blog/Cloud/SpringCloud/SpringCloud系列7-配置服务器/","text":"配置服务的需要:有时候，微服务要做集群，这就意味着，会有多个微服务实例。 在业务上有时候需要修改一些配置信息，比如说 版本信息吧~ 倘若没有配置服务， 那么就需要挨个修改微服务，挨个重新部署微服务，这样就比较麻烦。为了偷懒， 这些配置信息就会放在一个公共的地方，比如git, 然后通过配置服务器把它获取下来，然后微服务再从配置服务器上取下来。这样只要修改git上的信息，那么同一个集群里的所有微服务都立即获取相应信息了，这样就大大节约了开发，上线和重新部署的时间了。 如图所示，我们先在 git 里保存 version 信息， 然后通过 ConfigServer 去获取 version 信息， 接着不同的视图微服务实例再去 ConfigServer 里获取 version. git首先要准备git。如下是已经准备好的 git:https://github.com/taoliu-hub/spring-cloud-angular11/blob/develop/respo/product-view-service-feign-dev.properties这里就准备了版本信息： version &#x3D; Version 1.1 创建子项目: config-serverpom.xml主要是 spring-cloud-config-server 这个 jar 包 1234567891011121314151617181920212223242526272829303132&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;parent&gt; &lt;artifactId&gt;spring-cloud-parent&lt;/artifactId&gt; &lt;groupId&gt;cn.peach&lt;/groupId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;relativePath&gt;../spring-cloud-parent/pom.xml&lt;/relativePath&gt; &lt;/parent&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;cn.peach&lt;/groupId&gt; &lt;artifactId&gt;config-server&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;name&gt;config-server&lt;/name&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-client&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-config-server&lt;/artifactId&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt; 启动类：ConfigServerApplication123456789101112131415161718package cn.peach;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.cloud.client.discovery.EnableDiscoveryClient;import org.springframework.cloud.config.server.EnableConfigServer;import org.springframework.cloud.netflix.eureka.EnableEurekaClient;@SpringBootApplication@EnableConfigServer@EnableDiscoveryClient@EnableEurekaClientpublic class ConfigServerApplication &#123; public static void main(String[] args) &#123; // 推荐 8030 SpringApplication.run(ConfigServerApplication.class, args); &#125;&#125; application.yml12345678910111213141516171819spring: application: name: config-server cloud: config: label: develop name: product-view-service-feign profile: dev server: git: uri: https://github.com/taoliu-hub/spring-cloud-angular11.git searchPaths: respo default-label: main timeout: 500000eureka: client: serviceUrl: defaultZone: http://localhost:8761/eureka/ 启动先启动 EurekaServerApplication， 再启动 ConfigServerApplication， 然后访问http://localhost:8030/version/dev","tags":[],"categories":[{"name":"Cloud","slug":"Cloud","permalink":"http://example.com/categories/Cloud/"},{"name":"SpringCloud","slug":"Cloud/SpringCloud","permalink":"http://example.com/categories/Cloud/SpringCloud/"}]},{"title":"消息队列","date":"2021-05-31T16:00:00.000Z","path":"blog/工具和中间件/消息队列/消息队列/","text":"什么是消息中间件呢？以公众号为例，如果某用户订阅（关注）了这个公众号，每当管理员发布新文章的时候，都可以在这个公众号得到通知，这就是一种广播订阅模式。 公众号如何实现这一点呢？ 可以通过 消息中间件 来轻松实现。 管理员把最新的教程信息 发给 消息中间件服务器， 用户手机上的微信里的 消息中间件客户端，就会自动去把消息获取出来显示，这样管理员就达到了文章广播的效果了。 消息队列的作用： 应用耦合：多应用间通过消息队列对同一消息进行处理，避免调用接口失败导致整个过程失败； 异步处理：多应用对消息队列中同一消息进行处理，应用间并发处理消息，相比串行处理，减少处理时间； 限流削峰：广泛应用于秒杀或抢购活动中，避免流量过大导致应用系统挂掉的情况； 消息驱动的系统：系统分为消息队列、消息生产者、消息消费者，生产者负责产生消息，消费者(可能有多个)负责对消息进行处理； 选择消息队列要满足以下几个条件： 开源 流行 兼容性强 消息队列需要： 消息的可靠传递：确保不丢消息； 性能：具备足够好的性能，能满足绝大多数场景的性能要求。 Cluster：支持集群，确保不会因为某个节点宕机导致服务不可用，当然也不能丢消息； 实现选择：消息中间件有很多种，如 Activemq, Rabbitmq, RocketMQ, Kafka 等等","tags":[],"categories":[{"name":"工具和中间件","slug":"工具和中间件","permalink":"http://example.com/categories/%E5%B7%A5%E5%85%B7%E5%92%8C%E4%B8%AD%E9%97%B4%E4%BB%B6/"},{"name":"消息队列","slug":"工具和中间件/消息队列","permalink":"http://example.com/categories/%E5%B7%A5%E5%85%B7%E5%92%8C%E4%B8%AD%E9%97%B4%E4%BB%B6/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/"}]},{"title":"ActiveMQ-队列模式","date":"2021-05-31T16:00:00.000Z","path":"blog/工具和中间件/消息队列/Activemq/ActiveMQ-主题模式/","text":"主题模式主题模式就是每个订阅了的消费者，都可以获取所有的消息，而不像队列模式那样要争抢。 pom.xml导入两个包，一个是 activemq ，另一个是 hutool jar包 123456789101112131415161718192021&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;cn.peach&lt;/groupId&gt; &lt;artifactId&gt;activemq&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;name&gt;activemq&lt;/name&gt; &lt;description&gt;activemq&lt;/description&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.activemq&lt;/groupId&gt; &lt;artifactId&gt;activemq-all&lt;/artifactId&gt; &lt;version&gt;5.15.9&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;cn.hutool&lt;/groupId&gt; &lt;artifactId&gt;hutool-all&lt;/artifactId&gt; &lt;version&gt;4.3.1&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/project&gt;","tags":[{"name":"ActiveMQ","slug":"ActiveMQ","permalink":"http://example.com/tags/ActiveMQ/"}],"categories":[{"name":"工具和中间件","slug":"工具和中间件","permalink":"http://example.com/categories/%E5%B7%A5%E5%85%B7%E5%92%8C%E4%B8%AD%E9%97%B4%E4%BB%B6/"},{"name":"消息队列","slug":"工具和中间件/消息队列","permalink":"http://example.com/categories/%E5%B7%A5%E5%85%B7%E5%92%8C%E4%B8%AD%E9%97%B4%E4%BB%B6/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/"},{"name":"Activemq","slug":"工具和中间件/消息队列/Activemq","permalink":"http://example.com/categories/%E5%B7%A5%E5%85%B7%E5%92%8C%E4%B8%AD%E9%97%B4%E4%BB%B6/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/Activemq/"}]},{"title":"ActiveMQ-队列模式","date":"2021-05-31T16:00:00.000Z","path":"blog/工具和中间件/消息队列/Activemq/ActiveMQ-队列模式/","text":"pom.xml导入两个包，一个是 activemq ，另一个是 hutool jar包 123456789101112131415161718192021&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;cn.peach&lt;/groupId&gt; &lt;artifactId&gt;activemq&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;name&gt;activemq&lt;/name&gt; &lt;description&gt;activemq&lt;/description&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.activemq&lt;/groupId&gt; &lt;artifactId&gt;activemq-all&lt;/artifactId&gt; &lt;version&gt;5.15.9&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;cn.hutool&lt;/groupId&gt; &lt;artifactId&gt;hutool-all&lt;/artifactId&gt; &lt;version&gt;4.3.1&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/project&gt; TestProducer.java生产100条消息。注： activemq 服务器应先启动。 12345678910111213141516171819202122232425262728293031323334353637383940414243package cn.peach.queue; import javax.jms.Connection;import javax.jms.ConnectionFactory;import javax.jms.Destination;import javax.jms.JMSException;import javax.jms.MessageProducer;import javax.jms.Session;import javax.jms.TextMessage; import org.apache.activemq.ActiveMQConnectionFactory; public class TestProducer &#123; //服务地址，端口默认61616 private static final String url=&quot;tcp://127.0.0.1:61616&quot;; //这次发送的消息名称 private static final String topicName=&quot;queue_style&quot;; public static void main(String[] args) throws JMSException &#123; //1.创建ConnectionFactory,绑定地址 ConnectionFactory factory=new ActiveMQConnectionFactory(url); //2.创建Connection Connection connection= factory.createConnection(); //3.启动连接 connection.start(); //4.创建会话 Session session=connection.createSession(false, Session.AUTO_ACKNOWLEDGE); //5.创建一个目标 (队列类型) Destination destination=session.createQueue(topicName); //6.创建一个生产者 MessageProducer producer=session.createProducer(destination); for (int i = 0; i &lt; 100; i++) &#123; //7.创建消息 TextMessage textMessage=session.createTextMessage(&quot;队列消息-&quot;+i); //8.发送消息 producer.send(textMessage); System.out.println(&quot;发送：&quot;+textMessage.getText()); &#125; //7. 关闭连接 connection.close(); &#125;&#125; TestConsumer.java消费者，消费服务器上的消息。注： activemq 服务器应先启动。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162package cn.peach.queue; import javax.jms.Connection;import javax.jms.ConnectionFactory;import javax.jms.Destination;import javax.jms.JMSException;import javax.jms.Message;import javax.jms.MessageConsumer;import javax.jms.MessageListener;import javax.jms.Session;import javax.jms.TextMessage; import org.apache.activemq.ActiveMQConnectionFactory; import cn.how2j.util.ActiveMQUtil;import cn.hutool.core.util.RandomUtil;/** * 订阅者 * @author root * */public class TestConsumer &#123; //服务地址，端口默认61616 private static final String url=&quot;tcp://127.0.0.1:61616&quot;; //这次消费的消息名称 private static final String topicName=&quot;queue_style&quot;; //消费者有可能是多个，为了区分不同的消费者，为其创建随机名称 private static final String consumerName=&quot;consumer-&quot; + RandomUtil.randomString(5); public static void main(String[] args) throws JMSException &#123; //1.创建ConnectiongFactory,绑定地址 ConnectionFactory factory=new ActiveMQConnectionFactory(url); //2.创建Connection Connection connection= factory.createConnection(); //3.启动连接 connection.start(); //4.创建会话 Session session=connection.createSession(false, Session.AUTO_ACKNOWLEDGE); //5.创建一个目标 （队列类型） Destination destination=session.createQueue(topicName); //6.创建一个消费者 MessageConsumer consumer=session.createConsumer(destination); //7.创建一个监听器 consumer.setMessageListener(new MessageListener() &#123; public void onMessage(Message arg0) &#123; // TODO Auto-generated method stub TextMessage textMessage=(TextMessage)arg0; try &#123; System.out.println(consumerName +&quot; 接收消息：&quot;+textMessage.getText()); &#125; catch (JMSException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); &#125; &#125; &#125;); //8. 因为不知道什么时候有，所以没法主动关闭，就不关闭了，一直处于监听状态 //connection.close(); &#125;&#125; 管理界面访问地址： http://127.0.0.1:8161/admin/queues.jsp 就可以看到 刚才的消息处理情况。 queue_style 是在代码中定义的消息名称。 number Of Consumers 表示有2个消费者。 Messages Enqueued：表示收到了 100 个消息。 Messages Dequeued：表示消费了 100 个消息。","tags":[{"name":"ActiveMQ","slug":"ActiveMQ","permalink":"http://example.com/tags/ActiveMQ/"}],"categories":[{"name":"工具和中间件","slug":"工具和中间件","permalink":"http://example.com/categories/%E5%B7%A5%E5%85%B7%E5%92%8C%E4%B8%AD%E9%97%B4%E4%BB%B6/"},{"name":"消息队列","slug":"工具和中间件/消息队列","permalink":"http://example.com/categories/%E5%B7%A5%E5%85%B7%E5%92%8C%E4%B8%AD%E9%97%B4%E4%BB%B6/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/"},{"name":"Activemq","slug":"工具和中间件/消息队列/Activemq","permalink":"http://example.com/categories/%E5%B7%A5%E5%85%B7%E5%92%8C%E4%B8%AD%E9%97%B4%E4%BB%B6/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/Activemq/"}]},{"title":"ActiveMQ-基础","date":"2021-05-31T16:00:00.000Z","path":"blog/工具和中间件/消息队列/Activemq/ActiveMQ-基础/","text":"ActiveMQ：ActiveMQ 是用 Java 语言开发的消息中间件，简单易用。只需要操作系统支持Java虚拟机，ActiveMQ便可执行。 下载并启动：apache-activemq-5.15.8-bin.rar, 解压并运行32或者64位操作系统对应的 activemq.bat 就启动 启动成功界面 访问地址启动好之后， 访问地址 http://127.0.0.1:8161/ 就可以看到如图所示的界面。这就是服务器的管理界面，在里面就可以看到都有哪些消息被创建了，哪些被消费了 管理界面点击 manage activeMQ broker, 或者直接访问地址：http://127.0.0.1:8161/admin/会弹出登录对话框，输入默认的账号和密码，都是： admin就来到了管理界面了。 观察数据这里可以观察到 队列数据和主题数据等信息，不过还没有客户端发消息来，所以也没有数据，就先不管。 模式activeMQ 有两种模式，分别是 队列模式 和 主题模式。 队列模式，其实就是分食模式。 比如生产方发了 10条消息到 activeMQ 服务器， 而此时有多个 消费方，那么这些消费方就会瓜分这些10条消息，一条消息只会被一个消费方得到。 主题模式，就是订阅模式。 比如生产方发了10条消息，而此时有多个消费方，那么多个消费方都能得到这 10条消息，就如同订阅公众号那样。","tags":[{"name":"ActiveMQ","slug":"ActiveMQ","permalink":"http://example.com/tags/ActiveMQ/"}],"categories":[{"name":"工具和中间件","slug":"工具和中间件","permalink":"http://example.com/categories/%E5%B7%A5%E5%85%B7%E5%92%8C%E4%B8%AD%E9%97%B4%E4%BB%B6/"},{"name":"消息队列","slug":"工具和中间件/消息队列","permalink":"http://example.com/categories/%E5%B7%A5%E5%85%B7%E5%92%8C%E4%B8%AD%E9%97%B4%E4%BB%B6/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/"},{"name":"Activemq","slug":"工具和中间件/消息队列/Activemq","permalink":"http://example.com/categories/%E5%B7%A5%E5%85%B7%E5%92%8C%E4%B8%AD%E9%97%B4%E4%BB%B6/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/Activemq/"}]},{"title":"Solr分页查询-Solrj","date":"2018-11-30T16:00:00.000Z","path":"blog/工具和中间件/搜索引擎技术/Solr/Solr分页查询-Solrj/","text":"SolrUtil.javaSolrUtil 增加分页查询的方法: 123456789public static QueryResponse query(String keywords,int startOfPage, int numberOfPage) throws SolrServerException, IOException &#123; SolrQuery query = new SolrQuery(); query.setStart(startOfPage); query.setRows(numberOfPage); query.setQuery(keywords); QueryResponse rsp = client.query(query); return rsp;&#125; 完整代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748package cn.peach;import java.io.IOException;import java.util.List; import org.apache.solr.client.solrj.SolrClient;import org.apache.solr.client.solrj.SolrQuery;import org.apache.solr.client.solrj.SolrServerException;import org.apache.solr.client.solrj.beans.DocumentObjectBinder;import org.apache.solr.client.solrj.impl.HttpSolrClient;import org.apache.solr.client.solrj.response.QueryResponse;import org.apache.solr.common.SolrDocument;import org.apache.solr.common.SolrDocumentList;import org.apache.solr.common.SolrInputDocument;import org.apache.solr.common.util.NamedList; public class SolrUtil &#123; public static SolrClient client; private static String url; static &#123; url = &quot;http://localhost:8983/solr/howToSolr&quot;; client = new HttpSolrClient.Builder(url).build(); &#125; public static &lt;T&gt; boolean batchSaveOrUpdate(List&lt;T&gt; entities) throws SolrServerException, IOException &#123; DocumentObjectBinder binder = new DocumentObjectBinder(); int total = entities.size(); int count=0; for (T t : entities) &#123; SolrInputDocument doc = binder.toSolrInputDocument(t); client.add(doc); System.out.printf(&quot;添加数据到索引中，总共要添加 %d 条记录，当前添加第%d条 %n&quot;,total,++count); &#125; client.commit(); return true; &#125; public static QueryResponse query(String keywords,int startOfPage, int numberOfPage) throws SolrServerException, IOException &#123; SolrQuery query = new SolrQuery(); query.setStart(startOfPage); query.setRows(numberOfPage); query.setQuery(keywords); QueryResponse rsp = client.query(query); return rsp; &#125; &#125; TestSolr4j.java拿到分页查询的结果，遍历出来 12345678910111213141516171819202122232425262728293031323334353637383940package cn.peach; import java.io.IOException;import java.util.Collection; import org.apache.solr.client.solrj.SolrServerException;import org.apache.solr.client.solrj.response.QueryResponse;import org.apache.solr.common.SolrDocument;import org.apache.solr.common.SolrDocumentList; public class TestSolr4j &#123; public static void main(String[] args) throws SolrServerException, IOException &#123; //查询 QueryResponse queryResponse = SolrUtil.query(&quot;name:手机&quot;,0,10); SolrDocumentList documents= queryResponse.getResults(); System.out.println(&quot;累计找到的条数：&quot;+documents.getNumFound()); if(!documents.isEmpty())&#123; Collection&lt;String&gt; fieldNames = documents.get(0).getFieldNames(); for (String fieldName : fieldNames) &#123; System.out.print(fieldName+&quot;\\t&quot;); &#125; System.out.println(); &#125; for (SolrDocument solrDocument : documents) &#123; Collection&lt;String&gt; fieldNames= solrDocument.getFieldNames(); for (String fieldName : fieldNames) &#123; System.out.print(solrDocument.get(fieldName)+&quot;\\t&quot;); &#125; System.out.println(); &#125; &#125; &#125;","tags":[{"name":"Solr","slug":"Solr","permalink":"http://example.com/tags/Solr/"}],"categories":[{"name":"工具和中间件","slug":"工具和中间件","permalink":"http://example.com/categories/%E5%B7%A5%E5%85%B7%E5%92%8C%E4%B8%AD%E9%97%B4%E4%BB%B6/"},{"name":"搜索引擎技术","slug":"工具和中间件/搜索引擎技术","permalink":"http://example.com/categories/%E5%B7%A5%E5%85%B7%E5%92%8C%E4%B8%AD%E9%97%B4%E4%BB%B6/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E6%8A%80%E6%9C%AF/"},{"name":"Solr","slug":"工具和中间件/搜索引擎技术/Solr","permalink":"http://example.com/categories/%E5%B7%A5%E5%85%B7%E5%92%8C%E4%B8%AD%E9%97%B4%E4%BB%B6/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E6%8A%80%E6%9C%AF/Solr/"}]},{"title":"Solr基础","date":"2018-11-30T16:00:00.000Z","path":"blog/工具和中间件/搜索引擎技术/Solr/Solr基础/","text":"Solr概念：以连接数据库为类比：Lucene 就相当于JDBC，是基本的用法。Solr 就相当 Mybatis， 方便开发人员配置，访问和调用。而且Solr 被做成了 webapp形式，以tomcat的应用的方式启动，提供了可视化的配置界面 启动服务器官网(https://lucene.apache.org/solr/)下载solr并解压(我下载的是solr-7.2.1.rar), 我的解压目录在 D:\\software\\solr-7.2.1 12cd d:\\software\\solr-7.2.1\\binsolr.cmd start 如此就启动了服务器，会占用端口8983。 倘若端口被占用，会启动失败. 访问服务器:浏览器输入：http://127.0.0.1:8983/solr/#/ Core 概念：如果说Solr相当于一个数据库的话，那么Core就相当于一张表 不要通过图形界面创建Core如图所示，通过图形界面创建Core会失败，应该使用 命令行方式创建Core: 命令行方式创建Core12cd d:\\software\\solr-7.2.1\\binsolr.cmd create -c howToSolr 删除 new_core如果点击了步骤 不要通过图形界面创建Core 里的图形界面里的 Add Core,那么就会一直有错误提醒，那么按照如下方式删除 new_core 就不会再有错误提醒了 12cd d:\\software\\solr-7.2.1\\binsolr.cmd delete -c new_core","tags":[{"name":"Solr","slug":"Solr","permalink":"http://example.com/tags/Solr/"}],"categories":[{"name":"工具和中间件","slug":"工具和中间件","permalink":"http://example.com/categories/%E5%B7%A5%E5%85%B7%E5%92%8C%E4%B8%AD%E9%97%B4%E4%BB%B6/"},{"name":"搜索引擎技术","slug":"工具和中间件/搜索引擎技术","permalink":"http://example.com/categories/%E5%B7%A5%E5%85%B7%E5%92%8C%E4%B8%AD%E9%97%B4%E4%BB%B6/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E6%8A%80%E6%9C%AF/"},{"name":"Solr","slug":"工具和中间件/搜索引擎技术/Solr","permalink":"http://example.com/categories/%E5%B7%A5%E5%85%B7%E5%92%8C%E4%B8%AD%E9%97%B4%E4%BB%B6/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E6%8A%80%E6%9C%AF/Solr/"}]},{"title":"Solr中文分词器IKAnalyzer","date":"2018-11-30T16:00:00.000Z","path":"blog/工具和中间件/搜索引擎技术/Solr/Solr中文分词器IKAnalyzer/","text":"中文分词默认情况下是没有中文分词的，如图所示，通过点击左边的howToSolr-&gt;Analysis 然后输入 四川省成都市动物园，得到是按照每个字的分词效果 配置中文分词下载 IKAnalyzer6.5.0.jar复制到如下目录：D:\\software\\solr-7.2.1\\server\\solr-webapp\\webapp\\WEB-INF\\lib 增加新的字段类型修改配置文件 managed-schema： 1D:\\software\\solr-7.2.1\\server\\solr\\howToSolr\\conf\\managed-schema 在line41 位置处 &lt;schema…&gt; 标签下增加如下代码: 12345&lt;schema name=&quot;default-config&quot; version=&quot;1.6&quot;&gt; &lt;fieldType name=&quot;text_ik&quot; class=&quot;solr.TextField&quot;&gt; &lt;analyzer class=&quot;org.wltea.analyzer.lucene.IKAnalyzer&quot;/&gt; &lt;/fieldType&gt; &lt;field name=&quot;text_ik&quot; type=&quot;text_ik&quot; indexed=&quot;true&quot; stored=&quot;true&quot; multiValued=&quot;false&quot; /&gt; 重启 Solr使用如下命令重启 123cd d:\\software\\solr-7.2.1\\binsolr.cmd stop -allsolr.cmd start 重新测试分词如图所示，使用中文分词后，就可以看到分词的效果了。注： FieldType 记得选增加新的字段类型 中的 text_ik","tags":[{"name":"Solr","slug":"Solr","permalink":"http://example.com/tags/Solr/"}],"categories":[{"name":"工具和中间件","slug":"工具和中间件","permalink":"http://example.com/categories/%E5%B7%A5%E5%85%B7%E5%92%8C%E4%B8%AD%E9%97%B4%E4%BB%B6/"},{"name":"搜索引擎技术","slug":"工具和中间件/搜索引擎技术","permalink":"http://example.com/categories/%E5%B7%A5%E5%85%B7%E5%92%8C%E4%B8%AD%E9%97%B4%E4%BB%B6/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E6%8A%80%E6%9C%AF/"},{"name":"Solr","slug":"工具和中间件/搜索引擎技术/Solr","permalink":"http://example.com/categories/%E5%B7%A5%E5%85%B7%E5%92%8C%E4%B8%AD%E9%97%B4%E4%BB%B6/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E6%8A%80%E6%9C%AF/Solr/"}]},{"title":"Solr更新和删除索引","date":"2018-11-30T16:00:00.000Z","path":"blog/工具和中间件/搜索引擎技术/Solr/Solr更新和删除索引/","text":"SolrUtil.javaSolrUtil提供一个对象的增加或者更新(都是同一个方法） 1234567public static &lt;T&gt; boolean saveOrUpdate(T entity) throws SolrServerException, IOException &#123; DocumentObjectBinder binder = new DocumentObjectBinder(); SolrInputDocument doc = binder.toSolrInputDocument(entity); client.add(doc); client.commit(); return true;&#125; 根据id删除这个索引 12345678910public static boolean deleteById(String id) &#123; try &#123; client.deleteById(id); client.commit(); &#125; catch (Exception e) &#123; e.printStackTrace(); return false; &#125; return true;&#125; 完整代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102package cn.peach;import java.io.IOException;import java.util.List;import org.apache.solr.client.solrj.SolrClient;import org.apache.solr.client.solrj.SolrQuery;import org.apache.solr.client.solrj.SolrServerException;import org.apache.solr.client.solrj.beans.DocumentObjectBinder;import org.apache.solr.client.solrj.impl.HttpSolrClient;import org.apache.solr.client.solrj.response.QueryResponse;import org.apache.solr.common.SolrDocument;import org.apache.solr.common.SolrDocumentList;import org.apache.solr.common.SolrInputDocument;import org.apache.solr.common.util.NamedList;public class SolrUtil &#123; public static SolrClient client; private static String url; static &#123; url = &quot;http://localhost:8983/solr/howToSolr&quot;; client = new HttpSolrClient.Builder(url).build(); &#125; public static void queryHighlight(String keywords) throws SolrServerException, IOException &#123; SolrQuery q = new SolrQuery(); //开始页数 q.setStart(0); //每页显示条数 q.setRows(10); // 设置查询关键字 q.setQuery(keywords); // 开启高亮 q.setHighlight(true); // 高亮字段 q.addHighlightField(&quot;name&quot;); // 高亮单词的前缀 q.setHighlightSimplePre(&quot;&lt;span style=&#x27;color:red&#x27;&gt;&quot;); // 高亮单词的后缀 q.setHighlightSimplePost(&quot;&lt;/span&gt;&quot;); //摘要最长100个字符 q.setHighlightFragsize(100); //查询 QueryResponse query = client.query(q); //获取高亮字段name相应结果 NamedList&lt;Object&gt; response = query.getResponse(); NamedList&lt;?&gt; highlighting = (NamedList&lt;?&gt;) response.get(&quot;highlighting&quot;); for (int i = 0; i &lt; highlighting.size(); i++) &#123; System.out.println(highlighting.getName(i) + &quot;：&quot; + highlighting.getVal(i)); &#125; //获取查询结果 SolrDocumentList results = query.getResults(); for (SolrDocument result : results) &#123; System.out.println(result.toString()); &#125; &#125; public static &lt;T&gt; boolean batchSaveOrUpdate(List&lt;T&gt; entities) throws SolrServerException, IOException &#123; DocumentObjectBinder binder = new DocumentObjectBinder(); int total = entities.size(); int count=0; for (T t : entities) &#123; SolrInputDocument doc = binder.toSolrInputDocument(t); client.add(doc); System.out.printf(&quot;添加数据到索引中，总共要添加 %d 条记录，当前添加第%d条 %n&quot;,total,++count); &#125; client.commit(); return true; &#125; public static QueryResponse query(String keywords,int startOfPage, int numberOfPage) throws SolrServerException, IOException &#123; SolrQuery query = new SolrQuery(); query.setStart(startOfPage); query.setRows(numberOfPage); query.setQuery(keywords); QueryResponse rsp = client.query(query); return rsp; &#125; public static &lt;T&gt; boolean saveOrUpdate(T entity) throws SolrServerException, IOException &#123; DocumentObjectBinder binder = new DocumentObjectBinder(); SolrInputDocument doc = binder.toSolrInputDocument(entity); client.add(doc); client.commit(); return true; &#125; public static boolean deleteById(String id) &#123; try &#123; client.deleteById(id); client.commit(); &#125; catch (Exception e) &#123; e.printStackTrace(); return false; &#125; return true; &#125; &#125; TestSolr4j.java 修改之前查询一次 修改之后查询一次 删除之后查询一次 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758package cn.peach;import java.io.IOException;import java.util.Collection;import java.util.List;import org.apache.solr.client.solrj.SolrServerException;import org.apache.solr.client.solrj.response.QueryResponse;import org.apache.solr.common.SolrDocument;import org.apache.solr.common.SolrDocumentList;public class TestSolr4j &#123; public static void main(String[] args) throws SolrServerException, IOException &#123; String keyword = &quot;name:鞭&quot;; System.out.println(&quot;修改之前&quot;); query(keyword); Product p = new Product(); p.setId(51173); p.setName(&quot;修改后的神鞭&quot;); SolrUtil.saveOrUpdate(p); System.out.println(&quot;修改之后&quot;); query(keyword); SolrUtil.deleteById(&quot;51173&quot;); System.out.println(&quot;删除之后&quot;); query(keyword); &#125; private static void query(String keyword) throws SolrServerException, IOException &#123; QueryResponse queryResponse = SolrUtil.query(keyword,0,10); SolrDocumentList documents= queryResponse.getResults(); System.out.println(&quot;累计找到的条数：&quot;+documents.getNumFound()); if(!documents.isEmpty())&#123; Collection&lt;String&gt; fieldNames = documents.get(0).getFieldNames(); for (String fieldName : fieldNames) &#123; System.out.print(fieldName+&quot;\\t&quot;); &#125; System.out.println(); &#125; for (SolrDocument solrDocument : documents) &#123; Collection&lt;String&gt; fieldNames= solrDocument.getFieldNames(); for (String fieldName : fieldNames) &#123; System.out.print(solrDocument.get(fieldName)+&quot;\\t&quot;); &#125; System.out.println(); &#125; &#125;&#125; 观察修改和删除的效果: Solr - 进一步学习:Solr官网展开学习：https://lucene.apache.org/solr/","tags":[{"name":"Solr","slug":"Solr","permalink":"http://example.com/tags/Solr/"}],"categories":[{"name":"工具和中间件","slug":"工具和中间件","permalink":"http://example.com/categories/%E5%B7%A5%E5%85%B7%E5%92%8C%E4%B8%AD%E9%97%B4%E4%BB%B6/"},{"name":"搜索引擎技术","slug":"工具和中间件/搜索引擎技术","permalink":"http://example.com/categories/%E5%B7%A5%E5%85%B7%E5%92%8C%E4%B8%AD%E9%97%B4%E4%BB%B6/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E6%8A%80%E6%9C%AF/"},{"name":"Solr","slug":"工具和中间件/搜索引擎技术/Solr","permalink":"http://example.com/categories/%E5%B7%A5%E5%85%B7%E5%92%8C%E4%B8%AD%E9%97%B4%E4%BB%B6/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E6%8A%80%E6%9C%AF/Solr/"}]},{"title":"Solr创建索引","date":"2018-11-30T16:00:00.000Z","path":"blog/工具和中间件/搜索引擎技术/Solr/Solr创建索引/","text":"如何创建索引：solr 提供了一种方式向其中增加索引的界面，但是不太方便，也和实际工作环境不相符合。以下配置通过程序把数据加入到Solr 索引里。 Product.java准备实体类来存放产品信息注： 每个字段上都有@Field 注解，用来告诉Solr 这些和 howToSolr core里的字段对应 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263package cn.peach; import org.apache.solr.client.solrj.beans.Field; public class Product &#123; @Field int id; @Field String name; @Field String category; @Field float price; @Field String place; @Field String code; public int getId() &#123; return id; &#125; public void setId(int id) &#123; this.id = id; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public String getCategory() &#123; return category; &#125; public void setCategory(String category) &#123; this.category = category; &#125; public float getPrice() &#123; return price; &#125; public void setPrice(float price) &#123; this.price = price; &#125; public String getPlace() &#123; return place; &#125; public void setPlace(String place) &#123; this.place = place; &#125; public String getCode() &#123; return code; &#125; public void setCode(String code) &#123; this.code = code; &#125; @Override public String toString() &#123; return &quot;Product [id=&quot; + id + &quot;, name=&quot; + name + &quot;, category=&quot; + category + &quot;, price=&quot; + price + &quot;, place=&quot; + place + &quot;, code=&quot; + code + &quot;]&quot;; &#125; &#125; ProductUtil.java工具类，把 140k_products.txt 文本文件，转换为泛型是Product的集合, 参考Lucene分页查询工具类。 SolrUtil.java工具类，用来把产品集合批量增加到Solr. 这里就用到了SolrJ第三方包里的api了。 123456789101112131415161718192021222324252627282930313233package cn.peach;import java.io.IOException;import java.util.List; import org.apache.solr.client.solrj.SolrClient;import org.apache.solr.client.solrj.SolrServerException;import org.apache.solr.client.solrj.beans.DocumentObjectBinder;import org.apache.solr.client.solrj.impl.HttpSolrClient;import org.apache.solr.common.SolrInputDocument; public class SolrUtil &#123; public static SolrClient client; private static String url; static &#123; url = &quot;http://localhost:8983/solr/howToSolr&quot;; client = new HttpSolrClient.Builder(url).build(); &#125; public static &lt;T&gt; boolean batchSaveOrUpdate(List&lt;T&gt; entities) throws SolrServerException, IOException &#123; DocumentObjectBinder binder = new DocumentObjectBinder(); int total = entities.size(); int count=0; for (T t : entities) &#123; SolrInputDocument doc = binder.toSolrInputDocument(t); client.add(doc); System.out.printf(&quot;添加数据到索引中，总共要添加 %d 条记录，当前添加第%d条 %n&quot;,total,++count); &#125; client.commit(); return true; &#125; &#125; TestSolr4j:得到14万个产品对象，然后通过SolrUtil 工具类提交到Solr 服务器。 123456789101112package cn.peach;import java.io.IOException;import java.util.List;import org.apache.solr.client.solrj.SolrServerException; public class TestSolr4j &#123; public static void main(String[] args) throws SolrServerException, IOException &#123; List&lt;Product&gt; products = ProductUtil.file2list(&quot;140k_products.txt&quot;); SolrUtil.batchSaveOrUpdate(products); &#125;&#125; 验证提交效果:","tags":[{"name":"Solr","slug":"Solr","permalink":"http://example.com/tags/Solr/"}],"categories":[{"name":"工具和中间件","slug":"工具和中间件","permalink":"http://example.com/categories/%E5%B7%A5%E5%85%B7%E5%92%8C%E4%B8%AD%E9%97%B4%E4%BB%B6/"},{"name":"搜索引擎技术","slug":"工具和中间件/搜索引擎技术","permalink":"http://example.com/categories/%E5%B7%A5%E5%85%B7%E5%92%8C%E4%B8%AD%E9%97%B4%E4%BB%B6/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E6%8A%80%E6%9C%AF/"},{"name":"Solr","slug":"工具和中间件/搜索引擎技术/Solr","permalink":"http://example.com/categories/%E5%B7%A5%E5%85%B7%E5%92%8C%E4%B8%AD%E9%97%B4%E4%BB%B6/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E6%8A%80%E6%9C%AF/Solr/"}]},{"title":"Solr高亮显示","date":"2018-11-30T16:00:00.000Z","path":"blog/工具和中间件/搜索引擎技术/Solr/Solr高亮显示/","text":"SolrUtil.java增加queryHighlight 方法 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283package cn.peach;import java.io.IOException;import java.util.List; import org.apache.solr.client.solrj.SolrClient;import org.apache.solr.client.solrj.SolrQuery;import org.apache.solr.client.solrj.SolrServerException;import org.apache.solr.client.solrj.beans.DocumentObjectBinder;import org.apache.solr.client.solrj.impl.HttpSolrClient;import org.apache.solr.client.solrj.response.QueryResponse;import org.apache.solr.common.SolrDocument;import org.apache.solr.common.SolrDocumentList;import org.apache.solr.common.SolrInputDocument;import org.apache.solr.common.util.NamedList; public class SolrUtil &#123; public static SolrClient client; private static String url; static &#123; url = &quot;http://localhost:8983/solr/howToSolr&quot;; client = new HttpSolrClient.Builder(url).build(); &#125; public static void queryHighlight(String keywords) throws SolrServerException, IOException &#123; SolrQuery q = new SolrQuery(); //开始页数 q.setStart(0); //每页显示条数 q.setRows(10); // 设置查询关键字 q.setQuery(keywords); // 开启高亮 q.setHighlight(true); // 高亮字段 q.addHighlightField(&quot;name&quot;); // 高亮单词的前缀 q.setHighlightSimplePre(&quot;&lt;span style=&#x27;color:red&#x27;&gt;&quot;); // 高亮单词的后缀 q.setHighlightSimplePost(&quot;&lt;/span&gt;&quot;); //摘要最长100个字符 q.setHighlightFragsize(100); //查询 QueryResponse query = client.query(q); //获取高亮字段name相应结果 NamedList&lt;Object&gt; response = query.getResponse(); NamedList&lt;?&gt; highlighting = (NamedList&lt;?&gt;) response.get(&quot;highlighting&quot;); for (int i = 0; i &lt; highlighting.size(); i++) &#123; System.out.println(highlighting.getName(i) + &quot;：&quot; + highlighting.getVal(i)); &#125; //获取查询结果 SolrDocumentList results = query.getResults(); for (SolrDocument result : results) &#123; System.out.println(result.toString()); &#125; &#125; public static &lt;T&gt; boolean batchSaveOrUpdate(List&lt;T&gt; entities) throws SolrServerException, IOException &#123; DocumentObjectBinder binder = new DocumentObjectBinder(); int total = entities.size(); int count=0; for (T t : entities) &#123; SolrInputDocument doc = binder.toSolrInputDocument(t); client.add(doc); System.out.printf(&quot;添加数据到索引中，总共要添加 %d 条记录，当前添加第%d条 %n&quot;,total,++count); &#125; client.commit(); return true; &#125; public static QueryResponse query(String keywords,int startOfPage, int numberOfPage) throws SolrServerException, IOException &#123; SolrQuery query = new SolrQuery(); query.setStart(startOfPage); query.setRows(numberOfPage); query.setQuery(keywords); QueryResponse rsp = client.query(query); return rsp; &#125; &#125; TestSolr4j.java调用queryHighlight 方法 123456789101112131415package cn.peach; import java.io.IOException; import org.apache.solr.client.solrj.SolrServerException; public class TestSolr4j &#123; public static void main(String[] args) throws SolrServerException, IOException &#123; //高亮查询查询 SolrUtil.queryHighlight(&quot;name:手机&quot;); &#125; &#125;","tags":[{"name":"Solr","slug":"Solr","permalink":"http://example.com/tags/Solr/"}],"categories":[{"name":"工具和中间件","slug":"工具和中间件","permalink":"http://example.com/categories/%E5%B7%A5%E5%85%B7%E5%92%8C%E4%B8%AD%E9%97%B4%E4%BB%B6/"},{"name":"搜索引擎技术","slug":"工具和中间件/搜索引擎技术","permalink":"http://example.com/categories/%E5%B7%A5%E5%85%B7%E5%92%8C%E4%B8%AD%E9%97%B4%E4%BB%B6/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E6%8A%80%E6%9C%AF/"},{"name":"Solr","slug":"工具和中间件/搜索引擎技术/Solr","permalink":"http://example.com/categories/%E5%B7%A5%E5%85%B7%E5%92%8C%E4%B8%AD%E9%97%B4%E4%BB%B6/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E6%8A%80%E6%9C%AF/Solr/"}]},{"title":"Solr设置字段","date":"2018-11-30T16:00:00.000Z","path":"blog/工具和中间件/搜索引擎技术/Solr/Solr设置字段/","text":"字段概念:创建Core 中的Core就相当于表，那么接下来就要为这个表设置字段，用于存放数据。 创建字段： 创建name字段：左边选中 howToSolr -&gt; Schema -&gt; Add Field 输入name: name， field type: text_ik, 这里一定要使用中文分词 中新创建的 text_ik类型，否则后续查询中文会失败。然后点击 Add Field按钮进行添加： 创建其他字段：按照创建name字段 的方式，继续创建如下字段： category text_ik, price pfloat, place text_ik, code text_ik注： price的类型是pfloat 关于id字段:id字段是默认就有的，无需自己创建 查看创建的字段:","tags":[{"name":"Solr","slug":"Solr","permalink":"http://example.com/tags/Solr/"}],"categories":[{"name":"工具和中间件","slug":"工具和中间件","permalink":"http://example.com/categories/%E5%B7%A5%E5%85%B7%E5%92%8C%E4%B8%AD%E9%97%B4%E4%BB%B6/"},{"name":"搜索引擎技术","slug":"工具和中间件/搜索引擎技术","permalink":"http://example.com/categories/%E5%B7%A5%E5%85%B7%E5%92%8C%E4%B8%AD%E9%97%B4%E4%BB%B6/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E6%8A%80%E6%9C%AF/"},{"name":"Solr","slug":"工具和中间件/搜索引擎技术/Solr","permalink":"http://example.com/categories/%E5%B7%A5%E5%85%B7%E5%92%8C%E4%B8%AD%E9%97%B4%E4%BB%B6/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E6%8A%80%E6%9C%AF/Solr/"}]},{"title":"Lucene分词器","date":"2018-10-01T16:00:00.000Z","path":"blog/工具和中间件/搜索引擎技术/Lucene/Lucene分词器/","text":"分词器概念：分词器指的是搜索引擎如何使用关键字进行匹配，如 基础 中的关键字：护眼带光源。 如果使用like,那么%护眼带光源%，匹配出来的结果就是要么全匹配，要不都不匹配。而使用分词器，就会把这个关键字分为 护眼，带，光源 3个关键字，这样就可以找到不同相关程度的结果了。 IKAnalyzer6.5.0.jarIKAnalyzer 这个分词器很久都没有维护了，也不支持Lucene7。 代码演示 TestAnalyzer12345678910111213141516171819package cn.peach; import java.io.IOException; import org.apache.lucene.analysis.TokenStream;import org.wltea.analyzer.lucene.IKAnalyzer; public class TestAnalyzer &#123; public static void main(String[] args) throws IOException &#123; IKAnalyzer analyzer = new IKAnalyzer(); TokenStream ts= analyzer.tokenStream(&quot;name&quot;, &quot;护眼带光源&quot;); ts.reset(); while(ts.incrementToken())&#123; System.out.println(ts.reflectAsString(false)); &#125; &#125;&#125; 高亮显示:70,71行 81,82行 增加高亮显示123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106package cn.peach;import java.io.IOException;import java.io.StringReader;import java.util.ArrayList;import java.util.List;import org.apache.lucene.analysis.TokenStream;import org.apache.lucene.document.Document;import org.apache.lucene.document.Field;import org.apache.lucene.document.TextField;import org.apache.lucene.index.DirectoryReader;import org.apache.lucene.index.IndexReader;import org.apache.lucene.index.IndexWriter;import org.apache.lucene.index.IndexWriterConfig;import org.apache.lucene.index.IndexableField;import org.apache.lucene.queryparser.classic.QueryParser;import org.apache.lucene.search.IndexSearcher;import org.apache.lucene.search.Query;import org.apache.lucene.search.ScoreDoc;import org.apache.lucene.search.highlight.Highlighter;import org.apache.lucene.search.highlight.QueryScorer;import org.apache.lucene.search.highlight.SimpleHTMLFormatter;import org.apache.lucene.store.Directory;import org.apache.lucene.store.RAMDirectory;import org.wltea.analyzer.lucene.IKAnalyzer;public class TestLucene &#123; public static void main(String[] args) throws Exception &#123; // 1. 准备中文分词器 IKAnalyzer analyzer = new IKAnalyzer(); // 2. 索引 List&lt;String&gt; productNames = new ArrayList&lt;&gt;(); productNames.add(&quot;飞利浦led灯泡e27螺口暖白球泡灯家用照明超亮节能灯泡转色温灯泡&quot;); productNames.add(&quot;飞利浦led灯泡e14螺口蜡烛灯泡3W尖泡拉尾节能灯泡暖黄光源Lamp&quot;); productNames.add(&quot;雷士照明 LED灯泡 e27大螺口节能灯3W球泡灯 Lamp led节能灯泡&quot;); productNames.add(&quot;飞利浦 led灯泡 e27螺口家用3w暖白球泡灯节能灯5W灯泡LED单灯7w&quot;); productNames.add(&quot;飞利浦led小球泡e14螺口4.5w透明款led节能灯泡照明光源lamp单灯&quot;); productNames.add(&quot;飞利浦蒲公英护眼台灯工作学习阅读节能灯具30508带光源&quot;); productNames.add(&quot;欧普照明led灯泡蜡烛节能灯泡e14螺口球泡灯超亮照明单灯光源&quot;); productNames.add(&quot;欧普照明led灯泡节能灯泡超亮光源e14e27螺旋螺口小球泡暖黄家用&quot;); productNames.add(&quot;聚欧普照明led灯泡节能灯泡e27螺口球泡家用led照明单灯超亮光源&quot;); Directory index = createIndex(analyzer, productNames); // 3. 查询器 String keyword = &quot;护眼带光源&quot;; Query query = new QueryParser(&quot;name&quot;, analyzer).parse(keyword); // 4. 搜索 IndexReader reader = DirectoryReader.open(index); IndexSearcher searcher = new IndexSearcher(reader); int numberPerPage = 1000; System.out.printf(&quot;当前一共有%d条数据%n&quot;,productNames.size()); System.out.printf(&quot;查询关键字是：\\&quot;%s\\&quot;%n&quot;,keyword); ScoreDoc[] hits = searcher.search(query, numberPerPage).scoreDocs; // 5. 显示查询结果 showSearchResults(searcher, hits, query, analyzer); // 6. 关闭查询 reader.close(); &#125; private static void showSearchResults(IndexSearcher searcher, ScoreDoc[] hits, Query query, IKAnalyzer analyzer) throws Exception &#123; System.out.println(&quot;找到 &quot; + hits.length + &quot; 个命中.&quot;); System.out.println(&quot;序号\\t匹配度得分\\t结果&quot;); SimpleHTMLFormatter simpleHTMLFormatter = new SimpleHTMLFormatter(&quot;&lt;span style=&#x27;color:red&#x27;&gt;&quot;, &quot;&lt;/span&gt;&quot;); Highlighter highlighter = new Highlighter(simpleHTMLFormatter, new QueryScorer(query)); for (int i = 0; i &lt; hits.length; ++i) &#123; ScoreDoc scoreDoc= hits[i]; int docId = scoreDoc.doc; Document d = searcher.doc(docId); List&lt;IndexableField&gt; fields = d.getFields(); System.out.print((i + 1)); System.out.print(&quot;\\t&quot; + scoreDoc.score); for (IndexableField f : fields) &#123; TokenStream tokenStream = analyzer.tokenStream(f.name(), new StringReader(d.get(f.name()))); String fieldContent = highlighter.getBestFragment(tokenStream, d.get(f.name())); System.out.print(&quot;\\t&quot; + fieldContent); &#125; System.out.println(&quot;&lt;br&gt;&quot;); &#125; &#125; private static Directory createIndex(IKAnalyzer analyzer, List&lt;String&gt; products) throws IOException &#123; Directory index = new RAMDirectory(); IndexWriterConfig config = new IndexWriterConfig(analyzer); IndexWriter writer = new IndexWriter(index, config); for (String name : products) &#123; addDoc(writer, name); &#125; writer.close(); return index; &#125; private static void addDoc(IndexWriter w, String name) throws IOException &#123; Document doc = new Document(); doc.add(new TextField(&quot;name&quot;, name, Field.Store.YES)); w.addDocument(doc); &#125;&#125; 运行结果运行结果是html代码，为了正常显示，复制到一个html文件里，打开就可以看到效果了.","tags":[{"name":"Lucene","slug":"Lucene","permalink":"http://example.com/tags/Lucene/"}],"categories":[{"name":"工具和中间件","slug":"工具和中间件","permalink":"http://example.com/categories/%E5%B7%A5%E5%85%B7%E5%92%8C%E4%B8%AD%E9%97%B4%E4%BB%B6/"},{"name":"搜索引擎技术","slug":"工具和中间件/搜索引擎技术","permalink":"http://example.com/categories/%E5%B7%A5%E5%85%B7%E5%92%8C%E4%B8%AD%E9%97%B4%E4%BB%B6/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E6%8A%80%E6%9C%AF/"},{"name":"Lucene","slug":"工具和中间件/搜索引擎技术/Lucene","permalink":"http://example.com/categories/%E5%B7%A5%E5%85%B7%E5%92%8C%E4%B8%AD%E9%97%B4%E4%BB%B6/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E6%8A%80%E6%9C%AF/Lucene/"}]},{"title":"Lucene基础","date":"2018-10-01T16:00:00.000Z","path":"blog/工具和中间件/搜索引擎技术/Lucene/Lucene基础/","text":"Lucene 概念:Lucene 这个开源项目，使得 Java开发人员可以很方便地得到像搜索引擎google baidu那样的搜索效果。 Lucene是一个全文检索框架，通过程序扫描文本中每个单词，针对单词建立索引，并保存该单词在文本中的位置、以及出现次数。用户查询时通过之前建立好的索引来查询，将索引中单词对应文本位置、出现次数返给用户，有了具体文本位置，则可将具体内容读取出来。 Lucene基于倒排索引，对于使用的数据库主键索引是通过主键定位到某条数据，而倒排索引刚好相反，是通过数据对应到主键。 分词器：准备中文分词器， 12// 1. 准备中文分词器IKAnalyzer analyzer = new IKAnalyzer(); 创建索引:首先准备10条数据这10条数据都是字符串，相当于产品表里的数据 123456789101112// 索引List&lt;String&gt; productNames = new ArrayList&lt;&gt;();productNames.add(&quot;飞利浦led灯泡e27螺口暖白球泡灯家用照明超亮节能灯泡转色温灯泡&quot;);productNames.add(&quot;飞利浦led灯泡e14螺口蜡烛灯泡3W尖泡拉尾节能灯泡暖黄光源Lamp&quot;);productNames.add(&quot;雷士照明 LED灯泡 e27大螺口节能灯3W球泡灯 Lamp led节能灯泡&quot;);productNames.add(&quot;飞利浦 led灯泡 e27螺口家用3w暖白球泡灯节能灯5W灯泡LED单灯7w&quot;);productNames.add(&quot;飞利浦led小球泡e14螺口4.5w透明款led节能灯泡照明光源lamp单灯&quot;);productNames.add(&quot;飞利浦蒲公英护眼台灯工作学习阅读节能灯具30508带光源&quot;);productNames.add(&quot;欧普照明led灯泡蜡烛节能灯泡e14螺口球泡灯超亮照明单灯光源&quot;);productNames.add(&quot;欧普照明led灯泡节能灯泡超亮光源e14e27螺旋螺口小球泡暖黄家用&quot;);productNames.add(&quot;聚欧普照明led灯泡节能灯泡e27螺口球泡家用led照明单灯超亮光源&quot;); Directory index = createIndex(analyzer, productNames); 通过 createIndex 方法，把它加入到索引当中: 问题： 创建内存索引，为什么Lucene会比数据库快? 因为它是从内存里查，自然就比数据库里快多了.1234567891011121314private static Directory createIndex(IKAnalyzer analyzer, List&lt;String&gt; products) throws IOException &#123; &lt;!-- 创建内存索引 --&gt; Directory index = new RAMDirectory(); &lt;!-- 根据中文分词器创建配置对象 --&gt; IndexWriterConfig config = new IndexWriterConfig(analyzer); &lt;!-- 创建索引 writer --&gt; IndexWriter writer = new IndexWriter(index, config); &lt;!-- 遍历那10条数据，把他们挨个放进索引里 --&gt; for (String name : products) &#123; addDoc(writer, name); &#125; writer.close(); return index;&#125; 每条数据创建一个Document，并把这个Document放进索引里。 这个Document有一个字段，叫做”name”。 TestLucene.java 第49行创建查询器，就会指定查询这个字段. 12345private static void addDoc(IndexWriter w, String name) throws IOException &#123; Document doc = new Document(); doc.add(new TextField(&quot;name&quot;, name, Field.Store.YES)); w.addDocument(doc);&#125; 创建查询器根据关键字 护眼带光源，基于 “name” 字段进行查询。 这个 “name” 字段就是在创建索引步骤里每个Document的 “name” 字段，相当于表的字段名. 12String keyword = &quot;护眼带光源&quot;;Query query = new QueryParser(&quot;name&quot;, analyzer).parse(keyword); 执行搜索12345678910&lt;!-- 创建索引 reader: --&gt;IndexReader reader = DirectoryReader.open(index);&lt;!-- 基于 reader 创建搜索器： --&gt;IndexSearcher searcher = new IndexSearcher(reader);&lt;!-- 指定每页要显示多少条数据： --&gt;int numberPerPage = 1000;System.out.printf(&quot;当前一共有%d条数据%n&quot;,productNames.size());System.out.printf(&quot;查询关键字是：\\&quot;%s\\&quot;%n&quot;,keyword);&lt;!-- 执行搜索 --&gt;ScoreDoc[] hits = searcher.search(query, numberPerPage).scoreDocs; 显示查询结果12345678910111213141516171819202122private static void showSearchResults(IndexSearcher searcher, ScoreDoc[] hits, Query query, IKAnalyzer analyzer) throws Exception &#123; System.out.println(&quot;找到 &quot; + hits.length + &quot; 个命中.&quot;); System.out.println(&quot;序号\\t匹配度得分\\t结果&quot;); &lt;!-- 每一个ScoreDoc[] hits 就是一个搜索结果，首先把他遍历出来 --&gt; for (int i = 0; i &lt; hits.length; ++i) &#123; ScoreDoc scoreDoc= hits[i]; &lt;!-- 然后获取当前结果的docid, 这个docid相当于就是这个数据在索引中的主键 --&gt; int docId = scoreDoc.doc; &lt;!-- 再根据主键docid，通过搜索器从索引里把对应的Document取出来 --&gt; Document d = searcher.doc(docId); &lt;!-- 接着就打印出这个Document里面的数据。 虽然当前Document只有name一个字段，但是代码还是通过遍历所有字段的形式，打印出里面的值，这样当Docment有多个字段的时候，代码就不用修改了，兼容性更好点。scoreDoc.score 表示当前命中的匹配度得分，越高表示匹配程度越高 --&gt; List&lt;IndexableField&gt; fields = d.getFields(); System.out.print((i + 1)); System.out.print(&quot;\\t&quot; + scoreDoc.score); for (IndexableField f : fields) &#123; System.out.print(&quot;\\t&quot; + d.get(f.name())); &#125; System.out.println(); &#125;&#125; 运行结果:如图所示，一共是10条数据，通过关键字查询出来6条命中结果，不同的命中结果有不同的匹配度得分，比如第一条，命中都就很高，既有 护眼， 也有 带光源。 其他的命中度就比较低，没有护眼关键字的匹配，只有光源关键字的匹配。 思路图:整理一下做 Lucene的思路: 首先搜集数据数据可以是文件系统，数据库，网络上，手工输入的，或者像本例直接写在内存上的 通过数据创建索引 用户输入关键字 通过关键字创建查询器 根据查询器到索引里获取数据 然后把查询结果展示在用户面前 和like的区别：like 也可以进行查询，那么使用lucene 的方式有什么区别呢？ 主要是两点： 相关度:通过观察运行结果，可以看到不同相关度的结果都会查询出来，但是使用 like，就做不到这一点了 性能:数据量小的时候，like 也会有很好的表现，但是数据量一大，like 的表现就差很多了。 在接下来的教程里会演示对 14万条数据 的查询","tags":[{"name":"Lucene","slug":"Lucene","permalink":"http://example.com/tags/Lucene/"}],"categories":[{"name":"工具和中间件","slug":"工具和中间件","permalink":"http://example.com/categories/%E5%B7%A5%E5%85%B7%E5%92%8C%E4%B8%AD%E9%97%B4%E4%BB%B6/"},{"name":"搜索引擎技术","slug":"工具和中间件/搜索引擎技术","permalink":"http://example.com/categories/%E5%B7%A5%E5%85%B7%E5%92%8C%E4%B8%AD%E9%97%B4%E4%BB%B6/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E6%8A%80%E6%9C%AF/"},{"name":"Lucene","slug":"工具和中间件/搜索引擎技术/Lucene","permalink":"http://example.com/categories/%E5%B7%A5%E5%85%B7%E5%92%8C%E4%B8%AD%E9%97%B4%E4%BB%B6/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E6%8A%80%E6%9C%AF/Lucene/"}]},{"title":"Lucene分页查询","date":"2018-10-01T16:00:00.000Z","path":"blog/工具和中间件/搜索引擎技术/Lucene/Lucene分页查询/","text":"分页查询-两种方式:Lucene 分页通常来讲有两种方式： 第一种是把100条数据查出来，然后取最后10条。 优点是快，缺点是对内存消耗大。 第二种是把第90条查询出来，然后基于这一条，通过searchAfter方法查询10条数据。 优点是内存消耗小，缺点是比第一种更慢 准备实体类来存放产品信息12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455package cn.peach;public class Product &#123; int id; String name; String category; float price; String place; String code; public int getId() &#123; return id; &#125; public void setId(int id) &#123; this.id = id; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public String getCategory() &#123; return category; &#125; public void setCategory(String category) &#123; this.category = category; &#125; public float getPrice() &#123; return price; &#125; public void setPrice(float price) &#123; this.price = price; &#125; public String getPlace() &#123; return place; &#125; public void setPlace(String place) &#123; this.place = place; &#125; public String getCode() &#123; return code; &#125; public void setCode(String code) &#123; this.code = code; &#125; @Override public String toString() &#123; return &quot;Product [id=&quot; + id + &quot;, name=&quot; + name + &quot;, category=&quot; + category + &quot;, price=&quot; + price + &quot;, place=&quot; + place + &quot;, code=&quot; + code + &quot;]&quot;; &#125;&#125; 准备工具类读取14万条数据把140k_products.txt 文本文件，转换为泛型是Product的集合 12345678910111213141516171819202122232425262728293031323334353637383940414243444546package cn.peach;import java.awt.AWTException;import java.io.File;import java.io.IOException;import java.util.ArrayList;import java.util.HashSet;import java.util.List;import java.util.Set;import org.apache.commons.io.FileUtils; public class ProductUtil &#123; public static void main(String[] args) throws IOException, InterruptedException, AWTException &#123; String fileName = &quot;140k_products.txt&quot;; List&lt;Product&gt; products = file2list(fileName); System.out.println(products.size()); &#125; public static List&lt;Product&gt; file2list(String fileName) throws IOException &#123; File f = new File(fileName); List&lt;String&gt; lines = FileUtils.readLines(f,&quot;UTF-8&quot;); List&lt;Product&gt; products = new ArrayList&lt;&gt;(); for (String line : lines) &#123; Product p = line2product(line); products.add(p); &#125; return products; &#125; private static Product line2product(String line) &#123; Product p = new Product(); String[] fields = line.split(&quot;,&quot;); p.setId(Integer.parseInt(fields[0])); p.setName(fields[1]); p.setCategory(fields[2]); p.setPrice(Float.parseFloat(fields[3])); p.setPlace(fields[4]); p.setCode(fields[5]); return p; &#125;&#125; 第一种:一共查出 pageNow*pageSize条，然后取最后pageSize条： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142package cn.peach;import java.io.IOException;import java.io.StringReader;import java.util.ArrayList;import java.util.List;import org.apache.lucene.analysis.TokenStream;import org.apache.lucene.document.Document;import org.apache.lucene.document.Field;import org.apache.lucene.document.TextField;import org.apache.lucene.index.DirectoryReader;import org.apache.lucene.index.IndexReader;import org.apache.lucene.index.IndexWriter;import org.apache.lucene.index.IndexWriterConfig;import org.apache.lucene.index.IndexableField;import org.apache.lucene.queryparser.classic.QueryParser;import org.apache.lucene.search.IndexSearcher;import org.apache.lucene.search.Query;import org.apache.lucene.search.ScoreDoc;import org.apache.lucene.search.TopDocs;import org.apache.lucene.search.highlight.Highlighter;import org.apache.lucene.search.highlight.QueryScorer;import org.apache.lucene.search.highlight.SimpleHTMLFormatter;import org.apache.lucene.store.Directory;import org.apache.lucene.store.RAMDirectory;import org.wltea.analyzer.lucene.IKAnalyzer;public class TestLucene &#123; public static void main(String[] args) throws Exception &#123; // 1. 准备中文分词器 IKAnalyzer analyzer = new IKAnalyzer(); // 2. 索引 Directory index = createIndex(analyzer); // 3. 查询器 String keyword = &quot;手机&quot;; System.out.println(&quot;当前关键字是：&quot;+keyword); Query query = new QueryParser( &quot;name&quot;, analyzer).parse(keyword); // 4. 搜索 IndexReader reader = DirectoryReader.open(index); IndexSearcher searcher=new IndexSearcher(reader); int pageNow = 1; int pageSize = 10; ScoreDoc[] hits = pageSearch1(query, searcher, pageNow, pageSize); // 5. 显示查询结果 showSearchResults(searcher, hits,query,analyzer); // 6. 关闭查询 reader.close(); &#125; private static ScoreDoc[] pageSearch1(Query query, IndexSearcher searcher, int pageNow, int pageSize) throws IOException &#123; TopDocs topDocs = searcher.search(query, pageNow*pageSize); System.out.println(&quot;查询到的总条数\\t&quot;+topDocs.totalHits); ScoreDoc [] alllScores = topDocs.scoreDocs; List&lt;ScoreDoc&gt; hitScores = new ArrayList&lt;&gt;(); int start = (pageNow -1)*pageSize ; int end = pageSize*pageNow; for(int i=start;i&lt;end;i++) hitScores.add(alllScores[i]); ScoreDoc[] hits = hitScores.toArray(new ScoreDoc[]&#123;&#125;); return hits; &#125; private static void showSearchResults(IndexSearcher searcher, ScoreDoc[] hits, Query query, IKAnalyzer analyzer) throws Exception &#123; System.out.println(&quot;找到 &quot; + hits.length + &quot; 个命中.&quot;); SimpleHTMLFormatter simpleHTMLFormatter = new SimpleHTMLFormatter(&quot;&lt;span style=&#x27;color:red&#x27;&gt;&quot;, &quot;&lt;/span&gt;&quot;); Highlighter highlighter = new Highlighter(simpleHTMLFormatter, new QueryScorer(query)); System.out.println(&quot;找到 &quot; + hits.length + &quot; 个命中.&quot;); System.out.println(&quot;序号\\t匹配度得分\\t结果&quot;); for (int i = 0; i &lt; hits.length; ++i) &#123; ScoreDoc scoreDoc= hits[i]; int docId = scoreDoc.doc; Document d = searcher.doc(docId); List&lt;IndexableField&gt; fields= d.getFields(); System.out.print((i + 1) ); System.out.print(&quot;\\t&quot; + scoreDoc.score); for (IndexableField f : fields) &#123; if(&quot;name&quot;.equals(f.name()))&#123; TokenStream tokenStream = analyzer.tokenStream(f.name(), new StringReader(d.get(f.name()))); String fieldContent = highlighter.getBestFragment(tokenStream, d.get(f.name())); System.out.print(&quot;\\t&quot;+fieldContent); &#125; else&#123; System.out.print(&quot;\\t&quot;+d.get(f.name())); &#125; &#125; System.out.println(&quot;&lt;br&gt;&quot;); &#125; &#125; private static Directory createIndex(IKAnalyzer analyzer) throws IOException &#123; Directory index = new RAMDirectory(); IndexWriterConfig config = new IndexWriterConfig(analyzer); IndexWriter writer = new IndexWriter(index, config); String fileName = &quot;140k_products.txt&quot;; List&lt;Product&gt; products = ProductUtil.file2list(fileName); int total = products.size(); int count = 0; int per = 0; int oldPer =0; for (Product p : products) &#123; addDoc(writer, p); count++; per = count*100/total; if(per!=oldPer)&#123; oldPer = per; System.out.printf(&quot;索引中，总共要添加 %d 条记录，当前添加进度是： %d%% %n&quot;,total,per); &#125; if(per&gt;10) break; &#125; writer.close(); return index; &#125; private static void addDoc(IndexWriter w, Product p) throws IOException &#123; Document doc = new Document(); doc.add(new TextField(&quot;id&quot;, String.valueOf(p.getId()), Field.Store.YES)); doc.add(new TextField(&quot;name&quot;, p.getName(), Field.Store.YES)); doc.add(new TextField(&quot;category&quot;, p.getCategory(), Field.Store.YES)); doc.add(new TextField(&quot;price&quot;, String.valueOf(p.getPrice()), Field.Store.YES)); doc.add(new TextField(&quot;place&quot;, p.getPlace(), Field.Store.YES)); doc.add(new TextField(&quot;code&quot;, p.getCode(), Field.Store.YES)); w.addDocument(doc); &#125;&#125; 第二种首先是边界条件，如果是第一页，就直接查询了。如果不是第一页，那么就取start-1那一条，然后再根据它通过searchAfter 来查询： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162package cn.peach;import java.io.IOException;import java.io.StringReader;import java.util.ArrayList;import java.util.List;import org.apache.lucene.analysis.TokenStream;import org.apache.lucene.document.Document;import org.apache.lucene.document.Field;import org.apache.lucene.document.TextField;import org.apache.lucene.index.DirectoryReader;import org.apache.lucene.index.IndexReader;import org.apache.lucene.index.IndexWriter;import org.apache.lucene.index.IndexWriterConfig;import org.apache.lucene.index.IndexableField;import org.apache.lucene.queryparser.classic.QueryParser;import org.apache.lucene.search.IndexSearcher;import org.apache.lucene.search.Query;import org.apache.lucene.search.ScoreDoc;import org.apache.lucene.search.TopDocs;import org.apache.lucene.search.highlight.Highlighter;import org.apache.lucene.search.highlight.QueryScorer;import org.apache.lucene.search.highlight.SimpleHTMLFormatter;import org.apache.lucene.store.Directory;import org.apache.lucene.store.RAMDirectory;import org.wltea.analyzer.lucene.IKAnalyzer;public class TestLucene &#123; public static void main(String[] args) throws Exception &#123; // 1. 准备中文分词器 IKAnalyzer analyzer = new IKAnalyzer(); // 2. 索引 Directory index = createIndex(analyzer); // 3. 查询器 String keyword = &quot;手机&quot;; System.out.println(&quot;当前关键字是：&quot;+keyword); Query query = new QueryParser( &quot;name&quot;, analyzer).parse(keyword); // 4. 搜索 IndexReader reader = DirectoryReader.open(index); IndexSearcher searcher=new IndexSearcher(reader); int pageNow = 1; int pageSize = 10; ScoreDoc[] hits = pageSearch2(query, searcher, pageNow, pageSize); // 5. 显示查询结果 showSearchResults(searcher, hits,query,analyzer); // 6. 关闭查询 reader.close(); &#125; private static ScoreDoc[] pageSearch1(Query query, IndexSearcher searcher, int pageNow, int pageSize) throws IOException &#123; TopDocs topDocs = searcher.search(query, pageNow*pageSize); System.out.println(&quot;查询到的总条数\\t&quot;+topDocs.totalHits); ScoreDoc [] alllScores = topDocs.scoreDocs; List&lt;ScoreDoc&gt; hitScores = new ArrayList&lt;&gt;(); int start = (pageNow -1)*pageSize ; int end = pageSize*pageNow; for(int i=start;i&lt;end;i++) hitScores.add(alllScores[i]); ScoreDoc[] hits = hitScores.toArray(new ScoreDoc[]&#123;&#125;); return hits; &#125; private static ScoreDoc[] pageSearch2(Query query, IndexSearcher searcher, int pageNow, int pageSize) throws IOException &#123; int start = (pageNow - 1) * pageSize; if(0==start)&#123; TopDocs topDocs = searcher.search(query, pageNow*pageSize); return topDocs.scoreDocs; &#125; // 查询数据， 结束页面自前的数据都会查询到，但是只取本页的数据 TopDocs topDocs = searcher.search(query, start); //获取到上一页最后一条 ScoreDoc preScore= topDocs.scoreDocs[start-1]; //查询最后一条后的数据的一页数据 topDocs = searcher.searchAfter(preScore, query, pageSize); return topDocs.scoreDocs; &#125; private static void showSearchResults(IndexSearcher searcher, ScoreDoc[] hits, Query query, IKAnalyzer analyzer) throws Exception &#123; System.out.println(&quot;找到 &quot; + hits.length + &quot; 个命中.&quot;); SimpleHTMLFormatter simpleHTMLFormatter = new SimpleHTMLFormatter(&quot;&lt;span style=&#x27;color:red&#x27;&gt;&quot;, &quot;&lt;/span&gt;&quot;); Highlighter highlighter = new Highlighter(simpleHTMLFormatter, new QueryScorer(query)); System.out.println(&quot;找到 &quot; + hits.length + &quot; 个命中.&quot;); System.out.println(&quot;序号\\t匹配度得分\\t结果&quot;); for (int i = 0; i &lt; hits.length; ++i) &#123; ScoreDoc scoreDoc= hits[i]; int docId = scoreDoc.doc; Document d = searcher.doc(docId); List&lt;IndexableField&gt; fields= d.getFields(); System.out.print((i + 1) ); System.out.print(&quot;\\t&quot; + scoreDoc.score); for (IndexableField f : fields) &#123; if(&quot;name&quot;.equals(f.name()))&#123; TokenStream tokenStream = analyzer.tokenStream(f.name(), new StringReader(d.get(f.name()))); String fieldContent = highlighter.getBestFragment(tokenStream, d.get(f.name())); System.out.print(&quot;\\t&quot;+fieldContent); &#125; else&#123; System.out.print(&quot;\\t&quot;+d.get(f.name())); &#125; &#125; System.out.println(&quot;&lt;br&gt;&quot;); &#125; &#125; private static Directory createIndex(IKAnalyzer analyzer) throws IOException &#123; Directory index = new RAMDirectory(); IndexWriterConfig config = new IndexWriterConfig(analyzer); IndexWriter writer = new IndexWriter(index, config); String fileName = &quot;140k_products.txt&quot;; List&lt;Product&gt; products = ProductUtil.file2list(fileName); int total = products.size(); int count = 0; int per = 0; int oldPer =0; for (Product p : products) &#123; addDoc(writer, p); count++; per = count*100/total; if(per!=oldPer)&#123; oldPer = per; System.out.printf(&quot;索引中，总共要添加 %d 条记录，当前添加进度是： %d%% %n&quot;,total,per); &#125; if(per&gt;10) break; &#125; writer.close(); return index; &#125; private static void addDoc(IndexWriter w, Product p) throws IOException &#123; Document doc = new Document(); doc.add(new TextField(&quot;id&quot;, String.valueOf(p.getId()), Field.Store.YES)); doc.add(new TextField(&quot;name&quot;, p.getName(), Field.Store.YES)); doc.add(new TextField(&quot;category&quot;, p.getCategory(), Field.Store.YES)); doc.add(new TextField(&quot;price&quot;, String.valueOf(p.getPrice()), Field.Store.YES)); doc.add(new TextField(&quot;place&quot;, p.getPlace(), Field.Store.YES)); doc.add(new TextField(&quot;code&quot;, p.getCode(), Field.Store.YES)); w.addDocument(doc); &#125;&#125;","tags":[{"name":"Lucene","slug":"Lucene","permalink":"http://example.com/tags/Lucene/"}],"categories":[{"name":"工具和中间件","slug":"工具和中间件","permalink":"http://example.com/categories/%E5%B7%A5%E5%85%B7%E5%92%8C%E4%B8%AD%E9%97%B4%E4%BB%B6/"},{"name":"搜索引擎技术","slug":"工具和中间件/搜索引擎技术","permalink":"http://example.com/categories/%E5%B7%A5%E5%85%B7%E5%92%8C%E4%B8%AD%E9%97%B4%E4%BB%B6/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E6%8A%80%E6%9C%AF/"},{"name":"Lucene","slug":"工具和中间件/搜索引擎技术/Lucene","permalink":"http://example.com/categories/%E5%B7%A5%E5%85%B7%E5%92%8C%E4%B8%AD%E9%97%B4%E4%BB%B6/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E6%8A%80%E6%9C%AF/Lucene/"}]},{"title":"Lucene索引删除和更新","date":"2018-10-01T16:00:00.000Z","path":"blog/工具和中间件/搜索引擎技术/Lucene/Lucene索引删除和更新/","text":"索引删除和更新索引建立好了之后，还是需要维护的，比如新增，删除和维护。 新增就是建立索引的过程。索引里的数据，其实就是一个一个的Document 对象，那么本文就是介绍如何删除和更新这些Documen对象。 删除索引123456//删除id=51173的数据IndexWriterConfig config = new IndexWriterConfig(analyzer);IndexWriter indexWriter = new IndexWriter(index, config);indexWriter.deleteDocuments(new Term(&quot;id&quot;, &quot;51173&quot;));indexWriter.commit();indexWriter.close(); 更多删除还可以按照如下方法来删除索引: DeleteDocuments(Query query):根据Query条件来删除单个或多个Document DeleteDocuments(Query[] queries):根据Query条件来删除单个或多个Document DeleteDocuments(Term term):根据Term来删除单个或多个Document DeleteDocuments(Term[] terms):根据Term来删除单个或多个Document DeleteAll():删除所有的Document 更新索引12345678910111213// 更新索引IndexWriterConfig config = new IndexWriterConfig(analyzer);IndexWriter indexWriter = new IndexWriter(index, config);Document doc = new Document();doc.add(new TextField(&quot;id&quot;, &quot;51173&quot;, Field.Store.YES));doc.add(new TextField(&quot;name&quot;, &quot;神鞭，鞭没了，神还在&quot;, Field.Store.YES));doc.add(new TextField(&quot;category&quot;, &quot;道具&quot;, Field.Store.YES));doc.add(new TextField(&quot;price&quot;, &quot;998&quot;, Field.Store.YES));doc.add(new TextField(&quot;place&quot;, &quot;南海群岛&quot;, Field.Store.YES));doc.add(new TextField(&quot;code&quot;, &quot;888888&quot;, Field.Store.YES));indexWriter.updateDocument(new Term(&quot;id&quot;, &quot;51173&quot;), doc );indexWriter.commit();indexWriter.close(); LUCENE - 进一步学习:Lucene官网展开学习：https://lucene.apache.org/","tags":[{"name":"Lucene","slug":"Lucene","permalink":"http://example.com/tags/Lucene/"}],"categories":[{"name":"工具和中间件","slug":"工具和中间件","permalink":"http://example.com/categories/%E5%B7%A5%E5%85%B7%E5%92%8C%E4%B8%AD%E9%97%B4%E4%BB%B6/"},{"name":"搜索引擎技术","slug":"工具和中间件/搜索引擎技术","permalink":"http://example.com/categories/%E5%B7%A5%E5%85%B7%E5%92%8C%E4%B8%AD%E9%97%B4%E4%BB%B6/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E6%8A%80%E6%9C%AF/"},{"name":"Lucene","slug":"工具和中间件/搜索引擎技术/Lucene","permalink":"http://example.com/categories/%E5%B7%A5%E5%85%B7%E5%92%8C%E4%B8%AD%E9%97%B4%E4%BB%B6/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E6%8A%80%E6%9C%AF/Lucene/"}]},{"title":"","date":"2017-12-14T16:00:00.000Z","path":"blog/index/","text":"Welcome TaoLiu’s Blog","tags":[],"categories":[]},{"title":"Gateway源码分析","date":"2017-12-14T16:00:00.000Z","path":"blog/Cloud/Gateway源码分析/","text":"Gateway源码:网关作为流量的入口，常用的功能包括路由转发，权限校验，限流等，Spring Cloud Gateway是Spring Cloud官方推出的由**WebFlux + Netty + Reactor实现的响应式的第二代API网关**框架，定位于取代Netflix Zuul。 Spring Cloud Gateway的核心概念：路由Route、断言、过滤器。路由是网关中最基础的部分，路由信息包括一个ID、一个目的URI、一组断言工厂、一组Filter组成，若断言为真则说明请求的URL和配置的路由匹配；Spring Cloud Gateway中的断言函数类型是Spring5.0框架中的**ServerWebExchange，允许开发者去定义匹配Http Request中的任何信息，如请求头和参数等；Spring Cloud Gateway的过滤器分为GatewayFilIer和GlobalFilter，可对请求和响应进行处理**。 Spring Cloud Gateway工作原理跟Zuul的差不多，最大的区别就是Gateway的Filter只有**pre和post两种，客户端向Spring Cloud Gateway发出请求，若请求与网关定义的路由匹配，则该请求会被发送到网关Web处理程序，此时处理程序运行特定的请求过滤器链。过滤器之间用虚线分开的原因是过滤器可能会在发送代理请求的前后执行逻辑。所有pre过滤器逻辑先执行，然后执行代理请求；代理请求完成后执行post过滤器逻辑**。 Gateway对请求处理的核心逻辑是在**DispatcherHandler中，在DispatcherHandler中依次调用HandlerMapping、HandlerAdapter、HandlerResultHandler**三个核心接口 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647public class DispatcherHandler implements WebHandler, ApplicationContextAware &#123; public Mono&lt;Void&gt; handle(ServerWebExchange exchange) &#123; if (this.handlerMappings == null) &#123; return createNotFoundError(); &#125; return Flux.fromIterable(this.handlerMappings) .concatMap(mapping -&gt; mapping.getHandler(exchange)) // 获取具体的HandlerMapping，这里返回FilteringWebHandler .next() .switchIfEmpty(createNotFoundError()) // 若路由断言匹配未匹配到，则返回Empty，这里对Empty进行处理 .flatMap(handler -&gt; invokeHandler(exchange, handler)) // 调用具体的HandlerAdapter的handle .flatMap(result -&gt; handleResult(exchange, result)); &#125; private Mono&lt;HandlerResult&gt; invokeHandler(ServerWebExchange exchange, Object handler) &#123; if (this.handlerAdapters != null) &#123; for (HandlerAdapter handlerAdapter : this.handlerAdapters) &#123; if (handlerAdapter.supports(handler)) &#123; return handlerAdapter.handle(exchange, handler); &#125; &#125; &#125; return Mono.error(new IllegalStateException(&quot;No HandlerAdapter: &quot; + handler)); &#125; private Mono&lt;Void&gt; handleResult(ServerWebExchange exchange, HandlerResult result) &#123; return getResultHandler(result).handleResult(exchange, result) .checkpoint(&quot;Handler &quot; + result.getHandler() + &quot; [DispatcherHandler]&quot;) .onErrorResume(ex -&gt; result.applyExceptionHandler(ex).flatMap(exResult -&gt; &#123; String text = &quot;Exception handler &quot; + exResult.getHandler() + &quot;, error=\\&quot;&quot; + ex.getMessage() + &quot;\\&quot; [DispatcherHandler]&quot;; return getResultHandler(exResult).handleResult(exchange, exResult).checkpoint(text); &#125;)); &#125; private HandlerResultHandler getResultHandler(HandlerResult handlerResult) &#123; if (this.resultHandlers != null) &#123; for (HandlerResultHandler resultHandler : this.resultHandlers) &#123; if (resultHandler.supports(handlerResult)) &#123; return resultHandler; &#125; &#125; &#125; throw new IllegalStateException(&quot;No HandlerResultHandler for &quot; + handlerResult.getReturnValue()); &#125; private &lt;R&gt; Mono&lt;R&gt; createNotFoundError() &#123; return Mono.defer(() -&gt; &#123; Exception ex = new ResponseStatusException(HttpStatus.NOT_FOUND, &quot;No matching handler&quot;); return Mono.error(ex); &#125;); &#125;&#125; HandlerMappingHandlerMapping负责路径到Handler的映射，Gateway中RoutePredicateHandlerMapping实现了AbstractHandlerMapping，其作用是执行所有的Route的断言工厂PredicateFactory匹配路由信息，通过断言判断路由是否可用，且将路由信息绑定到请求上下文中，最终返回**FilteringWebHandler**。 也可自定义断言工厂需继承AbstractRoutePredicateFactory类重写apply方法的逻辑。在apply方法中可以通过exchange.getRequest()拿到ServerHttpRequest对象，从而可获取到请求的参数、请求方式、请求头等信息。 12345678910111213141516public abstract class AbstractHandlerMapping extends ApplicationObjectSupport implements HandlerMapping, Ordered, BeanNameAware &#123; public Mono&lt;Object&gt; getHandler(ServerWebExchange exchange) &#123; return getHandlerInternal(exchange).map(handler -&gt; &#123; ServerHttpRequest request = exchange.getRequest(); if (hasCorsConfigurationSource(handler) || CorsUtils.isPreFlightRequest(request)) &#123; // 处理跨域问题 CorsConfiguration config = (this.corsConfigurationSource != null ? this.corsConfigurationSource.getCorsConfiguration(exchange) : null); CorsConfiguration handlerConfig = getCorsConfiguration(handler, exchange); config = (config != null ? config.combine(handlerConfig) : handlerConfig); if (!this.corsProcessor.process(config, exchange) || CorsUtils.isPreFlightRequest(request)) &#123; return REQUEST_HANDLED_HANDLER; &#125; &#125; return handler; &#125;); &#125;&#125; 首先通过**lookupRoute方法找出所有与当前请求匹配的Route，在匹配之前从RouteLocator的实现类CachingRouteLocator中已经转换好的Route，在应用启动时会通过RouteLocator的实现类RouteDefinitionRouteLocator通过PropertiesRouteDefinitionLocator从GatewayProperties中读取路由配置RouteDefinition且将其转换为Route并缓存到CachingRouteLocator中。除此之外若在DiscoveryClientRouteDefinitionLocator会获取集群中所有的实例并将其构建成RouteDefinition，最终转换并合并到CachingRouteLocator**中。 在**lookupRoute中通过遍历所有的Route，并遍历调用其具体的PredicateFactory的test方法，过滤出其test方法放回true的route。然后将匹配的路由绑定到请求上下文中。最终返回FilteringWebHandler** 1234567891011121314151617181920212223242526public class RoutePredicateHandlerMapping extends AbstractHandlerMapping &#123; protected Mono&lt;?&gt; getHandlerInternal(ServerWebExchange exchange) &#123; if (this.managementPortType == DIFFERENT &amp;&amp; this.managementPort != null &amp;&amp; exchange.getRequest().getURI().getPort() == this.managementPort) &#123; return Mono.empty(); &#125; exchange.getAttributes().put(GATEWAY_HANDLER_MAPPER_ATTR, getSimpleName()); return lookupRoute(exchange).flatMap((Function&lt;Route, Mono&lt;?&gt;&gt;) r -&gt; &#123; exchange.getAttributes().remove(GATEWAY_PREDICATE_ROUTE_ATTR); exchange.getAttributes().put(GATEWAY_ROUTE_ATTR, r); // 将匹配的路由绑定到请求上下文中，以便FilteringWebHandler的handle方法中使用 return Mono.just(webHandler); // 最终返回FilteringWebHandler &#125;).switchIfEmpty(Mono.empty().then(Mono.fromRunnable(() -&gt; &#123; // 未找到匹配的路由 exchange.getAttributes().remove(GATEWAY_PREDICATE_ROUTE_ATTR); &#125;))); &#125; protected Mono&lt;Route&gt; lookupRoute(ServerWebExchange exchange) &#123; return this.routeLocator.getRoutes().concatMap(route -&gt; Mono.just(route).filterWhen(r -&gt; &#123; exchange.getAttributes().put(GATEWAY_PREDICATE_ROUTE_ATTR, r.getId()); return r.getPredicate().apply(exchange); // 调用具体的PredicateFactory的test方法，过滤出test方法放回true的route &#125;).doOnError(e -&gt; logger.error(&quot;Error applying predicate for route: &quot; + route.getId(), e)).onErrorResume(e -&gt; Mono.empty())) .next() .map(route -&gt; &#123; validateRoute(route, exchange); return route; &#125;); &#125;&#125; HandlerAdapter调用具体的**HandlerAdapter的调用，在DelegatingWebFluxConfiguration配置类的超类WebFluxConfigurationSupport中注入了SimpleHandlerAdapter。而FilteringWebHandler是WebHandler的子类。在SimpleHandlerAdapter的handle方法中调用FilteringWebHandler的handle方法。由于SimpleHandlerAdapter返回的是Mono.empty()故不会触发handleResult**方法。 123456789101112public class SimpleHandlerAdapter implements HandlerAdapter &#123; @Override public boolean supports(Object handler) &#123; return WebHandler.class.isAssignableFrom(handler.getClass()); &#125; @Override public Mono&lt;HandlerResult&gt; handle(ServerWebExchange exchange, Object handler) &#123; WebHandler webHandler = (WebHandler) handler; Mono&lt;Void&gt; mono = webHandler.handle(exchange); return mono.then(Mono.empty()); &#125;&#125; 在**GatewayAutoConfiguration配置类中注入了FilteringWebHandler，由于全局的过滤器GlobalFilter与GatewayFilter故在其构造方法中通过适配器模式将GlobalFilter转换成了GatewayFilter。然后通过责任链模式挨个调用GatewayFilter的filter**方法。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960@Configuration(proxyBeanMethods = false)@ConditionalOnProperty(name = &quot;spring.cloud.gateway.enabled&quot;, matchIfMissing = true)@EnableConfigurationProperties@AutoConfigureBefore(&#123; HttpHandlerAutoConfiguration.class, WebFluxAutoConfiguration.class &#125;)@AutoConfigureAfter(&#123; GatewayLoadBalancerClientAutoConfiguration.class, GatewayClassPathWarningAutoConfiguration.class &#125;)@ConditionalOnClass(DispatcherHandler.class)public class GatewayAutoConfiguration &#123; public FilteringWebHandler filteringWebHandler(List&lt;GlobalFilter&gt; globalFilters) &#123; return new FilteringWebHandler(globalFilters); &#125;&#125;public class FilteringWebHandler implements WebHandler &#123; private final List&lt;GatewayFilter&gt; globalFilters; public FilteringWebHandler(List&lt;GlobalFilter&gt; globalFilters) &#123; this.globalFilters = loadFilters(globalFilters);// 通过适配器模式将GlobalFilter转换为GatewayFilter &#125; private static List&lt;GatewayFilter&gt; loadFilters(List&lt;GlobalFilter&gt; filters) &#123; return filters.stream().map(filter -&gt; &#123; GatewayFilterAdapter gatewayFilter = new GatewayFilterAdapter(filter); if (filter instanceof Ordered) &#123; int order = ((Ordered) filter).getOrder(); return new OrderedGatewayFilter(gatewayFilter, order); &#125; return gatewayFilter; &#125;).collect(Collectors.toList()); &#125; public Mono&lt;Void&gt; handle(ServerWebExchange exchange) &#123; Route route = exchange.getRequiredAttribute(GATEWAY_ROUTE_ATTR); // 从请求上下文中取出前面绑定的Route List&lt;GatewayFilter&gt; gatewayFilters = route.getFilters(); // 获取Route中配置的filters List&lt;GatewayFilter&gt; combined = new ArrayList&lt;&gt;(this.globalFilters); combined.addAll(gatewayFilters); // 合并配置的filters和自动注入的全局的filters AnnotationAwareOrderComparator.sort(combined); // 对GatewayFilter列表排序 return new DefaultGatewayFilterChain(combined).filter(exchange); &#125;&#125;private static class DefaultGatewayFilterChain implements GatewayFilterChain &#123; private final int index; private final List&lt;GatewayFilter&gt; filters; DefaultGatewayFilterChain(List&lt;GatewayFilter&gt; filters) &#123; this.filters = filters; this.index = 0; &#125; private DefaultGatewayFilterChain(DefaultGatewayFilterChain parent, int index) &#123; this.filters = parent.getFilters(); this.index = index; &#125; public Mono&lt;Void&gt; filter(ServerWebExchange exchange) &#123; return Mono.defer(() -&gt; &#123; if (this.index &lt; filters.size()) &#123; GatewayFilter filter = filters.get(this.index); DefaultGatewayFilterChain chain = new DefaultGatewayFilterChain(this, this.index + 1); return filter.filter(exchange, chain); &#125; else &#123; return Mono.empty(); // complete &#125; &#125;); &#125;&#125; 也可自定义**GatewayFilter，自定义GatewayFilter是通过自定义过滤器工厂来完成的，自定义工厂可集成一些列的AbstractGatewayFilterFactory来完成响应的功能，还可通过实现GlobalFilter来自定义全局的过滤器。对于uri支持lb://的方式类配置目标微服务的请求地址，就是通过LoadBalancerClientFilter**过滤器来完成的。 123456789101112131415161718192021222324252627282930313233343536373839public class LoadBalancerClientFilter implements GlobalFilter, Ordered &#123; public static final int LOAD_BALANCER_CLIENT_FILTER_ORDER = 10100; protected final LoadBalancerClient loadBalancer; private LoadBalancerProperties properties; public LoadBalancerClientFilter(LoadBalancerClient loadBalancer, LoadBalancerProperties properties) &#123; this.loadBalancer = loadBalancer; this.properties = properties; &#125; public int getOrder() &#123; return LOAD_BALANCER_CLIENT_FILTER_ORDER; &#125; @Override @SuppressWarnings(&quot;Duplicates&quot;) public Mono&lt;Void&gt; filter(ServerWebExchange exchange, GatewayFilterChain chain) &#123; URI url = exchange.getAttribute(GATEWAY_REQUEST_URL_ATTR); String schemePrefix = exchange.getAttribute(GATEWAY_SCHEME_PREFIX_ATTR); if (url == null || (!&quot;lb&quot;.equals(url.getScheme()) &amp;&amp; !&quot;lb&quot;.equals(schemePrefix))) &#123; return chain.filter(exchange); &#125; addOriginalRequestUrl(exchange, url); final ServiceInstance instance = choose(exchange); if (instance == null) &#123; throw NotFoundException.create(properties.isUse404(), &quot;Unable to find instance for &quot; + url.getHost()); &#125; URI uri = exchange.getRequest().getURI(); String overrideScheme = instance.isSecure() ? &quot;https&quot; : &quot;http&quot;; if (schemePrefix != null) &#123; overrideScheme = url.getScheme(); &#125; URI requestUrl = loadBalancer.reconstructURI(new DelegatingServiceInstance(instance, overrideScheme), uri); exchange.getAttributes().put(GATEWAY_REQUEST_URL_ATTR, requestUrl); return chain.filter(exchange); &#125; protected ServiceInstance choose(ServerWebExchange exchange) &#123; // 通过负载均衡算法获取具体的实例对象 return loadBalancer.choose(((URI) exchange.getAttribute(GATEWAY_REQUEST_URL_ATTR)).getHost()); &#125;&#125;","tags":[],"categories":[{"name":"Cloud","slug":"Cloud","permalink":"http://example.com/categories/Cloud/"}]},{"title":"Redis实践-Java","date":"2017-12-14T16:00:00.000Z","path":"blog/Cloud/Redis/Redis实践-Java/","text":"什么是Jedis在常见命令中，使用各种Redis自带客户端的命令行方式访问Redis服务。 而在实际工作中却需要用到Java代码才能访问，使用第三方jar包 ：Jedis就能方便地访问Redis的各种服务了。 简单运用：TestJedis：12345678910111213package redis;import redis.clients.jedis.Jedis;public class TestRedis &#123; public static void main(String[] args) &#123; Jedis jedis = new Jedis(&quot;localhost&quot;); jedis.set(&quot;foo&quot;, &quot;bar&quot;); String value = jedis.get(&quot;foo&quot;); System.out.println(value); &#125;&#125; TestRedisManyCommands:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151package redis;import java.util.HashMap;import java.util.Iterator;import java.util.List;import java.util.Map; import org.junit.Before;import org.junit.Test; import redis.clients.jedis.Jedis;import redis.clients.jedis.JedisPool; public class TestRedisManyCommands &#123; JedisPool pool; Jedis jedis; @Before public void setUp() &#123; jedis = new Jedis(&quot;localhost&quot;); &#125; /** * Redis存储初级的字符串 * CRUD */ @Test public void testBasicString()&#123; //-----添加数据---------- jedis.set(&quot;name&quot;,&quot;meepo&quot;);//向key--&gt;name中放入了value--&gt;meepo System.out.println(jedis.get(&quot;name&quot;));//执行结果：meepo //-----修改数据----------- //1、在原来基础上修改 jedis.append(&quot;name&quot;,&quot;dota&quot;); //很直观，类似map 将dota append到已经有的value之后 System.out.println(jedis.get(&quot;name&quot;));//执行结果:meepodota //2、直接覆盖原来的数据 jedis.set(&quot;name&quot;,&quot;poofu&quot;); System.out.println(jedis.get(&quot;name&quot;));//执行结果：poofu //删除key对应的记录 jedis.del(&quot;name&quot;); System.out.println(jedis.get(&quot;name&quot;));//执行结果：null /** * mset相当于 * jedis.set(&quot;name&quot;,&quot;meepo&quot;); * jedis.set(&quot;dota&quot;,&quot;poofu&quot;); */ jedis.mset(&quot;name&quot;,&quot;meepo&quot;,&quot;dota&quot;,&quot;poofu&quot;); System.out.println(jedis.mget(&quot;name&quot;,&quot;dota&quot;)); &#125; /** * jedis操作Map */ @Test public void testMap()&#123; Map&lt;String,String&gt; user=new HashMap&lt;String,String&gt;(); user.put(&quot;name&quot;,&quot;meepo&quot;); user.put(&quot;pwd&quot;,&quot;password&quot;); jedis.hmset(&quot;user&quot;,user); //取出user中的name，执行结果:[meepo]--&gt;注意结果是一个泛型的List //第一个参数是存入redis中map对象的key，后面跟的是放入map中的对象的key，后面的key可以跟多个，是可变参数 List&lt;String&gt; rsmap = jedis.hmget(&quot;user&quot;, &quot;name&quot;); System.out.println(rsmap); //删除map中的某个键值 // jedis.hdel(&quot;user&quot;,&quot;pwd&quot;); System.out.println(jedis.hmget(&quot;user&quot;, &quot;pwd&quot;)); //因为删除了，所以返回的是null System.out.println(jedis.hlen(&quot;user&quot;)); //返回key为user的键中存放的值的个数1 System.out.println(jedis.exists(&quot;user&quot;));//是否存在key为user的记录 返回true System.out.println(jedis.hkeys(&quot;user&quot;));//返回map对象中的所有key [pwd, name] System.out.println(jedis.hvals(&quot;user&quot;));//返回map对象中的所有value [meepo, password] Iterator&lt;String&gt; iter=jedis.hkeys(&quot;user&quot;).iterator(); while (iter.hasNext())&#123; String key = iter.next(); System.out.println(key+&quot;:&quot;+jedis.hmget(&quot;user&quot;,key)); &#125; &#125; /** * jedis操作List */ @Test public void testList()&#123; //开始前，先移除所有的内容 jedis.del(&quot;java framework&quot;); // 第一个是key，第二个是起始位置，第三个是结束位置，jedis.llen获取长度 -1表示取得所有 System.out.println(jedis.lrange(&quot;java framework&quot;,0,-1)); //先向key java framework中存放三条数据 jedis.lpush(&quot;java framework&quot;,&quot;spring&quot;); jedis.lpush(&quot;java framework&quot;,&quot;struts&quot;); jedis.lpush(&quot;java framework&quot;,&quot;hibernate&quot;); //再取出所有数据jedis.lrange是按范围取出， // 第一个是key，第二个是起始位置，第三个是结束位置，jedis.llen获取长度 -1表示取得所有 System.out.println(jedis.lrange(&quot;java framework&quot;,0,-1)); &#125; /** * jedis操作Set */ @Test public void testSet()&#123; //添加 jedis.sadd(&quot;sname&quot;,&quot;meepo&quot;); jedis.sadd(&quot;sname&quot;,&quot;dota&quot;); jedis.sadd(&quot;sname&quot;,&quot;poofu&quot;); jedis.sadd(&quot;sname&quot;,&quot;noname&quot;); //移除noname jedis.srem(&quot;sname&quot;,&quot;noname&quot;); System.out.println(jedis.smembers(&quot;sname&quot;));//获取所有加入的value System.out.println(jedis.sismember(&quot;sname&quot;, &quot;meepo&quot;));//判断 meepo 是否是sname集合的元素 System.out.println(jedis.srandmember(&quot;sname&quot;)); System.out.println(jedis.scard(&quot;sname&quot;));//返回集合的元素个数 &#125; @Test public void test() throws InterruptedException &#123; //keys中传入的可以用通配符 System.out.println(jedis.keys(&quot;*&quot;)); //返回当前库中所有的key [sose, sanme, name, dota, foo, sname, java framework, user, braand] System.out.println(jedis.keys(&quot;*name&quot;));//返回的sname [sname, name] System.out.println(jedis.del(&quot;sanmdde&quot;));//删除key为sanmdde的对象 删除成功返回1 删除失败（或者不存在）返回 0 System.out.println(jedis.ttl(&quot;sname&quot;));//返回给定key的有效时间，如果是-1则表示永远有效 jedis.setex(&quot;timekey&quot;, 10, &quot;min&quot;);//通过此方法，可以指定key的存活（有效时间） 时间为秒 Thread.sleep(5000);//睡眠5秒后，剩余时间将为&lt;=5 System.out.println(jedis.ttl(&quot;timekey&quot;)); //输出结果为5 jedis.setex(&quot;timekey&quot;, 1, &quot;min&quot;); //设为1后，下面再看剩余时间就是1了 System.out.println(jedis.ttl(&quot;timekey&quot;)); //输出结果为1 System.out.println(jedis.exists(&quot;key&quot;));//检查key是否存在 System.out.println(jedis.rename(&quot;timekey&quot;,&quot;time&quot;)); System.out.println(jedis.get(&quot;timekey&quot;));//因为移除，返回为null System.out.println(jedis.get(&quot;time&quot;)); //因为将timekey 重命名为time 所以可以取得值 min //jedis 排序 //注意，此处的rpush和lpush是List的操作。是一个双向链表（但从表现来看的） jedis.del(&quot;a&quot;);//先清除数据，再加入数据进行测试 jedis.rpush(&quot;a&quot;, &quot;1&quot;); jedis.lpush(&quot;a&quot;,&quot;6&quot;); jedis.lpush(&quot;a&quot;,&quot;3&quot;); jedis.lpush(&quot;a&quot;,&quot;9&quot;); System.out.println(jedis.lrange(&quot;a&quot;,0,-1));// [9, 3, 6, 1] System.out.println(jedis.sort(&quot;a&quot;)); //[1, 3, 6, 9] //输入排序后结果 System.out.println(jedis.lrange(&quot;a&quot;,0,-1)); &#125; &#125;","tags":[{"name":"Redis","slug":"Redis","permalink":"http://example.com/tags/Redis/"}],"categories":[{"name":"Cloud","slug":"Cloud","permalink":"http://example.com/categories/Cloud/"},{"name":"Redis","slug":"Cloud/Redis","permalink":"http://example.com/categories/Cloud/Redis/"}]},{"title":"Redis安装","date":"2017-12-14T16:00:00.000Z","path":"blog/Cloud/Redis/Redis安装/","text":"Redis官网： redis官网：http://redis.io windows版本的下载地址是： http://redis.io/download 点击进去之后会跳转到： https://github.com/mythz/redis-windows 是一个开源项目，所以从github上下载后，需要自己编译生成exe文件，但是为了编译生成exe文件，又需要用到Visual Studio一套。 启动服务端：redis-server.exe 启动客户端:redis-cli.exe 详细步骤：1234567891011121314151617181920212223242526272829303132333435# 安装gccyum install gcc# 把下载好的redis-5.0.3.tar.gz放在/usr/local文件夹下，并解压wget http://download.redis.io/releases/redis-5.0.3.tar.gztar xzf redis-5.0.3.tar.gzcd redis-5.0.3# 进入到解压好的redis-5.0.3目录下，进行编译与安装make# 修改配置daemonize yes # 后台启动protected-mode no # 关闭保护模式，若开启只有本机才可访问redis# bind 127.0.0.1 绑定机器网卡ip，若有多块网卡可配多个ip，代表允许客户端通过机器哪些网卡ip去访问，内网一般可不配置bind，注释掉即可# 启动服务src/redis-server redis.conf# 验证启动是否成功 ps -ef | grep redis # 进入redis客户端 src/redis-cli # 退出客户端quit# 退出redis服务pkill redis-server kill 进程号 src/redis-cli shutdown # 查看redis支持的最大连接数，在redis.conf文件中可修改，默认maxclients 10000CONFIG GET maxclients 简单运用:12set hero gareenget hero 就可以实现了向服务器设置 hero 这个键值，并从服务器获取hero对应的值","tags":[{"name":"Redis","slug":"Redis","permalink":"http://example.com/tags/Redis/"}],"categories":[{"name":"Cloud","slug":"Cloud","permalink":"http://example.com/categories/Cloud/"},{"name":"Redis","slug":"Cloud/Redis","permalink":"http://example.com/categories/Cloud/Redis/"}]},{"title":"Redis分布式锁实现","date":"2017-12-14T16:00:00.000Z","path":"blog/Cloud/Redis/Redis分布式锁实现/","text":"分布式锁的各种问题及优化并发情况下以下代码可能导致超买 12345678int stock = Integer.parseInt(stringRedisTemplate.opsForValue().get(&quot;stock&quot;)); // jedis.get(&quot;stock&quot;)if (stock &gt; 0) &#123; int realStock = stock - 1; stringRedisTemplate.opsForValue().set(&quot;stock&quot;, realStock + &quot;&quot;); // jedis.set(key,value) System.out.println(&quot;扣减成功，剩余库存:&quot; + realStock);&#125; else &#123; System.out.println(&quot;扣减失败，库存不足&quot;);&#125; 为了解决该问题可以通过redis加上分布式锁，该方式是解决了并发问题，但是引入了新的问题，若业务代码异常可能导致锁永远得不到释放。 1234567891011121314String lockKey = &quot;product_101&quot;;Boolean result = stringRedisTemplate.opsForValue().setIfAbsent(lockKey, &quot;product_id&quot;);if (!result) &#123; return &quot;error_code&quot;;&#125;int stock = Integer.parseInt(stringRedisTemplate.opsForValue().get(&quot;stock&quot;));if (stock &gt; 0) &#123; int realStock = stock - 1; stringRedisTemplate.opsForValue().set(&quot;stock&quot;, realStock + &quot;&quot;); System.out.println(&quot;扣减成功，剩余库存:&quot; + realStock);&#125; else &#123; System.out.println(&quot;扣减失败，库存不足&quot;);&#125;stringRedisTemplate.delete(lockKey); 可以通过finally中来释放锁来解决业务代码异常的情况，但若当锁获取成功后机器宕机了，同样锁还是不能得到释放。 1234567891011121314151617String lockKey = &quot;product_101&quot;;try &#123; Boolean result = stringRedisTemplate.opsForValue().setIfAbsent(lockKey, &quot;product_id&quot;); if (!result) &#123; return &quot;error_code&quot;; &#125; int stock = Integer.parseInt(stringRedisTemplate.opsForValue().get(&quot;stock&quot;)); if (stock &gt; 0) &#123; int realStock = stock - 1; stringRedisTemplate.opsForValue().set(&quot;stock&quot;, realStock + &quot;&quot;); System.out.println(&quot;扣减成功，剩余库存:&quot; + realStock); &#125; else &#123; System.out.println(&quot;扣减失败，库存不足&quot;); &#125;&#125; finally &#123; stringRedisTemplate.delete(lockKey);&#125; 可以通过给锁加上一个过期时间的方式来解决获取锁成功后机器宕机，导致锁不能被释放的情况，但是这种写法还是没有完全解决，因为加锁和设置缓存时间不是原子操作。 123456789101112131415161718String lockKey = &quot;product_101&quot;;try &#123; Boolean result = stringRedisTemplate.opsForValue().setIfAbsent(lockKey, &quot;product_id&quot;); stringRedisTemplate.expire(lockKey, 10, TimeUnit.SECONDS); if (!result) &#123; return &quot;error_code&quot;; &#125; int stock = Integer.parseInt(stringRedisTemplate.opsForValue().get(&quot;stock&quot;)); if (stock &gt; 0) &#123; int realStock = stock - 1; stringRedisTemplate.opsForValue().set(&quot;stock&quot;, realStock + &quot;&quot;); System.out.println(&quot;扣减成功，剩余库存:&quot; + realStock); &#125; else &#123; System.out.println(&quot;扣减失败，库存不足&quot;); &#125;&#125; finally &#123; stringRedisTemplate.delete(lockKey);&#125; 可通过在加锁的同时设置超时原子操作来解决该问题，但设置了超时时间若当前业务代码没有被执行完其本身没有释放锁，但由于过期锁被清理掉了，新的线程加锁进来后，之前执行业务代码的线程又去把新的线程的锁释放了，将导致锁完全失效。 1234567891011121314151617String lockKey = &quot;product_101&quot;;try &#123; Boolean result = stringRedisTemplate.opsForValue().setIfAbsent(lockKey, &quot;product_id&quot;, 30, TimeUnit.SECONDS); if (!result) &#123; return &quot;error_code&quot;; &#125; int stock = Integer.parseInt(stringRedisTemplate.opsForValue().get(&quot;stock&quot;)); if (stock &gt; 0) &#123; int realStock = stock - 1; stringRedisTemplate.opsForValue().set(&quot;stock&quot;, realStock + &quot;&quot;); System.out.println(&quot;扣减成功，剩余库存:&quot; + realStock); &#125; else &#123; System.out.println(&quot;扣减失败，库存不足&quot;); &#125;&#125; finally &#123; stringRedisTemplate.delete(lockKey);&#125; 可通过给锁设置唯一标识的方式来解决其他线程释放非自身设置的锁，所有线程只能释放本线程设置的锁。 1234567891011121314151617181920String lockKey = &quot;product_101&quot;;String clientId = UUID.randomUUID().toString();try &#123; Boolean result = stringRedisTemplate.opsForValue().setIfAbsent(lockKey, &quot;product_id&quot;, 30, TimeUnit.SECONDS); if (!result) &#123; return &quot;error_code&quot;; &#125; int stock = Integer.parseInt(stringRedisTemplate.opsForValue().get(&quot;stock&quot;)); if (stock &gt; 0) &#123; int realStock = stock - 1; stringRedisTemplate.opsForValue().set(&quot;stock&quot;, realStock + &quot;&quot;); System.out.println(&quot;扣减成功，剩余库存:&quot; + realStock); &#125; else &#123; System.out.println(&quot;扣减失败，库存不足&quot;); &#125;&#125; finally &#123; if (clientId.equals(stringRedisTemplate.opsForValue().get(lockKey))) &#123; stringRedisTemplate.delete(lockKey); &#125;&#125; 虽然上面的锁已经很完善了，但还是有锁因为超时时间导致的极小概率的并发问题，该问题可以通过给锁续命即判断业务代码是否执行完成，若未完成则重置超时时间的方式来解决该问题。**Redisson**就是这样做的。 1234567891011121314151617String lockKey = &quot;product_101&quot;;String clientId = UUID.randomUUID().toString();RLock redissonLock = redisson.getLock(lockKey);try &#123; //加锁 redissonLock.lock(); //setIfAbsent(lockKey, clientId, 30, TimeUnit.SECONDS); int stock = Integer.parseInt(stringRedisTemplate.opsForValue().get(&quot;stock&quot;)); // jedis.get(&quot;stock&quot;) if (stock &gt; 0) &#123; int realStock = stock - 1; stringRedisTemplate.opsForValue().set(&quot;stock&quot;, realStock + &quot;&quot;); // jedis.set(key,value) System.out.println(&quot;扣减成功，剩余库存:&quot; + realStock); &#125; else &#123; System.out.println(&quot;扣减失败，库存不足&quot;); &#125;&#125; finally &#123; redissonLock.unlock();&#125; 红锁**RedLock**是一种利用多Master对共享资源做互斥访问，基于N个完全独立的Redis节点，运行Redlock算法通过在客户端依次执行下面的步骤来完成获取锁的操作： 获取当前时间，毫秒数 按顺序依次向N个Redis节点执行获取锁操作，该获取操作跟前面基于单Redis节点获取锁过程相同，为了保证在某个Redis节点不可用的时候算法能够继续运行，该获取锁操作还有一个超时时间，几十毫秒量级，它要远小于锁的有效时间。客户端在向某个Redis节点获取锁失败后，应该立即尝试下一个Redis节点，这里的失败应该包含任何类型的失败，如该Redis节点不可用、该Redis节点上的锁已经被其它客户端持有 计算整个获取锁的过程总共消耗了多长时间，计算方法是用当前时间减去第1步记录的时间。若客户端从大多数Redis节点即**&gt;= N/2+1成功获取到了锁，且获取锁总耗时没有超过锁的有效时间**，则此时客户端才认为最终获取锁成功；否则认为最终获取锁失败 若最终获取锁成功，则该锁的有效时间应该重新计算，它等于最初的锁的有效时间减去第3步计算出来的获取锁消耗的时间 若最终获取锁失败了，可能由于获取到锁的Redis节点个数少于**N/2+1，或整个获取锁的过程耗时超过了锁的最初有效时间，则客户端应该立即向所有Redis节点发起释放锁操作** 释放锁的过程比较简单：客户端向所有Redis节点发起释放锁的操作，不管这些节点当时在获取锁时成功与否。在最后释放锁时，客户端应该向所有Redis节点发起释放锁的操作，即使当时向某个节点获取锁没有成功，在释放锁时也不应该漏掉该节点。因为若客户端发给某个Redis节点获取锁的请求成功到达了该Redis节点，该节点也成功执行了SET操作，但返回给客户端的响应包却丢失。在客户端看来，获取锁的请求由于超时而失败了，但在Redis这边看来，加锁已经成功了。因此释放锁时，客户端也应该对当时获取锁失败的那些Redis节点同样发起请求。 但由于N个Redis节点中的大多数能正常工作就能保证Redlock正常工作，因此理论上它的可用性更高。单Redis节点的分布式锁在failover的时锁失效的问题，在Redlock中不存在了，但若有节点发生崩溃重启，还是会对锁的安全性有影响，具体的影响程度跟Redis对数据的持久化程度有关。 假设一共有5个Redis节点**A、B、C、D、E，若客户端1成功锁住了A、B、C，获取锁成功， 但D和E没有锁住，节点C崩溃重启了，但客户端1在C上加的锁没有持久化下来，丢失了，节点C重启后，客户端2锁住了C、D、E， 获取锁成功，针对同一资源客户端1和客户端2同时获得了锁**。 Redis的**AOF持久化方式默认是每秒写一次磁盘，最坏情况下可能丢失1秒的数据，为了尽可能不丢数据，Redis允许设置成每次修改数据都进行fsync，但这会降低性能。当然，即使执行了fsync也仍然有可能丢失数据，这取决于系统而不是Redis的实现。故上面分析的由于节点重启引发的锁失效问题，总是有可能出现的。为了应对这一问题，可通过延迟重启，即一个节点崩溃后，先不立即重启它，而是等待一段时间再重启，该时间应该大于锁的有效时间，该节点在重启前所参与的锁都会过期**，它在重启后就不会对现有的锁造成影响。 Redisson锁原理123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119public class Redisson implements RedissonClient &#123; public RLock getLock(String name) &#123; return new RedissonLock(connectionManager.getCommandExecutor(), name); &#125;&#125;public class RedissonLock extends RedissonExpirable implements RLock &#123; public RedissonLock(CommandAsyncExecutor commandExecutor, String name) &#123; super(commandExecutor, name); this.commandExecutor = commandExecutor; this.id = commandExecutor.getConnectionManager().getId(); this.internalLockLeaseTime = commandExecutor.getConnectionManager().getCfg().getLockWatchdogTimeout(); &#125; public void lock(long leaseTime, TimeUnit unit) &#123; try &#123; lockInterruptibly(leaseTime, unit); &#125; catch (InterruptedException e) &#123; Thread.currentThread().interrupt(); &#125; &#125; public void lockInterruptibly(long leaseTime, TimeUnit unit) throws InterruptedException &#123; long threadId = Thread.currentThread().getId(); // 获取当前线程的ID Long ttl = tryAcquire(leaseTime, unit, threadId); // 尝试获取锁，并返回锁剩余持有时间 if (ttl == null) &#123; // 若锁剩余持有时间为null，表示获取锁成功 return; // 获取锁成功 &#125; RFuture&lt;RedissonLockEntry&gt; future = subscribe(threadId); commandExecutor.syncSubscription(future); try &#123; while (true) &#123; ttl = tryAcquire(leaseTime, unit, threadId); if (ttl == null) &#123; break; &#125; if (ttl &gt;= 0) &#123; getEntry(threadId).getLatch().tryAcquire(ttl, TimeUnit.MILLISECONDS); &#125; else &#123; getEntry(threadId).getLatch().acquire(); &#125; &#125; &#125; finally &#123; unsubscribe(future, threadId); &#125; &#125; private Long tryAcquire(long leaseTime, TimeUnit unit, long threadId) &#123; return get(tryAcquireAsync(leaseTime, unit, threadId)); &#125; private &lt;T&gt; RFuture&lt;Long&gt; tryAcquireAsync(long leaseTime, TimeUnit unit, final long threadId) &#123; if (leaseTime != -1) &#123; // 设置了超时时间的逻辑 return tryLockInnerAsync(leaseTime, unit, threadId, RedisCommands.EVAL_LONG); &#125; // 未设置超时时间默认设置超时时间为30s RFuture&lt;Long&gt; ttlRemainingFuture = tryLockInnerAsync(commandExecutor.getConnectionManager().getCfg().getLockWatchdogTimeout(), TimeUnit.MILLISECONDS, threadId, RedisCommands.EVAL_LONG); ttlRemainingFuture.addListener(new FutureListener&lt;Long&gt;() &#123; @Override public void operationComplete(Future&lt;Long&gt; future) throws Exception &#123; if (!future.isSuccess()) &#123; return; &#125; Long ttlRemaining = future.getNow(); if (ttlRemaining == null) &#123; // 若当前锁还没有释放，则给当前锁续超时时间 scheduleExpirationRenewal(threadId); &#125; &#125; &#125;); return ttlRemainingFuture; &#125; &lt;T&gt; RFuture&lt;T&gt; tryLockInnerAsync(long leaseTime, TimeUnit unit, long threadId, RedisStrictCommand&lt;T&gt; command) &#123; internalLockLeaseTime = unit.toMillis(leaseTime); // 异步执行lua命令获取锁，若获取锁成功返回null，否则返回剩余持有时间 return commandExecutor .evalWriteAsync(getName(), LongCodec.INSTANCE, command, &quot;if (redis.call(&#x27;exists&#x27;, KEYS[1]) == 0) then &quot; + // 判断锁是否存在 &quot;redis.call(&#x27;hset&#x27;, KEYS[1], ARGV[2], 1); &quot; + // 将锁的的状态设置为1 &quot;redis.call(&#x27;pexpire&#x27;, KEYS[1], ARGV[1]); &quot; + // 给锁加上失效时间 &quot;return nil; &quot; + &quot;end; &quot; + &quot;if (redis.call(&#x27;hexists&#x27;, KEYS[1], ARGV[2]) == 1) then &quot; + // 重入锁的处理，锁存在，且加锁对象是当前线程 &quot;redis.call(&#x27;hincrby&#x27;, KEYS[1], ARGV[2], 1); &quot; + // 将锁加一 &quot;redis.call(&#x27;pexpire&#x27;, KEYS[1], ARGV[1]); &quot; + // 重置失效时间 &quot;return nil; &quot; + &quot;end; &quot; + &quot;return redis.call(&#x27;pttl&#x27;, KEYS[1]);&quot;, // 返回锁剩余的失效时间 Collections.&lt;Object&gt;singletonList(getName()), internalLockLeaseTime, getLockName(threadId)); &#125; private void scheduleExpirationRenewal(final long threadId) &#123; if (expirationRenewalMap.containsKey(getEntryName())) &#123; return; &#125; // 每10s执行一次 Timeout task = commandExecutor.getConnectionManager().newTimeout(new TimerTask() &#123; @Override public void run(Timeout timeout) throws Exception &#123; RFuture&lt;Boolean&gt; future = commandExecutor .evalWriteAsync(getName(), LongCodec.INSTANCE, RedisCommands.EVAL_BOOLEAN, &quot;if (redis.call(&#x27;hexists&#x27;, KEYS[1], ARGV[2]) == 1) then &quot; + // 若KEY存在则返回true，否则返回false &quot;redis.call(&#x27;pexpire&#x27;, KEYS[1], ARGV[1]); &quot; + // 重置超时时间 &quot;return 1; &quot; + &quot;end; &quot; + &quot;return 0;&quot;, Collections.&lt;Object&gt;singletonList(getName()), internalLockLeaseTime, getLockName(threadId)); future.addListener(new FutureListener&lt;Boolean&gt;() &#123; @Override public void operationComplete(Future&lt;Boolean&gt; future) throws Exception &#123; expirationRenewalMap.remove(getEntryName()); if (!future.isSuccess()) &#123; return; &#125; if (future.getNow()) &#123; // 若当前锁还没有释放，则给当前锁续超时时间 scheduleExpirationRenewal(threadId); &#125; &#125; &#125;); &#125; &#125;, internalLockLeaseTime / 3, TimeUnit.MILLISECONDS); if (expirationRenewalMap.putIfAbsent(getEntryName(), task) != null) &#123; task.cancel(); &#125; &#125;&#125; LUA脚本Redis在2.6推出了脚本功能，允许开发者使用Lua语言编写脚本传到Redis中执行： 减少网络开销：本来5次网络请求的操作，可用一个请求完成，原先5次请求的逻辑放在redis服务器上完成。使用脚本，减少了网络往返时延。这点跟管道类似。 原子操作：Redis会将整个脚本作为一个整体执行，中间不会被其他命令插入。管道不是原子的，不过redis的批量操作命令是原子。 替代redis的事务功能：redis自带的事务功能很鸡肋，而redis的lua脚本几乎实现了常规的事务功能，官方推荐如果要使用redis的事务功能可以用redis lua替代。 可以使用EVAL命令对Lua脚本进行求值。EVAL命令的格式如下： 1EVAL script numkeys key [key ...] arg [arg ...] script参数是一段Lua脚本程序，它会被运行在Redis服务器上下文中，**numkeys参数用于指定键名参数的个数。键名参数key [key ...]从EVAL的第三个参数开始算起，表示在脚本中所用到的那些Redis键(key)，这些键名参数可在Lua中通过全局变量KEYS数组，用1为基址**的形式访问KEYS[1]，KEYS[2] 以此类推。 在命令的最后不是键名参数的附加参数**arg [arg ...]，可在Lua中通过全局变量ARGV数组访问，访问的形式和KEYS变量类似(ARGV[1]、ARGV[2]，在Lua脚本中，可使用redis.call()**函数来执行Redis命令： 1234567891011jedis.set(&quot;product_stock_10016&quot;, &quot;15&quot;); //初始化商品10016的库存String script = &quot; local count = redis.call(&#x27;get&#x27;, KEYS[1]) &quot; + &quot; local a = tonumber(count) &quot; + &quot; local b = tonumber(ARGV[1]) &quot; + &quot; if a &gt;= b then &quot; + &quot; redis.call(&#x27;set&#x27;, KEYS[1], a-b) &quot; + &quot; return 1 &quot; + &quot; end &quot; + &quot; return 0 &quot;;Object obj = jedis.eval(script, Arrays.asList(&quot;product_stock_10016&quot;), Arrays.asList(&quot;10&quot;));System.out.println(obj); 不要在Lua脚本中出现死循环和耗时的运算，否则redis会阻塞，将不接受其他的命令，所以使用时要注意不能出现死循环、耗时的运算。redis是单进程、单线程执行脚本。**管道不会阻塞redis**。","tags":[{"name":"Redis","slug":"Redis","permalink":"http://example.com/tags/Redis/"}],"categories":[{"name":"Cloud","slug":"Cloud","permalink":"http://example.com/categories/Cloud/"},{"name":"Redis","slug":"Cloud/Redis","permalink":"http://example.com/categories/Cloud/Redis/"}]},{"title":"Redis基础","date":"2017-12-14T16:00:00.000Z","path":"blog/Cloud/Redis/Redis基础/","text":"什么是Redis： Redis是一个开源的使用ANSI C语言编写、支持网络、可基于内存亦可持久化的日志型、Key-Value数据库，并提供多种语言的API。 换句话说，Redis就像是一个HashMap，不过不是在JVM中运行，而是以一个独立进程的形式运行。 一般说来，会被当作缓存使用。 因为它比数据库(mysql)快，所以常用的数据，可以考虑放在这里，这样就提高了性能。 Redis是非关系型的键值对数据库，可根据键以O(1)时间复杂度取出或插入关联值，Redis数据是存在内存中，键值对中键可以是字符串、整型、浮点型等且键唯一。值的类型可以是string、list、hash、set、sorted set等。内置了复制、磁盘持久化、LUA脚本、事务、SSL、ACLs、客户端缓存、客户端代理等功能，通过哨兵模式和Cluster模式提供高可用。 Redis的速度非常的快，单机的Redis就可以支撑每秒10几万的并发，相对于MySQL来说，性能是MySQL的几十倍。 完全基于内存操作 C语言实现，优化过的数据结构，基于几种基础的数据结构，Redis做了大量的优化，性能极高 使用单线程，无上下文的切换成本 基于非阻塞的IO多路复用机制 常见命令5种数据类型： String（字符串） List（列表） Hash（字典） Set（集合） Sorted Set（有序集合） 不同的数据类型，有不同的命令方式:String 字符串： SET key value 设置key&#x3D;value GET key 获得键key对应的值 GETRANGE key start end 得到字符串的子字符串存放在一个键 GETSET key value 设置键的字符串值，并返回旧值 GETBIT key offset 返回存储在键位值的字符串值的偏移 MGET key1 [key2..] 得到所有的给定键的值 SETBIT key offset value 设置或清除该位在存储在键的字符串值偏移 SETEX key seconds value 键到期时设置值 SETNX key value 设置键的值，只有当该键不存在 SETRANGE key offset value 覆盖字符串的一部分从指定键的偏移 STRLEN key 得到存储在键的值的长度 MSET key value [key value…] 设置多个键和多个值 MSETNX key value [key value…] 设置多个键多个值，只有在当没有按键的存在时 PSETEX key milliseconds value 设置键的毫秒值和到期时间 INCR key 增加键的整数值一次 INCRBY key increment 由给定的数量递增键的整数值 INCRBYFLOAT key increment 由给定的数量递增键的浮点值 DECR key 递减键一次的整数值 DECRBY key decrement 由给定数目递减键的整数值 APPEND key value 追加值到一个键 DEL key 如果存在删除键 DUMP key 返回存储在指定键的值的序列化版本 EXISTS key 此命令检查该键是否存在 EXPIRE key seconds 指定键的过期时间 EXPIREAT key timestamp 指定的键过期时间。在这里，时间是在Unix时间戳格式 PEXPIRE key milliseconds 设置键以毫秒为单位到期 PEXPIREAT key milliseconds-timestamp 设置键在Unix时间戳指定为毫秒到期 KEYS pattern 查找与指定模式匹配的所有键 MOVE key db 移动键到另一个数据库 PERSIST key 移除过期的键 PTTL key 以毫秒为单位获取剩余时间的到期键。 TTL key 获取键到期的剩余时间。 RANDOMKEY 从Redis返回随机键 RENAME key newkey 更改键的名称 RENAMENX key newkey 重命名键，如果新的键不存在 TYPE key 返回存储在键的数据类型的值。 List 列表： BLPOP key1 [key2 ] timeout 取出并获取列表中的第一个元素，或阻塞，直到有可用 BRPOP key1 [key2 ] timeout 取出并获取列表中的最后一个元素，或阻塞，直到有可用 BRPOPLPUSH source destination timeout 从列表中弹出一个值，它推到另一个列表并返回它;或阻塞，直到有可用 LINDEX key index 从一个列表其索引获取对应的元素 LINSERT key BEFORE|AFTER pivot value 在列表中的其他元素之后或之前插入一个元素 LLEN key 获取列表的长度 LPOP key 获取并取出列表中的第一个元素 LPUSH key value1 [value2] 在前面加上一个或多个值的列表 LPUSHX key value 在前面加上一个值列表，仅当列表中存在 LRANGE key start stop 从一个列表获取各种元素 LREM key count value 从列表中删除元素 LSET key index value 在列表中的索引设置一个元素的值 LTRIM key start stop 修剪列表到指定的范围内 RPOP key 取出并获取列表中的最后一个元素 RPOPLPUSH source destination 删除最后一个元素的列表，将其附加到另一个列表并返回它 RPUSH key value1 [value2] 添加一个或多个值到列表 RPUSHX key value 添加一个值列表，仅当列表中存在 Hash 字典，哈希表： HDEL key field[field…] 删除对象的一个或几个属性域，不存在的属性将被忽略 HEXISTS key field 查看对象是否存在该属性域 HGET key field 获取对象中该field属性域的值 HGETALL key 获取对象的所有属性域和值 HINCRBY key field value 将该对象中指定域的值增加给定的value，原子自增操作，只能是integer的属性值可以使用 HINCRBYFLOAT key field increment 将该对象中指定域的值增加给定的浮点数 HKEYS key 获取对象的所有属性字段 HVALS key 获取对象的所有属性值 HLEN key 获取对象的所有属性字段的总数 HMGET key field[field…] 获取对象的一个或多个指定字段的值 HSET key field value 设置对象指定字段的值 HMSET key field value [field value …] 同时设置对象中一个或多个字段的值 HSETNX key field value 只在对象不存在指定的字段时才设置字段的值 HSTRLEN key field 返回对象指定field的value的字符串长度，如果该对象或者field不存在，返回0. HSCAN key cursor [MATCH pattern] [COUNT count] 类似SCAN命令 Set 集合： SADD key member [member …] 添加一个或者多个元素到集合(set)里 SCARD key 获取集合里面的元素数量 SDIFF key [key …] 获得队列不存在的元素 SDIFFSTORE destination key [key …] 获得队列不存在的元素，并存储在一个关键的结果集 SINTER key [key …] 获得两个集合的交集 SINTERSTORE destination key [key …] 获得两个集合的交集，并存储在一个集合中 SISMEMBER key member 确定一个给定的值是一个集合的成员 SMEMBERS key 获取集合里面的所有key SMOVE source destination member 移动集合里面的一个key到另一个集合 SPOP key [count] 获取并删除一个集合里面的元素 SRANDMEMBER key [count] 从集合里面随机获取一个元素 SREM key member [member …] 从集合里删除一个或多个元素，不存在的元素会被忽略 SUNION key [key …] 添加多个set元素 SUNIONSTORE destination key [key …] 合并set元素，并将结果存入新的set里面 SSCAN key cursor [MATCH pattern] [COUNT count] 迭代set里面的元素 Sorted Set 有序集合： ZADD key score1 member1 [score2 member2] 添加一个或多个成员到有序集合，或者如果它已经存在更新其分数 ZCARD key 得到的有序集合成员的数量 ZCOUNT key min max 计算一个有序集合成员与给定值范围内的分数 ZINCRBY key increment member 在有序集合增加成员的分数 ZINTERSTORE destination numkeys key [key …] 多重交叉排序集合，并存储生成一个新的键有序集合。 ZLEXCOUNT key min max 计算一个给定的字典范围之间的有序集合成员的数量 ZRANGE key start stop [WITHSCORES] 由索引返回一个成员范围的有序集合（从低到高） ZRANGEBYLEX key min max [LIMIT offset count]返回一个成员范围的有序集合（由字典范围） ZRANGEBYSCORE key min max [WITHSCORES] [LIMIT] 返回有序集key中，所有 score 值介于 min 和 max 之间(包括等于 min 或 max )的成员，有序集成员按 score 值递增(从小到大)次序排列 ZRANK key member 确定成员的索引中有序集合 ZREM key member [member …] 从有序集合中删除一个或多个成员，不存在的成员将被忽略 ZREMRANGEBYLEX key min max 删除所有成员在给定的字典范围之间的有序集合 ZREMRANGEBYRANK key start stop 在给定的索引之内删除所有成员的有序集合 ZREMRANGEBYSCORE key min max 在给定的分数之内删除所有成员的有序集合 ZREVRANGE key start stop [WITHSCORES] 返回一个成员范围的有序集合，通过索引，以分数排序，从高分到低分 ZREVRANGEBYSCORE key max min [WITHSCORES] 返回一个成员范围的有序集合，以socre排序从高到低 ZREVRANK key member 确定一个有序集合成员的索引，以分数排序，从高分到低分 ZSCORE key member 获取给定成员相关联的分数在一个有序集合 ZUNIONSTORE destination numkeys key [key …] 添加多个集排序，所得排序集合存储在一个新的键 ZSCAN key cursor [MATCH pattern] [COUNT count] 增量迭代排序元素集和相关的分数 官方命令手册:如果还想查询每个命令的详细用法，请到redis官方命令手册： http://www.redis.cn/commands.html Redis备份策略 写crontab定时调度脚本，每小时都copy一份rdb或aof的备份到一个目录中去，仅仅保留最近48小时的备份； 每天都保留一份当日的数据备份到一个目录中去，可以保留最近1个月的备份，每次copy备份的时候，都把太旧的备份删除； 每天晚上将当前机器上的备份复制一份到其他机器上，以防机器损坏。 应用场景: 计数器：可对String进行自增自减运算从而实现计数器功能，这种内存型数据库读写性能非常高，很适合存储频繁读写的计数量 分布式ID生成：利用自增特性，一次请求一个大一点的步长如**incr 2000**，缓存在本地使用，用完再请求 海量数据统计：通过位图**bitmap存储是否参过某次活动，是否已读谋篇文章，用户是否为会员，日活统计** Session共享：可统一存储多台应用服务器会话信息，一个用户可请求任意一个应用服务器，从而更容易实现高可用性以及可伸缩性 分布式队列、阻塞队列：List双向链表可通过**lpush/rpush和rpop/lpop写入和读取消息，可通过使用brpop/blpop**来实现阻塞队列 分布式锁实现：使用Redis自带的**SETNX**命令实现分布式锁 热点数据存储：最新评论，最新文章列表，使用list存储，ltrim取出热点数据，删除老数据 社交类需求：可通过Set交集实现共同好友等功能，可通过Set求差集进行好友推荐、文章推荐 排行榜：**sorted_set**可实现有序性操作，从而实现排行榜等功能 延迟队列：通过**sorted_set使用当前时间戳 + 需要延迟的时长做score，消息内容作为元素，调用zadd来生产消息，消费者使用zrangbyscore获取当前时间之前的数据做轮询处理。消费完再删除任务rem key member**","tags":[{"name":"Redis","slug":"Redis","permalink":"http://example.com/tags/Redis/"}],"categories":[{"name":"Cloud","slug":"Cloud","permalink":"http://example.com/categories/Cloud/"},{"name":"Redis","slug":"Cloud/Redis","permalink":"http://example.com/categories/Cloud/Redis/"}]},{"title":"Redis集群架构","date":"2017-12-14T16:00:00.000Z","path":"blog/Cloud/Redis/Redis集群架构/","text":"主从架构：12345678910111213141516171819# 复制一份redis.conf文件port 6380pidfile /var/run/redis_6380.pid # 把pid进程号写入pidfile配置的文件logfile &quot;6380.log&quot;dir /usr/local/redis-5.0.3/data/6380 # 指定数据存放目录# 需要注释掉bind# bind 127.0.0.1 绑定机器网卡ip，多块网卡可配多个ip，代表允许客户端通过机器的哪些网卡ip去访问，内网一般可不配置bind# 配置主从复制replicaof 192.168.0.60 6379 # 从本机6379的redis实例复制数据，Redis 5.0之前使用slaveofreplica-read-only yes # 配置从节点只读# 启动从节点redis-server redis.conf# 连接从节点redis-cli -p 6380# 测试在6379实例上写数据，6380实例是否能及时同步新修改数据# 可以自己再配置一个6381的从节点 若为**master主节点配置了一个slave从节点，不管该slave从节点是否是第一次连接上Master主节点，都会发送一个PSYNC**命令给master请求复制数据。 master主节点收到**PSYNC命令后，会在后台进行数据持久化，通过bgsave生成最新的rdb快照文件，持久化期间master会继续接收客户端请求，且把这些可能修改数据集的请求缓存在内存中。当持久化进行完毕以后，master主节点会把这份rdb文件数据集发送给slave从节点，slave会把接收到的数据进行持久化生成rdb，然后再加载到内存中。master主节点再将之前缓存在内存中的命令发送给slave从节点**。 当master主节点与slave从节点之间的连接由于某些原因而断开时，slave从节点能够自动重连Master主节点，若master收到了多个slave从节点并发连接请求，它只会进行一次持久化，然后把这一份持久化的数据发送给多个并发连接的slave从节点。 当master主节点和slave从节点断开重连后，一般都会对整份数据进行复制。但从**Redis 2.8开始，PSYNC命令支持部分数据复制去master同步数据，slave从节点与master主节点能够在网络连接断开重连后只进行部分数据复制即断点续传**。 若有很多从节点，多个从节点同时复制主节点导致主节点压力过大，为了缓解主从复制风暴，可让部分从节点与从节点同步数据： 哨兵模式sentinel哨兵是特殊的redis服务，不提供读写服务，主要用来监控redis实例节点，哨兵架构下**client端第一次从哨兵找出redis的主节点，后续直接访问redis主节点，不会每次都通过sentinel哨兵代理访问redis的主节点，当redis的主节点发生变化，哨兵会第一时间感知到，并且将新的redis主节点通知给client端，redis的client端一般都实现了订阅**功能，订阅sentinel哨兵发布的节点变动消息。 12345678910111213141516171819# 复制一份sentinel.conf文件cp sentinel.conf sentinel-26379.confport 26379daemonize yespidfile &quot;/var/run/redis-sentinel-26379.pid&quot;logfile &quot;26379.log&quot;dir &quot;/usr/local/redis-5.0.3/data&quot;# sentinel monitor &lt;master-redis-name&gt; &lt;master-redis-ip&gt; &lt;master-redis-port&gt; &lt;quorum&gt;# quorum是一个数字，指明当有多少个sentinel认为一个master失效时(值一般为：sentinel总数/2 + 1)，master才算真正失效sentinel monitor mmaster 192.168.0.60 6379 2 # mmaster名字随便取，客户端访问时会用到# 启动sentinel哨兵实例src/redis-sentinel sentinel-26379.conf# 查看sentinel的info信息src/redis-cli -p 26379127.0.0.1:26379&gt;info # 可以看到Sentinel的info里已经识别出了redis的主从# 可再配置两个sentinel，端口26380和26381，注意上述配置文件里的对应数字都要修改 sentinel集群都启动完毕后，会将哨兵集群的元数据信息写入所有sentinel配置文件中，追加在文件的最下面： 1234sentinel known-replica mmaster 192.168.0.60 6380 #代表redis主节点的从节点信息sentinel known-replica mmaster 192.168.0.60 6381 #代表redis主节点的从节点信息sentinel known-sentinel mmaster 192.168.0.60 26380 52d0a5d70c1f90475b4fc03b6ce7c3c56935760f # 感知到的其它哨兵节点sentinel known-sentinel mmaster 192.168.0.60 26381 e9f530d3882f8043f76ebb8e1686438ba8bd5ca6 当redis主节点如果挂了，哨兵集群会重新选举出新的**redis主节点**，同时修改所有sentinel节点配置文件的集群元数据信息，如6379的redis挂了，假设选举出的新主节点是6380： 1234sentinel known-replica mmaster 192.168.0.60 6379 # 主节点的从节点信息sentinel known-replica mmaster 192.168.0.60 6381 # 主节点的从节点信息sentinel known-sentinel mmaster 192.168.0.60 26380 52d0a5d70c1f90475b4fc03b6ce7c3c56935760f # 感知到的其它哨兵节点sentinel known-sentinel mmaster 192.168.0.60 26381 e9f530d3882f8043f76ebb8e1686438ba8bd5ca6 同时修改sentinel文件里之前配置的mmaster对应的**6379端口，改为6380，当6379的redis实例再次启动时，哨兵集群根据集群元数据信息就可以将6379端口的redis节点作为从节点**加入集群: 1sentinel monitor mmaster 192.168.0.60 6380 2 1234567891011121314151617181920212223242526272829public class JedisSentinelTest &#123; public static void main(String[] args) throws IOException &#123; JedisPoolConfig config = new JedisPoolConfig(); config.setMaxTotal(20); config.setMaxIdle(10); config.setMinIdle(5); String masterName = &quot;mmaster&quot;; Set&lt;String&gt; sentinels = new HashSet&lt;String&gt;(); sentinels.add(new HostAndPort(&quot;172.16.20.53&quot;, 26379).toString()); sentinels.add(new HostAndPort(&quot;172.16.20.53&quot;, 26380).toString()); sentinels.add(new HostAndPort(&quot;172.16.20.53&quot;, 26381).toString()); // JedisSentinelPool其实本质跟JedisPool类似，都是与redis主节点建立的连接池 // JedisSentinelPool并不是说与sentinel建立的连接池，而是通过sentinel发现redis主节点并与其建立连接 JedisSentinelPool jedisSentinelPool = new JedisSentinelPool(masterName, sentinels, config, 3000, null); Jedis jedis = null; try &#123; jedis = jedisSentinelPool.getResource(); System.out.println(jedis.set(&quot;sentinel&quot;, &quot;eleven&quot;)); System.out.println(jedis.get(&quot;sentinel&quot;)); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; // 注意这里不是关闭连接，在JedisPool模式下，Jedis会被归还给资源池。 if (jedis != null) &#123; jedis.close(); &#125; &#125; &#125;&#125; Spring Boot整合Redis哨兵模式:只需要引入如下依赖，并将哨兵的节点信息配置到配置文件中，即可通过自动注入的方式引入**StringRedisTemplate或RedisTemplate**进行使用: 12345678&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.commons&lt;/groupId&gt; &lt;artifactId&gt;commons-pool2&lt;/artifactId&gt;&lt;/dependency&gt; 12345678910111213spring: redis: database: 0 timeout: 3000 sentinel: # 哨兵模式 master: mmaster # 主服务器所在集群名称 nodes: 192.168.0.60:26379,192.168.0.60:26380,192.168.0.60:26381 lettuce: pool: max-idle: 50 min-idle: 10 max-active: 100 max-wait: 1000 12345678910111213141516171819202122232425@RestControllerpublic class IndexController &#123; private static final Logger logger = LoggerFactory.getLogger(IndexController.class); @Autowired private StringRedisTemplate stringRedisTemplate; /** * 测试节点挂了哨兵重新选举新的master节点，客户端是否能动态感知到 * 新的master选举出来后，哨兵会把消息发布出去，客户端实际上是实现了一个消息监听机制， * 当哨兵把新master的消息发布出去，客户端会立马感知到新master的信息，从而动态切换访问的masterip */ @RequestMapping(&quot;/test_sentinel&quot;) public void testSentinel() throws InterruptedException &#123; int i = 1; while (true)&#123; try &#123; stringRedisTemplate.opsForValue().set(&quot;zhuge&quot;+i, i+&quot;&quot;); System.out.println(&quot;设置key：&quot;+ &quot;zhuge&quot; + i); i++; Thread.sleep(1000); &#125;catch (Exception e)&#123; logger.error(&quot;错误：&quot;, e); &#125; &#125; &#125;&#125; 问题：Redis 3.0以前的版本要实现集群一般是借助哨兵sentinel工具来监控master节点的状态，若master节点异常则会做主从切换，将某一台slave作为master，哨兵的配置略微复杂，且性能和高可用性等各方面表现一般，且在主从切换瞬间存在访问瞬断情况，且哨兵模式只有一个主节点对外提供服务，无法支持很高的并发，且单个主节点内存也不宜设置得过大，否则会导致持久化文件过大，影响数据恢复或主从同步的效率。 集群模式： Redis集群是一个由多个主从节点群组成的分布式服务器群，具有复制、高可用和分片特性，Redis集群不需要sentinel哨兵也能完成节点移除和故障转移的功能。只需要将每个节点设置成集群模式，这种集群模式没有中心节点可水平扩展，据官方文档称可以线性扩展到上万个节点，官方推荐不超过1000个节点。Redis集群的性能和高可用性均优于之前版本的哨兵模式，且集群配置非常简单，**Redis集群需要至少三个master主节点**。 1234567891011121314151617181920212223242526272829303132333435363738# 第一步：在第一台机器的/usr/local下创建文件夹redis-cluster，然后在其下面分别创建2个文件夾如下mkdir -p /usr/local/redis-clustermkdir 8001 8004# 把之前的redis.conf配置文件copy到8001下，修改如下内容：daemonize yesport 8001 # 分别对每个机器的端口号进行设置pidfile /var/run/redis_8001.pid # 把pid进程号写入pidfile配置的文件dir /usr/local/redis-cluster/8001/（指定数据文件存放位置，必须要指定不同的目录位置，不然会丢失数据）cluster-enabled yes（启动集群模式）cluster-config-file nodes-8001.conf（集群节点信息文件，这里800x最好和port对应上）cluster-node-timeout 10000# bind 127.0.0.1 绑定机器网卡ip，若有多块网卡可配多个ip，代表允许客户端通过机器的哪些网卡ip去访问protected-mode no # 关闭保护模式appendonly yes# 如果要设置密码需要增加如下配置：requirepass eleven # 设置redis访问密码masterauth eleven # 设置集群节点间访问密码，跟上面一致# 分别启动redis实例，然后检查是否启动成功src/redis-server redis.confps -ef | grep redis # 查看是否启动成功# 首先需要确认集群机器之间redis实例能相互访问，可先把所有机器防火墙关掉，若不关闭防火墙则需打开redis服务端口和集群节点gossip通信端口16379，默认是在redis端口号上加1W# systemctl stop firewalld # 临时关闭防火墙# systemctl disable firewalld # 禁止开机启动# 用redis-cli创建整个redis集群，redis5以前版本集群依靠ruby脚本redis-trib.rb实现# 命令中的1代表为每个创建的主服务器节点创建一个从服务器节点src/redis-cli -a zhuge --cluster create --cluster-replicas 1 192.168.0.61:8001 192.168.0.62:8002 192.168.0.63:8003 192.168.0.61:8004 192.168.0.62:8005 192.168.0.63:8006# 验证集群， -a访问服务端密码，-c表示集群模式，指定ip地址和端口号src/redis-cli -a eleven -c -h 192.168.0.61 -p 8001cluster info # 查看集群信息cluster nodes # 查看节点列表# 关闭集群则需要逐个进行关闭，使用命令：src/redis-cli -a eleven -c -h 192.168.0.60 -p 8001 shutdown 集群使用:借助redis的java客户端jedis可以操作以上集群，引用jedis版本的maven如下: 12345&lt;dependency&gt; &lt;groupId&gt;redis.clients&lt;/groupId&gt; &lt;artifactId&gt;jedis&lt;/artifactId&gt; &lt;version&gt;2.9.0&lt;/version&gt;&lt;/dependency&gt; 12345678910111213141516171819202122232425262728293031public class JedisClusterTest &#123; public static void main(String[] args) throws IOException &#123; JedisPoolConfig config = new JedisPoolConfig(); config.setMaxTotal(20); config.setMaxIdle(10); config.setMinIdle(5); Set&lt;HostAndPort&gt; jedisClusterNode = new HashSet&lt;HostAndPort&gt;(); jedisClusterNode.add(new HostAndPort(&quot;192.168.0.61&quot;, 8001)); jedisClusterNode.add(new HostAndPort(&quot;192.168.0.62&quot;, 8002)); jedisClusterNode.add(new HostAndPort(&quot;192.168.0.63&quot;, 8003)); jedisClusterNode.add(new HostAndPort(&quot;192.168.0.61&quot;, 8004)); jedisClusterNode.add(new HostAndPort(&quot;192.168.0.62&quot;, 8005)); jedisClusterNode.add(new HostAndPort(&quot;192.168.0.63&quot;, 8006)); JedisCluster jedisCluster = null; try &#123; // connectionTimeout：指的是连接一个url的连接等待时间 // soTimeout：指的是连接上一个url，获取response的返回等待时间 jedisCluster = new JedisCluster(jedisClusterNode, 6000, 5000, 10, &quot;eleven&quot;, config); System.out.println(jedisCluster.set(&quot;cluster&quot;, &quot;eleven&quot;)); System.out.println(jedisCluster.get(&quot;cluster&quot;)); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; finally &#123; if (jedisCluster != null) &#123; jedisCluster.close(); &#125; &#125; &#125;&#125; 集群的Spring Boot整合Redis连接12345678&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.commons&lt;/groupId&gt; &lt;artifactId&gt;commons-pool2&lt;/artifactId&gt;&lt;/dependency&gt; 12345678910111213spring: redis: database: 0 timeout: 3000 password: root cluster: nodes: 192.168.0.61:8001,192.168.0.62:8002,192.168.0.63:8003,192.168.0.61:8004,192.168.0.62:8005,192.168.0.63:8006 lettuce: pool: max-idle: 50 min-idle: 10 max-active: 100 max-wait: 1000 1234567891011@RestControllerpublic class IndexController &#123; @Autowired private StringRedisTemplate stringRedisTemplate; @RequestMapping(&quot;/test_cluster&quot;) public void testCluster() throws InterruptedException &#123; stringRedisTemplate.opsForValue().set(&quot;eleven&quot;, &quot;666&quot;); System.out.println(stringRedisTemplate.opsForValue().get(&quot;eleven&quot;)); &#125;&#125;","tags":[{"name":"Redis","slug":"Redis","permalink":"http://example.com/tags/Redis/"}],"categories":[{"name":"Cloud","slug":"Cloud","permalink":"http://example.com/categories/Cloud/"},{"name":"Redis","slug":"Cloud/Redis","permalink":"http://example.com/categories/Cloud/Redis/"}]},{"title":"Redis缓存及性能优化","date":"2017-12-14T16:00:00.000Z","path":"blog/Cloud/Redis/Redis缓存及性能优化/","text":"配置文件调优123456789101112131415161718192021222324252627282930313233343536373839# 列表对象listlist-max-ziplist-size -2 # 单个ziplist节点最大能存储8kb，超过则进行分裂将数据存储在新的ziplistlist-compress-depth 1 # 0表示所有节点都不压缩，1表示头结点和尾节点不压缩其他节点压缩# 哈希对象hashhash-max-ziplist-entries 512 # 元素个数超过512，将改为HashTable编码hash-max-ziplist-value 64 # 单个元素大小超过64byte，将改为HashTable编码# 集合对象setset-max-intset-entries 512 # 存储元素超过512时，使用HashTable编码# 有序集合对象zsetzset-max-ziplist-entries 128 # 元素个数超过128，将用skiplist编码zset-max-ziplist-value 64 # 单个元素大小超过64byte，将用skiplist编码# 持久化相关的save 60 1000 # 关闭RDB只需要将所有的save保存策略注释掉即可appendonly yes # 打开AOF功能appendfsync always # 每次有新命令追加到AOF文件时就执行一次fsync，非常慢也非常安全appendfsync everysec # 每秒fsync一次，足够快且在故障时只会丢失1秒钟的数据appendfsync no # 从不fsync，将数据交给操作系统来处理。更快，也更不安全的选择auto-aof-rewrite-min-size 64mb # aof文件至少达到64M才会自动重写，文件太小恢复速度本来就很快，重写意义不大auto-aof-rewrite-percentage 100 # aof文件自上一次重写后文件大小增长了100%则再次触发重写aof-use-rdb-preamble yes # 开启混合持久化，注意必须先开启aof# 集群相关的min-replicas-to-write 1 # 写数据成功最少同步的slave数量maxclients 10000 # redis支持的最大连接数maxmemory 0 # 最大可使用内存值byte，默认0不限制# volatile-lru：从已设置过期时间的key中，移出最近最少使用的key进行淘汰# volatile-ttl：从已设置过期时间的key中，根据过期时间的先后进行删除，越早过期的越先被删除# volatile-random：从已设置过期时间的key中，随机选择key淘汰# allkeys-lru：从所有key中选择最近最少使用的进行淘汰# allkeys-random：从所有key中随机选择key进行淘汰# noeviction：当内存达到阈值的时候，新写入操作报错# volatile-lfu：使用LFU算法筛选设置了过期时间的键值对删除最近一段时间被访问次数最少的数据# allkeys-lfu：使用LFU算法在所有数据中进行筛选删除最近一段时间被访问次数最少的数据maxmemory_policy noeviction # 当达到maxmemory时的淘汰策略 缓存穿透缓存穿透是指查询一个根本不存在的数据， 缓存层和存储层都不会命中， 通常出于容错的考虑， 若从存储层查不到数据则不写入缓存层。缓存穿透将导致不存在的数据每次请求都要到存储层去查询， 失去了缓存保护后端存储的意义。 缓存空对象空对象缓存过期时间设置的短一点，最长不超过5分钟 1234567891011121314151617String get(String key) &#123; // 从缓存中获取数据 String cacheValue = cache.get(key); // 缓存为空 if (StringUtils.isBlank(cacheValue)) &#123; // 从存储中获取 String storageValue = storage.get(key); cache.set(key, storageValue); // 若存储数据为空，需要设置一个过期时间(300秒) if (storageValue == null) &#123; cache.expire(key, 60 * 5); &#125; return storageValue; &#125; else &#123; return cacheValue; // 缓存非空 &#125;&#125; 布隆过滤器对于恶意攻击，向服务器请求大量不存在的数据造成的缓存穿透，可用布隆过滤器先做一次过滤，对于不存在的数据布隆过滤器一般都能够过滤掉，不让请求再往后端发送。布隆过滤器判定某个值存在时，该值可能不存在；当判定不存在时，则肯定不存在。 布隆过滤器适用于数据命中不高、 数据相对固定、 实时性低通常是数据集较大的应用场景，代码维护较为复杂，但是缓存空间占用很少。使用布隆过滤器需要把所有数据提前放入布隆过滤器，且在增加数据时也要往布隆过滤器里放。布隆过滤器不能删除数据，若要删除得重新初始化布隆过滤器。 12345&lt;dependency&gt; &lt;groupId&gt;org.redisson&lt;/groupId&gt; &lt;artifactId&gt;redisson&lt;/artifactId&gt; &lt;version&gt;3.6.5&lt;/version&gt; &lt;/dependency&gt; 1234567891011121314151617public class RedissonBloomFilter &#123; public static void main(String[] args) &#123; Config config = new Config(); config.useSingleServer().setAddress(&quot;redis://localhost:6379&quot;); // 构造Redisson RedissonClient redisson = Redisson.create(config); RBloomFilter&lt;String&gt; bloomFilter = redisson.getBloomFilter(&quot;nameList&quot;); // 初始化布隆过滤器：预计元素为100000000L，误差率为3%，根据这两个参数会计算出底层的bit数组大小 bloomFilter.tryInit(100000000L,0.03); // 将eleven插入到布隆过滤器中 bloomFilter.add(&quot;eleven&quot;); // 判断下面号码是否在布隆过滤器中 System.out.println(bloomFilter.contains(&quot;eleven&quot;)); //false System.out.println(bloomFilter.contains(&quot;张三&quot;)); //false System.out.println(bloomFilter.contains(&quot;李四&quot;)); //true &#125;&#125; 缓存失效大批量缓存在同一时间失效可能导致大量请求同时穿透缓存直达数据库，可能会造成数据库瞬间压力过大甚至挂掉，在批量增加缓存时最好将这一批数据的缓存过期时间设置为一个时间段内的不同时间。 123456789101112131415161718String get(String key) &#123; // 从缓存中获取数据 String cacheValue = cache.get(key); // 缓存为空 if (StringUtils.isBlank(cacheValue)) &#123; // 从存储中获取 String storageValue = storage.get(key); cache.set(key, storageValue); // 设置一个过期时间(300到600之间的一个随机数) int expireTime = new Random().nextInt(300) + 300; if (storageValue == null) &#123; cache.expire(key, expireTime); &#125; return storageValue; &#125; else &#123; return cacheValue; // 缓存非空 &#125;&#125; 缓存雪崩缓存雪崩是指缓存层支撑不住或宕掉后，大量请求打向后端存储层。由于缓存层承载着大量请求，有效地保护了存储层，但若缓存层由于某些原因不能提供服务，如超大并发缓存层支撑不住，或者由于缓存设计不好，类似大量请求访问**bigkey**，导致缓存能支撑的并发急剧下降，于是大量请求打到存储层，存储层调用量暴增，造成存储层也会级联宕机的情况。 预防和解决缓存雪崩问题， 可从以下三个方面进行着手。 事前：保证缓存层服务高可用性，比如使用**Redis Sentinel哨兵模式或Redis Cluster集群模式**。 事中：依赖隔离组件为后端限流熔断并降级。如使用**Sentinel或Hystrix**限流降级组件。可针对不同数据采取不同的处理方式。当业务应用访问的是非核心数据时，暂时停止从缓存中查询这些数据，而是直接返回预定义的默认降级信息、空值或是错误提示信息；当业务应用访问的是核心数据时，仍然允许查询缓存，如果缓存缺失，也可以继续通过数据库读取。 事后：开启Redis持久化机制，能尽快恢复缓存集群 提前演练。在项目上线前，演练缓存层宕掉后，应用以及后端的负载情况以及可能出现的问题，在此基础上做一些预案设定 热KEY重建优化使用缓存 + 过期时间策略既可以加速数据读写，又保证数据定期更新，这种模式基本能够满足绝大部分需求。但若当前key是一个热点key并发量非常大，或重建缓存不能在短时间完成，可能是一个复杂计算如复杂的SQL、多次IO、多个依赖等， 可能会对应用造成致命的危害。 在缓存失效的瞬间，有大量线程来重建缓存，造成后端负载加大，甚至可能会让应用崩溃，要解决该问题主要就是要避免大量线程同时重建缓存。可利用互斥锁来解决，只允许一个线程重建缓存，其他线程等待重建缓存的线程执行完，重新从缓存获取数据即可。 123456789101112131415161718192021String get(String key) &#123; // 从Redis中获取数据 String value = redis.get(key); // 如果value为空， 则开始重构缓存 if (value == null) &#123; // 只允许一个线程重建缓存， 使用nx， 并设置过期时间ex String mutexKey = &quot;mutext:key:&quot; + key; if (redis.set(mutexKey, &quot;1&quot;, &quot;ex 180&quot;, &quot;nx&quot;)) &#123; // 分布式锁 // 从数据源获取数据 value = db.get(key); // 回写Redis， 并设置过期时间 redis.setex(key, timeout, value); // 删除key_mutex redis.delete(mutexKey); &#125; else &#123; // 其他线程休息50毫秒后重试 Thread.sleep(50); get(key); &#125; &#125; return value;&#125; 缓存与数据库双写不一致在大并发下，同时操作数据库与缓存会存在数据不一致性问题 对于并发几率很小的数据，这种几乎不用考虑该问题，很少会发生缓存不一致，可给缓存数据加上过期时间，每隔一段时间触发读的主动更新即可。就算并发很高，若业务上能容忍短时间的缓存数据不一致，缓存加上过期时间依然可以解决大部分业务对于缓存的要求。 若不能容忍缓存数据不一致，可以通过加读写锁保证并发读写或写写的时候按顺序排好队，读读的时候相当于无锁。也可用阿里开源的**canal通过监听数据库的binlog日志**及时的去修改缓存，但是引入了新的中间件，增加了系统的复杂度。 以上针对的都是读多写少的情况加入缓存提高性能，若写多读多的情况又不能容忍缓存数据不一致，那就没必要加缓存了，可直接操作数据库。放入缓存的数据应该是对实时性、一致性要求不是很高的数据。切记不要为了用缓存，同时又要保证绝对的一致性做大量的过度设计和控制，增加系统复杂性。 性能优化KEY设计KEY的设计以业务名为前缀，用逗号分割，在保证语义的前提下，控制KEY的长度，不要包含空格、换行、单双引号等特殊字符。 bigkey对于value值要拒绝bigkey防止网卡流量限制以及慢查询，对于字符串类型value超过**10kb就是bigkey；非字符串类型元素个数不要超过5000；非字符串的bigkey不要使用del删除，使用hscan、sscan、zscan方式渐进式删除，同时要注意防止bigkey过期时间自动删除问题**。 bigkey会导致redis阻塞、网络拥堵等问题，每次获取要产生的网络流量较大，一般服务器会采用单机多实例的方式来部署，bigkey可能会对其他实例也造成影响。过期删除若未使用Redis 4.0的过期异步删除lazyfree-lazy-expire yes，则可能阻塞Redis。可通过bigkey拆分成几个段储存从而解决bigkey问题。 命令使用O(N)命令关注N的数量，如**hgetall、lrange、smembers、zrange、sinter等并非不能使用，但是需要明确N的值，有遍历的需求可使用hscan、sscan、zscan代替，禁止线上使用keys、flushall、flushdb**等，通过redis的rename机制禁掉命令，或者使用scan的方式渐进式处理。 redis的多数据库较弱，使用数字进行区分，很多客户端支持较差，同时多业务用多数据库实际还是单线程处理会有干扰，要合适使用数字进行分区。 过期策略惰性删除 | 被动删除当读或写key时，才对key进行检测，若已经达到过期时间，则删除。若这些过期的key没有被访问，那么他就一直无法被删除，而且一直占用内存。 定期删除 | 主动删除每隔一段时间对数据库做一次检查，删除里面的过期key。由于不可能对所有key去做轮询来删除，所以redis会每次随机取一些key去做检查和删除。 当前已用内存超过**maxmemory限定时，触发主动清理策略**。 定期+惰性都没有删除过期的key每次定期随机查询key的时候没有删掉，这些key也没有做查询的话，就会导致这些key一直保存无法被删除，这时候就会走到redis的内存淘汰机制。 volatile-lru：从已设置过期时间的key中，移出最近最少使用的key进行淘汰 volatile-ttl：从已设置过期时间的key中，根据过期时间的先后进行删除，越早过期的越先被删除 volatile-random：从已设置过期时间的key中，随机选择key淘汰 allkeys-lru：从所有key中选择最近最少使用的进行淘汰 allkeys-random：从所有key中随机选择key进行淘汰 noeviction：当内存达到阈值的时候，新写入操作报错 volatile-lfu：使用LFU算法筛选设置了过期时间的键值对删除最近一段时间被访问次数最少的数据 allkeys-lfu：使用LFU算法在所有数据中进行筛选删除最近一段时间被访问次数最少的数据 LRU &amp; LFULRU算法是以最近一次访问时间作为参考淘汰很久没被访问过的数据，LFU算法以次数作为参考淘汰最近一段时间被访问次数最少的数据。 当存在热点数据时LRU的效率很好，但偶发性、周期性的批量操作会导致LRU命中率急剧下降，缓存污染情况比较严重。这时使用LFU可能更好点。 根据自身业务类型，配置好**maxmemory-policy，默认是noeviction，推荐使用volatile-lru**。若不设置最大内存，当Redis内存超出物理内存限制时，内存数据会开始和磁盘产生频繁的交换swap，会让Redis性能急剧下降。当Redis运行在主从模式时，只有主结点才会执行过期删除策略，然后把删除操作del key同步到从结点删除数据。 连接池预热使用带有连接池的数据库，可以有效控制连接，同时提高效率 1234567891011121314151617181920212223242526List&lt;Jedis&gt; minIdleJedisList = new ArrayList&lt;Jedis&gt;(jedisPoolConfig.getMinIdle());for (int i = 0; i &lt; jedisPoolConfig.getMinIdle(); i++) &#123; Jedis jedis = null; try &#123; jedis = pool.getResource(); minIdleJedisList.add(jedis); jedis.ping(); &#125; catch (Exception e) &#123; logger.error(e.getMessage(), e); &#125; finally &#123; //注意，这里不能马上close将连接还回连接池，否则最后连接池里只会建立1个连接。。 //jedis.close(); &#125;&#125;//统一将预热的连接还回连接池for (int i = 0; i &lt; jedisPoolConfig.getMinIdle(); i++) &#123; Jedis jedis = null; try &#123; jedis = minIdleJedisList.get(i); //将连接归还回连接池 jedis.close(); &#125; catch (Exception e) &#123; logger.error(e.getMessage(), e); &#125; finally &#123; &#125;&#125;","tags":[{"name":"Redis","slug":"Redis","permalink":"http://example.com/tags/Redis/"}],"categories":[{"name":"Cloud","slug":"Cloud","permalink":"http://example.com/categories/Cloud/"},{"name":"Redis","slug":"Cloud/Redis","permalink":"http://example.com/categories/Cloud/Redis/"}]}],"categories":[{"name":"数据结构和算法","slug":"数据结构和算法","permalink":"http://example.com/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E5%92%8C%E7%AE%97%E6%B3%95/"},{"name":"工具和中间件","slug":"工具和中间件","permalink":"http://example.com/categories/%E5%B7%A5%E5%85%B7%E5%92%8C%E4%B8%AD%E9%97%B4%E4%BB%B6/"},{"name":"搜索引擎技术","slug":"工具和中间件/搜索引擎技术","permalink":"http://example.com/categories/%E5%B7%A5%E5%85%B7%E5%92%8C%E4%B8%AD%E9%97%B4%E4%BB%B6/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E6%8A%80%E6%9C%AF/"},{"name":"ElasticSearch","slug":"工具和中间件/搜索引擎技术/ElasticSearch","permalink":"http://example.com/categories/%E5%B7%A5%E5%85%B7%E5%92%8C%E4%B8%AD%E9%97%B4%E4%BB%B6/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E6%8A%80%E6%9C%AF/ElasticSearch/"},{"name":"Cloud","slug":"Cloud","permalink":"http://example.com/categories/Cloud/"},{"name":"SpringCloud","slug":"Cloud/SpringCloud","permalink":"http://example.com/categories/Cloud/SpringCloud/"},{"name":"消息队列","slug":"工具和中间件/消息队列","permalink":"http://example.com/categories/%E5%B7%A5%E5%85%B7%E5%92%8C%E4%B8%AD%E9%97%B4%E4%BB%B6/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/"},{"name":"Activemq","slug":"工具和中间件/消息队列/Activemq","permalink":"http://example.com/categories/%E5%B7%A5%E5%85%B7%E5%92%8C%E4%B8%AD%E9%97%B4%E4%BB%B6/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/Activemq/"},{"name":"Solr","slug":"工具和中间件/搜索引擎技术/Solr","permalink":"http://example.com/categories/%E5%B7%A5%E5%85%B7%E5%92%8C%E4%B8%AD%E9%97%B4%E4%BB%B6/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E6%8A%80%E6%9C%AF/Solr/"},{"name":"Lucene","slug":"工具和中间件/搜索引擎技术/Lucene","permalink":"http://example.com/categories/%E5%B7%A5%E5%85%B7%E5%92%8C%E4%B8%AD%E9%97%B4%E4%BB%B6/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E6%8A%80%E6%9C%AF/Lucene/"},{"name":"Redis","slug":"Cloud/Redis","permalink":"http://example.com/categories/Cloud/Redis/"}],"tags":[{"name":"JAVA数据结构和算法","slug":"JAVA数据结构和算法","permalink":"http://example.com/tags/JAVA%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E5%92%8C%E7%AE%97%E6%B3%95/"},{"name":"ElasticSearch","slug":"ElasticSearch","permalink":"http://example.com/tags/ElasticSearch/"},{"name":"Kibana","slug":"Kibana","permalink":"http://example.com/tags/Kibana/"},{"name":"ActiveMQ","slug":"ActiveMQ","permalink":"http://example.com/tags/ActiveMQ/"},{"name":"Solr","slug":"Solr","permalink":"http://example.com/tags/Solr/"},{"name":"Lucene","slug":"Lucene","permalink":"http://example.com/tags/Lucene/"},{"name":"Redis","slug":"Redis","permalink":"http://example.com/tags/Redis/"}]}