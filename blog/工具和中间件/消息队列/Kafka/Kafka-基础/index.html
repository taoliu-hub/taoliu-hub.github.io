<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    
    <title>Kafka-基础 | TaoLiu-Blog</title>
    
    
        <meta name="keywords" content="Kafka">
    
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <meta name="description" content="Kafka是什么?  Kafka   是   scala   语言编写的 支持partition分区、      replica多副本, 基于Zookeeper协调 的 分布式消息系统, 可 实时处理大量数据 以满足各种需求场景, 如基于hadoop批处理系统、    低延迟实时系统、    Storm&#x2F;Spark流式处理引擎, web&#x2F;nginx日志、    访问日志, 消息">
<meta property="og:type" content="article">
<meta property="og:title" content="Kafka-基础">
<meta property="og:url" content="http://example.com/blog/%E5%B7%A5%E5%85%B7%E5%92%8C%E4%B8%AD%E9%97%B4%E4%BB%B6/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/Kafka/Kafka-%E5%9F%BA%E7%A1%80/index.html">
<meta property="og:site_name" content="TaoLiu-Blog">
<meta property="og:description" content="Kafka是什么?  Kafka   是   scala   语言编写的 支持partition分区、      replica多副本, 基于Zookeeper协调 的 分布式消息系统, 可 实时处理大量数据 以满足各种需求场景, 如基于hadoop批处理系统、    低延迟实时系统、    Storm&#x2F;Spark流式处理引擎, web&#x2F;nginx日志、    访问日志, 消息">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/assets/png/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/Kafka/Anatomy-of-a-Topic.png">
<meta property="og:image" content="http://example.com/assets/png/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/Kafka/Kafka-Cluster.png">
<meta property="og:image" content="http://example.com/assets/png/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/Kafka/%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86.png">
<meta property="og:image" content="http://example.com/assets/png/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/Kafka/Kafka-zookeeper%E8%8A%82%E7%82%B9.png">
<meta property="og:image" content="http://example.com/assets/png/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/Kafka/Rebalance%E8%BF%87%E7%A8%8B.png">
<meta property="og:image" content="http://example.com/assets/png/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/Kafka/Producer%E5%8F%91%E5%B8%83%E6%B6%88%E6%81%AF%E6%9C%BA%E5%88%B6.png">
<meta property="og:image" content="http://example.com/assets/png/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/Kafka/%E6%B5%81%E8%BD%AC%E8%BF%87%E7%A8%8B.png">
<meta property="og:image" content="http://example.com/assets/png/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/Kafka/acks=1.png">
<meta property="article:published_time" content="2019-03-23T02:08:20.000Z">
<meta property="article:modified_time" content="2022-05-27T10:35:29.900Z">
<meta property="article:author" content="Tao Liu">
<meta property="article:tag" content="Kafka">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/assets/png/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/Kafka/Anatomy-of-a-Topic.png">
    

    
        <link rel="alternate" href="/atom.xml" title="TaoLiu-Blog" type="application/atom+xml">
    

    
        <link rel="icon" href="/favicon.ico">
    

    
<link rel="stylesheet" href="/libs/font-awesome/css/font-awesome.min.css">

    
<link rel="stylesheet" href="/libs/open-sans/styles.css">

    
<link rel="stylesheet" href="/libs/source-code-pro/styles.css">


    
<link rel="stylesheet" href="/css/style.css">

    
<script src="/libs/jquery/2.1.3/jquery.min.js"></script>

    
<script src="/libs/jquery/plugins/cookie/1.4.1/jquery.cookie.js"></script>

    
    
        
<link rel="stylesheet" href="/libs/lightgallery/css/lightgallery.min.css">

    
    
        
<link rel="stylesheet" href="/libs/justified-gallery/justifiedGallery.min.css">

    
    
    
    


    
        <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    
<meta name="generator" content="Hexo 5.4.2"></head>

<body>
    <div id="container">
        <header id="header">
    <div id="header-main" class="header-inner">
        <div class="outer">
            <a href="/" id="logo">
                <i class="logo"></i>
                <span class="site-title">TaoLiu-Blog</span>
            </a>
            <nav id="main-nav">
                
                    <a class="main-nav-link" href="/">Home</a>
                
                    <a class="main-nav-link" href="/archives">Archives</a>
                
                    <a class="main-nav-link" href="/categories">Categories</a>
                
                    <a class="main-nav-link" href="/tags">Tags</a>
                
                    <a class="main-nav-link" href="/about">About</a>
                
            </nav>
            
            <div id="search-form-wrap">

    <form class="search-form">
        <input type="text" class="ins-search-input search-form-input" placeholder="Search">
        <button type="submit" class="search-form-submit"></button>
    </form>
    <div class="ins-search">
    <div class="ins-search-mask"></div>
    <div class="ins-search-container">
        <div class="ins-input-wrapper">
            <input type="text" class="ins-search-input" placeholder="Type something...">
            <span class="ins-close ins-selectable"><i class="fa fa-times-circle"></i></span>
        </div>
        <div class="ins-section-wrapper">
            <div class="ins-section-container"></div>
        </div>
    </div>
</div>
<script>
(function (window) {
    var INSIGHT_CONFIG = {
        TRANSLATION: {
            POSTS: 'Posts',
            PAGES: 'Pages',
            CATEGORIES: 'Categories',
            TAGS: 'Tags',
            UNTITLED: '(Untitled)',
        },
        ROOT_URL: '/',
        CONTENT_URL: '/content.json',
    };
    window.INSIGHT_CONFIG = INSIGHT_CONFIG;
})(window);
</script>

<script src="/js/insight.js"></script>


</div>
        </div>
    </div>
    <div id="main-nav-mobile" class="header-sub header-inner">
        <table class="menu outer">
            <tr>
                
                    <td><a class="main-nav-link" href="/">Home</a></td>
                
                    <td><a class="main-nav-link" href="/archives">Archives</a></td>
                
                    <td><a class="main-nav-link" href="/categories">Categories</a></td>
                
                    <td><a class="main-nav-link" href="/tags">Tags</a></td>
                
                    <td><a class="main-nav-link" href="/about">About</a></td>
                
                <td>
                    
    <div class="search-form">
        <input type="text" class="ins-search-input search-form-input" placeholder="Search">
    </div>

                </td>
            </tr>
        </table>
    </div>
</header>

        <div class="outer">
            
            
                <aside id="sidebar">
   
        
    <div class="widget-wrap" id="categories">
        <h3 class="widget-title">
            <span>categories</span>
            &nbsp;
            <a id="allExpand" href="#">
                <i class="fa fa-angle-double-down fa-2x"></i>
            </a>
        </h3>
        
        
        
         <ul class="unstyled" id="tree"> 
                    <li class="directory">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder"></i>
                            &nbsp;
                            Cloud
                        </a>
                         <ul class="unstyled" id="tree"> 
                    <li class="directory">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder"></i>
                            &nbsp;
                            Redis
                        </a>
                         <ul class="unstyled" id="tree">  <li class="file"><a href="/blog/Cloud/Redis/Redis%E5%9F%BA%E7%A1%80/">Redis基础</a></li>  <li class="file"><a href="/blog/Cloud/Redis/Redis%E5%AE%89%E8%A3%85/">Redis安装</a></li>  <li class="file"><a href="/blog/Cloud/Redis/Redis%E9%9B%86%E7%BE%A4%E6%9E%B6%E6%9E%84/">Redis集群架构</a></li>  <li class="file"><a href="/blog/Cloud/Redis/Redis%E5%AE%9E%E8%B7%B5-Java/">Redis实践-Java</a></li>  <li class="file"><a href="/blog/Cloud/Redis/Redis%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%E5%AE%9E%E7%8E%B0/">Redis分布式锁实现</a></li>  <li class="file"><a href="/blog/Cloud/Redis/Redis%E7%BC%93%E5%AD%98%E5%8F%8A%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/">Redis缓存及性能优化</a></li>  </ul> 
                    </li> 
                    
                    <li class="directory">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder"></i>
                            &nbsp;
                            SpringCloud
                        </a>
                         <ul class="unstyled" id="tree">  <li class="file"><a href="/blog/Cloud/SpringCloud/SpringCloud/">SpringCloud介绍</a></li>  <li class="file"><a href="/blog/Cloud/SpringCloud/SpringCloud%E7%B3%BB%E5%88%971-%E7%88%B6%E5%AD%90(%E8%81%9A%E5%90%88)%E9%A1%B9%E7%9B%AE/">SpringCloud系列1-父子(聚合)项目</a></li>  <li class="file"><a href="/blog/Cloud/SpringCloud/SpringCloud%E7%B3%BB%E5%88%972-%E6%9C%8D%E5%8A%A1%E6%B3%A8%E5%86%8C%E4%B8%AD%E5%BF%83/">SpringCloud系列2-服务注册中心</a></li>  <li class="file"><a href="/blog/Cloud/SpringCloud/SpringCloud%E7%B3%BB%E5%88%973-%E6%B3%A8%E5%86%8C%E6%95%B0%E6%8D%AE%E5%BE%AE%E6%9C%8D%E5%8A%A1/">SpringCloud系列3-注册数据微服务</a></li>  <li class="file"><a href="/blog/Cloud/SpringCloud/SpringCloud%E7%B3%BB%E5%88%974-%E8%A7%86%E5%9B%BE%E5%BE%AE%E6%9C%8D%E5%8A%A1-RIBBON/">SpringCloud系列4-视图微服务-RIBBON</a></li>  <li class="file"><a href="/blog/Cloud/SpringCloud/SpringCloud%E7%B3%BB%E5%88%975-%E8%A7%86%E5%9B%BE%E5%BE%AE%E6%9C%8D%E5%8A%A1-Feign/">SpringCloud系列5-视图微服务-Feign</a></li>  <li class="file"><a href="/blog/Cloud/SpringCloud/SpringCloud%E7%B3%BB%E5%88%976-%E6%9C%8D%E5%8A%A1%E9%93%BE%E8%B7%AF%E8%BF%BD%E8%B8%AA/">SpringCloud系列6-服务链路追踪</a></li>  <li class="file"><a href="/blog/Cloud/SpringCloud/SpringCloud%E7%B3%BB%E5%88%977-%E9%85%8D%E7%BD%AE%E6%9C%8D%E5%8A%A1%E5%99%A8/">SpringCloud系列7-配置服务器</a></li>  <li class="file"><a href="/blog/Cloud/SpringCloud/SpringCloud%E7%B3%BB%E5%88%978-%E9%85%8D%E7%BD%AE%E5%AE%A2%E6%88%B7%E7%AB%AF/">SpringCloud系列8-配置客户端</a></li>  <li class="file"><a href="/blog/Cloud/SpringCloud/SpringCloud%E7%B3%BB%E5%88%979-%E6%B6%88%E6%81%AF%E6%80%BB%E7%BA%BFBus/">SpringCloud系列9-消息总线Bus</a></li>  <li class="file"><a href="/blog/Cloud/SpringCloud/SpringCloud%E7%B3%BB%E5%88%9710-%E6%96%AD%E8%B7%AF%E5%99%A8Hystrix/">SpringCloud系列10-断路器Hystrix</a></li>  <li class="file"><a href="/blog/Cloud/SpringCloud/SpringCloud%E7%B3%BB%E5%88%9711-%E6%96%AD%E8%B7%AF%E5%99%A8%E7%9B%91%E6%8E%A7/">SpringCloud系列11-断路器监控</a></li>  <li class="file"><a href="/blog/Cloud/SpringCloud/SpringCloud%E7%B3%BB%E5%88%9712-%E6%96%AD%E8%B7%AF%E5%99%A8%E8%81%9A%E5%90%88%E7%9B%91%E6%8E%A7/">SpringCloud系列12-断路器聚合监控</a></li>  <li class="file"><a href="/blog/Cloud/SpringCloud/SpringCloud%E7%B3%BB%E5%88%9713-%E7%BD%91%E5%85%B3Zuul/">SpringCloud系列13-网关Zuul</a></li>  <li class="file"><a href="/blog/Cloud/SpringCloud/SpringCloud%E7%B3%BB%E5%88%9714-%E6%80%BB%E7%BB%93/">SpringCloud系列14-总结</a></li>  </ul> 
                    </li> 
                     <li class="file"><a href="/blog/Cloud/Gateway%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/">Gateway源码分析</a></li>  </ul> 
                    </li> 
                    
                    <li class="directory open">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder-open"></i>
                            &nbsp;
                            工具和中间件
                        </a>
                         <ul class="unstyled" id="tree"> 
                    <li class="directory">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder"></i>
                            &nbsp;
                            搜索引擎技术
                        </a>
                         <ul class="unstyled" id="tree"> 
                    <li class="directory">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder"></i>
                            &nbsp;
                            ElasticSearch
                        </a>
                         <ul class="unstyled" id="tree">  <li class="file"><a href="/blog/%E5%B7%A5%E5%85%B7%E5%92%8C%E4%B8%AD%E9%97%B4%E4%BB%B6/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E6%8A%80%E6%9C%AF/ElasticSearch/ElasticSearch%E5%9F%BA%E7%A1%80/">ElasticSearch基础</a></li>  <li class="file"><a href="/blog/%E5%B7%A5%E5%85%B7%E5%92%8C%E4%B8%AD%E9%97%B4%E4%BB%B6/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E6%8A%80%E6%9C%AF/ElasticSearch/ElasticSearch%E5%AE%9E%E6%88%98/">ElasticSearch实战</a></li>  <li class="file"><a href="/blog/%E5%B7%A5%E5%85%B7%E5%92%8C%E4%B8%AD%E9%97%B4%E4%BB%B6/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E6%8A%80%E6%9C%AF/ElasticSearch/ElasticSearch%E8%BF%9B%E9%98%B6/">ElasticSearch进阶</a></li>  <li class="file"><a href="/blog/%E5%B7%A5%E5%85%B7%E5%92%8C%E4%B8%AD%E9%97%B4%E4%BB%B6/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E6%8A%80%E6%9C%AF/ElasticSearch/ElasticSearch%E5%B7%A5%E5%85%B7-Kibana/">ElasticSearch工具-Kibana</a></li>  <li class="file"><a href="/blog/%E5%B7%A5%E5%85%B7%E5%92%8C%E4%B8%AD%E9%97%B4%E4%BB%B6/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E6%8A%80%E6%9C%AF/ElasticSearch/ElasticSearch-Springboot/">ElasticSearch-Springboot</a></li>  </ul> 
                    </li> 
                    
                    <li class="directory">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder"></i>
                            &nbsp;
                            Lucene
                        </a>
                         <ul class="unstyled" id="tree">  <li class="file"><a href="/blog/%E5%B7%A5%E5%85%B7%E5%92%8C%E4%B8%AD%E9%97%B4%E4%BB%B6/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E6%8A%80%E6%9C%AF/Lucene/Lucene%E5%9F%BA%E7%A1%80/">Lucene基础</a></li>  <li class="file"><a href="/blog/%E5%B7%A5%E5%85%B7%E5%92%8C%E4%B8%AD%E9%97%B4%E4%BB%B6/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E6%8A%80%E6%9C%AF/Lucene/Lucene%E5%88%86%E8%AF%8D%E5%99%A8/">Lucene分词器</a></li>  <li class="file"><a href="/blog/%E5%B7%A5%E5%85%B7%E5%92%8C%E4%B8%AD%E9%97%B4%E4%BB%B6/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E6%8A%80%E6%9C%AF/Lucene/Lucene%E5%88%86%E9%A1%B5%E6%9F%A5%E8%AF%A2/">Lucene分页查询</a></li>  <li class="file"><a href="/blog/%E5%B7%A5%E5%85%B7%E5%92%8C%E4%B8%AD%E9%97%B4%E4%BB%B6/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E6%8A%80%E6%9C%AF/Lucene/Lucene%E7%B4%A2%E5%BC%95%E5%88%A0%E9%99%A4%E5%92%8C%E6%9B%B4%E6%96%B0/">Lucene索引删除和更新</a></li>  </ul> 
                    </li> 
                    
                    <li class="directory">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder"></i>
                            &nbsp;
                            Solr
                        </a>
                         <ul class="unstyled" id="tree">  <li class="file"><a href="/blog/%E5%B7%A5%E5%85%B7%E5%92%8C%E4%B8%AD%E9%97%B4%E4%BB%B6/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E6%8A%80%E6%9C%AF/Solr/Solr%E5%9F%BA%E7%A1%80/">Solr基础</a></li>  <li class="file"><a href="/blog/%E5%B7%A5%E5%85%B7%E5%92%8C%E4%B8%AD%E9%97%B4%E4%BB%B6/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E6%8A%80%E6%9C%AF/Solr/Solr%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D%E5%99%A8IKAnalyzer/">Solr中文分词器IKAnalyzer</a></li>  <li class="file"><a href="/blog/%E5%B7%A5%E5%85%B7%E5%92%8C%E4%B8%AD%E9%97%B4%E4%BB%B6/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E6%8A%80%E6%9C%AF/Solr/Solr%E8%AE%BE%E7%BD%AE%E5%AD%97%E6%AE%B5/">Solr设置字段</a></li>  <li class="file"><a href="/blog/%E5%B7%A5%E5%85%B7%E5%92%8C%E4%B8%AD%E9%97%B4%E4%BB%B6/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E6%8A%80%E6%9C%AF/Solr/Solr%E5%88%9B%E5%BB%BA%E7%B4%A2%E5%BC%95/">Solr创建索引</a></li>  <li class="file"><a href="/blog/%E5%B7%A5%E5%85%B7%E5%92%8C%E4%B8%AD%E9%97%B4%E4%BB%B6/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E6%8A%80%E6%9C%AF/Solr/Solr%E5%88%86%E9%A1%B5%E6%9F%A5%E8%AF%A2-Solrj/">Solr分页查询-Solrj</a></li>  <li class="file"><a href="/blog/%E5%B7%A5%E5%85%B7%E5%92%8C%E4%B8%AD%E9%97%B4%E4%BB%B6/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E6%8A%80%E6%9C%AF/Solr/Solr%E9%AB%98%E4%BA%AE%E6%98%BE%E7%A4%BA/">Solr高亮显示</a></li>  <li class="file"><a href="/blog/%E5%B7%A5%E5%85%B7%E5%92%8C%E4%B8%AD%E9%97%B4%E4%BB%B6/%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E6%8A%80%E6%9C%AF/Solr/Solr%E6%9B%B4%E6%96%B0%E5%92%8C%E5%88%A0%E9%99%A4%E7%B4%A2%E5%BC%95/">Solr更新和删除索引</a></li>  </ul> 
                    </li> 
                     </ul> 
                    </li> 
                    
                    <li class="directory open">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder-open"></i>
                            &nbsp;
                            消息队列
                        </a>
                         <ul class="unstyled" id="tree"> 
                    <li class="directory">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder"></i>
                            &nbsp;
                            Activemq
                        </a>
                         <ul class="unstyled" id="tree">  <li class="file"><a href="/blog/%E5%B7%A5%E5%85%B7%E5%92%8C%E4%B8%AD%E9%97%B4%E4%BB%B6/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/Activemq/ActiveMQ-%E5%9F%BA%E7%A1%80/">ActiveMQ-基础</a></li>  <li class="file"><a href="/blog/%E5%B7%A5%E5%85%B7%E5%92%8C%E4%B8%AD%E9%97%B4%E4%BB%B6/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/Activemq/ActiveMQ-%E9%98%9F%E5%88%97%E6%A8%A1%E5%BC%8F/">ActiveMQ-队列模式</a></li>  <li class="file"><a href="/blog/%E5%B7%A5%E5%85%B7%E5%92%8C%E4%B8%AD%E9%97%B4%E4%BB%B6/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/Activemq/ActiveMQ-%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%BC%8F/">ActiveMQ-主题模式</a></li>  <li class="file"><a href="/blog/%E5%B7%A5%E5%85%B7%E5%92%8C%E4%B8%AD%E9%97%B4%E4%BB%B6/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/Activemq/ActiveMQ-Spring%E6%96%B9%E5%BC%8F/">ActiveMQ-Spring方式</a></li>  </ul> 
                    </li> 
                    
                    <li class="directory open">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder-open"></i>
                            &nbsp;
                            Kafka
                        </a>
                         <ul class="unstyled" id="tree">  <li class="file active"><a href="/blog/%E5%B7%A5%E5%85%B7%E5%92%8C%E4%B8%AD%E9%97%B4%E4%BB%B6/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/Kafka/Kafka-%E5%9F%BA%E7%A1%80/">Kafka-基础</a></li>  </ul> 
                    </li> 
                    
                    <li class="directory">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder"></i>
                            &nbsp;
                            RabbitMQ
                        </a>
                         <ul class="unstyled" id="tree">  <li class="file"><a href="/blog/%E5%B7%A5%E5%85%B7%E5%92%8C%E4%B8%AD%E9%97%B4%E4%BB%B6/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/RabbitMQ/RabbitMQ-%E5%9F%BA%E7%A1%80/">RabbitMQ-基础</a></li>  <li class="file"><a href="/blog/%E5%B7%A5%E5%85%B7%E5%92%8C%E4%B8%AD%E9%97%B4%E4%BB%B6/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/RabbitMQ/RabbitMQ-fanout%E6%A8%A1%E5%BC%8F/">RabbitMQ-fanout模式</a></li>  <li class="file"><a href="/blog/%E5%B7%A5%E5%85%B7%E5%92%8C%E4%B8%AD%E9%97%B4%E4%BB%B6/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/RabbitMQ/RabbitMQ-direct%E6%A8%A1%E5%BC%8F/">RabbitMQ-direct模式</a></li>  <li class="file"><a href="/blog/%E5%B7%A5%E5%85%B7%E5%92%8C%E4%B8%AD%E9%97%B4%E4%BB%B6/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/RabbitMQ/RabbitMQ-topic%E6%A8%A1%E5%BC%8F/">RabbitMQ-topic模式</a></li>  <li class="file"><a href="/blog/%E5%B7%A5%E5%85%B7%E5%92%8C%E4%B8%AD%E9%97%B4%E4%BB%B6/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/RabbitMQ/RabbitMQ-%E9%AB%98%E7%BA%A7%E7%89%B9%E6%80%A7/">RabbitMQ-高级特性</a></li>  <li class="file"><a href="/blog/%E5%B7%A5%E5%85%B7%E5%92%8C%E4%B8%AD%E9%97%B4%E4%BB%B6/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/RabbitMQ/RabbitMQ-Spring%E9%9B%86%E6%88%90/">RabbitMQ-Spring集成</a></li>  </ul> 
                    </li> 
                     <li class="file"><a href="/blog/%E5%B7%A5%E5%85%B7%E5%92%8C%E4%B8%AD%E9%97%B4%E4%BB%B6/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/">消息队列</a></li>  </ul> 
                    </li> 
                     </ul> 
                    </li> 
                    
                    <li class="directory">
                        <a href="#" data-role="directory">
                            <i class="fa fa-folder"></i>
                            &nbsp;
                            数据结构和算法
                        </a>
                         <ul class="unstyled" id="tree">  <li class="file"><a href="/blog/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E5%92%8C%E7%AE%97%E6%B3%95/JAVA%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E5%92%8C%E7%AE%97%E6%B3%95/">JAVA数据结构和算法</a></li>  </ul> 
                    </li> 
                     <li class="file"><a href="/blog/hello-hexo/">Hello hexo</a></li>  <li class="file"><a href="/blog/index/">Welcome TaoLiu's Blog</a></li>  </ul> 
    </div>
    <script>
        $(document).ready(function() {
            var iconFolderOpenClass  = 'fa-folder-open';
            var iconFolderCloseClass = 'fa-folder';
            var iconAllExpandClass = 'fa-angle-double-down';
            var iconAllPackClass = 'fa-angle-double-up';
            // Handle directory-tree expansion:
            // 左键单独展开目录
            $(document).on('click', '#categories a[data-role="directory"]', function (event) {
                event.preventDefault();

                var icon = $(this).children('.fa');
                var expanded = icon.hasClass(iconFolderOpenClass);
                var subtree = $(this).siblings('ul');
                icon.removeClass(iconFolderOpenClass).removeClass(iconFolderCloseClass);
                if (expanded) {
                    if (typeof subtree != 'undefined') {
                        subtree.slideUp({ duration: 100 });
                    }
                    icon.addClass(iconFolderCloseClass);
                } else {
                    if (typeof subtree != 'undefined') {
                        subtree.slideDown({ duration: 100 });
                    }
                    icon.addClass(iconFolderOpenClass);
                }
            });
            // 右键展开下属所有目录
            $('#categories a[data-role="directory"]').bind("contextmenu", function(event){
                event.preventDefault();
                
                var icon = $(this).children('.fa');
                var expanded = icon.hasClass(iconFolderOpenClass);
                var listNode = $(this).siblings('ul');
                var subtrees = $.merge(listNode.find('li ul'), listNode);
                var icons = $.merge(listNode.find('.fa'), icon);
                icons.removeClass(iconFolderOpenClass).removeClass(iconFolderCloseClass);
                if(expanded) {
                    subtrees.slideUp({ duration: 100 });
                    icons.addClass(iconFolderCloseClass);
                } else {
                    subtrees.slideDown({ duration: 100 });
                    icons.addClass(iconFolderOpenClass);
                }
            })
            // 展开关闭所有目录按钮
            $(document).on('click', '#allExpand', function (event) {
                event.preventDefault();
                
                var icon = $(this).children('.fa');
                var expanded = icon.hasClass(iconAllExpandClass);
                icon.removeClass(iconAllExpandClass).removeClass(iconAllPackClass);
                if(expanded) {
                    $('#sidebar .fa.fa-folder').removeClass('fa-folder').addClass('fa-folder-open')
                    $('#categories li ul').slideDown({ duration: 100 });
                    icon.addClass(iconAllPackClass);
                } else {
                    $('#sidebar .fa.fa-folder-open').removeClass('fa-folder-open').addClass('fa-folder')
                    $('#categories li ul').slideUp({ duration: 100 });
                    icon.addClass(iconAllExpandClass);
                }
            });  
        });
    </script>

    
    <div id="toTop" class="fa fa-angle-up"></div>
</aside>
            
            <section id="main"><article id="post-工具和中间件/消息队列/Kafka/Kafka-基础" class="article article-type-post" itemscope itemprop="blogPost">
    <div class="article-inner">
        
        
            <header class="article-header">
                
                    <div class="article-meta">
                        
    <div class="article-category">
    	<i class="fa fa-folder"></i>
        <a class="article-category-link" href="/categories/%E5%B7%A5%E5%85%B7%E5%92%8C%E4%B8%AD%E9%97%B4%E4%BB%B6/">工具和中间件</a><i class="fa fa-angle-right"></i><a class="article-category-link" href="/categories/%E5%B7%A5%E5%85%B7%E5%92%8C%E4%B8%AD%E9%97%B4%E4%BB%B6/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/">消息队列</a><i class="fa fa-angle-right"></i><a class="article-category-link" href="/categories/%E5%B7%A5%E5%85%B7%E5%92%8C%E4%B8%AD%E9%97%B4%E4%BB%B6/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/Kafka/">Kafka</a>
    </div>

                        
    <div class="article-tag">
        <i class="fa fa-tag"></i>
        <a class="tag-link-link" href="/tags/Kafka/" rel="tag">Kafka</a>
    </div>

                        
    <div class="article-date">
        <i class="fa fa-calendar"></i>
        <a href="/blog/%E5%B7%A5%E5%85%B7%E5%92%8C%E4%B8%AD%E9%97%B4%E4%BB%B6/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/Kafka/Kafka-%E5%9F%BA%E7%A1%80/">
            <time datetime="2019-03-23T02:08:20.000Z" itemprop="datePublished">2019-03-23</time>
        </a>
    </div>


                        
                            <i class="fa fa-bar-chart"></i>
                            <span id="busuanzi_container_site_pv"><span id="busuanzi_value_page_pv"></span></span>    
                        
                        
                    </div>
                
                
    
        <h1 class="article-title" itemprop="name">
            Kafka-基础
        </h1>
    

            </header>
        
        
        <div class="article-entry" itemprop="articleBody">
        
        
            
        
        
            <h2 id="Kafka是什么"><a href="#Kafka是什么" class="headerlink" title="Kafka是什么?"></a>Kafka是什么?</h2><p>  <strong><code>Kafka</code></strong>   是   <strong><code>scala</code></strong>   语言编写的 <strong>支持<code>partition</code>分区</strong>、      <strong><code>replica</code>多副本</strong>, <strong>基于<code>Zookeeper</code>协调</strong> 的 <strong>分布式消息系统</strong>, 可 <strong>实时处理大量数据</strong> 以满足各种需求场景, 如基于hadoop批处理系统、    低延迟实时系统、    Storm&#x2F;Spark流式处理引擎, web&#x2F;nginx日志、    访问日志, 消息服务等. </p>
<table>
    <tr>
        <td> 名称 </td>
        <td> 解释 </td>
    </tr>
    <tr>
        <td> Broker </td>
        <td> 消息中间件处理节点, 一个Kafka节点就是一个Broker, 一个或者多个Broker组成Kafka集群 </td>
    </tr>
    <tr>
        <td> Topic </td>
        <td> Kafka根据Topic对消息进行归类, 发布到Kafka集群的每条消息都需指定一个Topic </td>
    </tr>
    <tr>
        <td> Producer </td>
        <td> 消息生产者, 向Broker发送消息的客户端, 通过TCP协议来完成通信 </td>
    </tr>
    <tr>
        <td> Consumer </td>
        <td> 消息消费者, 从Broker读取消息的客户端, 通过TCP协议来完成通信 </td>
    </tr>
    <tr>
        <td> Consumer Group </td>
        <td> 每个Consumer属于一个特定Consumer Group, 一条消息可被多个不同Consumer Group消费, 但一个Consumer Group中只能有一个Consumer能消费该消息 </td>
    </tr>
    <tr>
        <td> Partition </td>
        <td> 物理上的概念, 一个Topic可分为多个Partition, 每个Partition内部消息是有序的 </td>
    </tr>
</table>


<h2 id="使用场景"><a href="#使用场景" class="headerlink" title="使用场景"></a>使用场景</h2><ul>
<li><strong>日志收集</strong>：可用Kafka收集各种服务日志, 通过kafka以统一接口服务方式开放给各种Consumer, 如Hhadoop、   Hbase、   Solr等. </li>
<li><strong>消息系统</strong>：解耦生产者和消费者、   缓存消息等. </li>
<li><strong>用户活动跟踪</strong>：Kafka经常被用来记录Web用户或App用户的各种活动, 如浏览网页、   搜索、   点击等活动, 被各个服务器发布到kafka的Topic中, 然后订阅者通过订阅这些Topic来做<strong>实时监控分析</strong>, 或装载到Hadoop、   数据仓库中做离线分析和挖掘. </li>
<li><strong>运营指标</strong>：Kafka也经常用来记录<strong>运营监控数据</strong>. 包括收集各种分布式应用数据, 生产各种操作的集中反馈, 如报警和报告.</li>
</ul>
<h2 id="Kafka核心配置"><a href="#Kafka核心配置" class="headerlink" title="Kafka核心配置"></a>Kafka核心配置</h2><p>Kafka核心配置在   <strong><code>config/server.properties</code></strong>   配置文件中. </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">broker.id=0	# broker.id属性在kafka集群中必须要是唯一</span><br><span class="line">listeners=PLAINTEXT://localhost:9092 # kafka部署的机器ip和提供服务的端口号</span><br><span class="line">log.dir=/usr/local/data/kafka-logs # kafka的消息存储文件</span><br><span class="line">zookeeper.connect=192.168.65.60:2181 # kafka连接zookeeper的地址, 若是集群则用逗号分割</span><br></pre></td></tr></table></figure>

<table>
    <tr>
        <td> Property </td>
        <td> Default </td>
        <td> Description </td>
    </tr>
    <tr>
        <td> broker.id </td>
        <td> 0 </td>
        <td> 每个Broker都可用一个唯一非负整数id进行标识 </td>
    </tr>
    <tr>
        <td> log.dirs </td>
        <td> /tmp/kafka-logs </td>
        <td> 存放数据的路径, 该路径并不唯一, 可设置多个路径之间用逗号分隔, 创建新partition时选择包含最少partitions路径下创建 </td>
    </tr>
    <tr>
        <td> listeners </td>
        <td> PLAINTEXT://192.168.65.60:9092 </td>
        <td> Server接受客户端连接的端口, ip配置kafka本机ip即可 </td>
    </tr>
    <tr>
        <td> zookeeper.connect </td>
        <td> localhost:2181 </td>
        <td> Kafka连接Zookeeper的地址, 若是集群用逗号分隔 </td>
    </tr>
    <tr>
        <td> log.retention.hours </td>
        <td> 168 </td>
        <td> 每个日志文件的保存时间. 默认数据保存时间对所有topic都一样 </td>
    </tr>
    <tr>
        <td> num.partitions </td>
        <td> 1 </td>
        <td> 创建Topic默认分区数 </td>
    </tr>
    <tr>
        <td> default.replication.factor </td>
        <td> 1 </td>
        <td> 自动创建Topic默认副本数量, 建议设置为大于等于2 </td>
    </tr>
    <tr>
        <td> min.insync.replicas </td>
        <td> 1 </td>
        <td> 写数据到repica数量达到设定值才表示Producer发送消息成功, 若Producer设置acks为-1, 则每个repica写数据都必须成功 </td>
    </tr>
    <tr>
        <td> delete.topic.enable </td>
        <td> false </td>
        <td> 是否允许删除主题 </td>
    </tr>
</table>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-server-start.sh config/server.properties	# 启动kafka</span><br><span class="line">bin/kafka-server-stop.sh # 停止kafka</span><br><span class="line"># 创建名字为test的Topic, 该topic只有一个partition, 且备份因子为1</span><br><span class="line">bin/kafka-topics.sh --zookeeper localhost:2181 --create --replication-factor 1 --partitions 1 --topic test</span><br><span class="line"># 查看kafka中目前存在的topic</span><br><span class="line">bin/kafka-topics.sh --zookeeper localhost:2181 --list</span><br><span class="line"># 删除topic</span><br><span class="line">bin/kafka-topics.sh --zookeeper localhost:2181 --delete --topic test</span><br><span class="line"># 发送消息到kafka, 若是集群--broker-list参数用逗号隔开</span><br><span class="line">bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test</span><br><span class="line"># 消费kafka集群最新消息, 若是集群--bootstrap-server参数用逗号隔开</span><br><span class="line">bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test</span><br><span class="line"># 多主题消费kafka集群消息</span><br><span class="line">bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --whitelist &quot;test|test-2&quot;</span><br><span class="line"># 通过--from-beginning从开始读取kafka集群消息</span><br><span class="line">bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --from-beginning --topic test</span><br><span class="line"># 单播消费, 一条消息只能被某一个消费者消费, 让所有消费者在同一个消费组里即可</span><br><span class="line">bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --consumer-property group.id=testGroup --topic test</span><br><span class="line"># 多播消费, 同一条消息只能被同一个消费组下的某一个消费者消费的特性</span><br><span class="line">bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --consumer-property group.id=testGroup-2 --topic test</span><br><span class="line"># 查看消费组名</span><br><span class="line">bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --list</span><br><span class="line"># 查看消费组的消费偏移量, current-offset当前消费组的已消费偏移量, log-end-offset主题对应分区消息结束偏移量, lag当前消费组未消费消息数</span><br><span class="line">bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --describe --group testGroup</span><br><span class="line"># 查看下topic情况</span><br><span class="line">bin/kafka-topics.sh --zookeeper localhost:2181 --describe --topic test</span><br><span class="line"># 增加topic的分区数量, 目前kafka不支持减少分区</span><br><span class="line">bin/kafka-topics.sh --zookeeper localhost:2181 --alter --partitions 3 --topic test</span><br></pre></td></tr></table></figure>

<p>同类消息发送到同一个Topic下面, 每个Topic下面可有<strong>多个分区<code>Partition</code>日志文件</strong>,   <strong><code>Partition</code></strong>  是一个<strong>有序的message序列</strong>, 这些message按顺序添加到   <strong><code>commit log</code>文件</strong> 中. 每个Partition中消息都有一个唯一编号   <strong><code>offset</code></strong>  , 用来唯一标识某个分区中的message.  <strong>每个Partition都对应一个<code>commit log</code>文件</strong>, <strong>同一个<code>Partition</code></strong>   中的   <strong><code>message</code></strong>   的<code>offset</code>都是 <strong>唯一</strong> 的, 但 <strong>不同<code>Partition</code></strong>   中   <strong><code>message</code></strong>   的   <strong><code>offset</code></strong>   可能相同.<br><img src="/../../../../assets/png/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/Kafka/Anatomy-of-a-Topic.png"></p>
<p><strong>每个<code>Partition</code>分区</strong>中都有<strong>一个<code>Leader</code>副本节点</strong>和<strong>一个或多个<code>Replicas</code>副本</strong>以及一个  <strong><code>Isr</code></strong>  集合，  <strong><code>Partition</code>的<code>Leader</code>副本节点</strong>负责给定Partition的<strong>所有读写请求</strong>，Replicas表示某个Partition在哪几个Broker上存在<strong>备份</strong>，不管该节点是不是Leader，甚至该<strong>节点挂了也会列出</strong>.   <strong><code>Isr</code></strong>  集合是Replicas的一个子集，只列出<strong>存活</strong>的备份节点，且<strong>已同步备份</strong>了该Partition的节点. </p>
<p>  <strong><code>Kafka</code>一般不会删除消息</strong>，<strong>不管是否被消费</strong>. 只会根据<strong>配置的日志保留时间<code>log.retention.hours</code></strong>  确认消息多久被删除，<strong>默认保留最近一周</strong>的消息. <strong>Kafka性能与保留消息数据量大小没有关系</strong>. </p>
<p>每个Consumer是<strong>基于<code>commit log</code>中消费进度即<code>offset</code></strong>  来进行工作的，<strong>消费<code>offset</code>由Consumer来维护</strong>，一般<strong>按照顺序逐条消费<code>commit log</code>中的消息</strong>，<strong>可通过指定offset来重复消费某些消息</strong>或<strong>跳过某些消息</strong>. 意味Consumer对集群影响非常小，添加或减少Consumer对于集群或其他Consumer没有影响，因为<strong>每个<code>Consumer</code>维护各自的消费<code>offset</code></strong>  . </p>
<p>一个Topic代表<strong>逻辑上</strong>的一个<strong>业务数据集</strong>，对于大型网站来说，后端数据都是海量的，消息可能非常巨量，若把这么多数据都放在一台机器上可能会有<strong>容量限制</strong>问题，可在  <strong><code>Topic</code>内部</strong>划分<strong>多个<code>Partition</code></strong>  来<strong>分片存储数据</strong>，<strong>不同<code>Partition</code>可位于不同机器上</strong>，每台机器上都运行一个Kafka的Broker进程. </p>
<p><strong>分片存储的好处</strong>，<strong>提高并行度</strong>，且  <strong><code>commit log</code></strong>  文件会受到所在机器的文件系统大小的限制，分区后可将不同分区放在不同机器上，相当于对数据做<strong>分布式存储</strong>，理论上一个Topic可处理任意数量数据. </p>
<h2 id="Kafka集群"><a href="#Kafka集群" class="headerlink" title="Kafka集群"></a>Kafka集群</h2><p>Kafka将很多<strong>集群关键信息</strong>记录在 <strong><code>Zookeeper</code></strong> 中，<strong>保证自己的无状态</strong>，从而在<strong>水平扩容时非常方便</strong>.  <strong><code>commit log</code></strong> 的 <strong><code>Partitions</code></strong> 分布在Kafka集群中不同Broker上，每个Broker上<strong>该<code>Partition</code>分区的副本</strong>可请求备份其他Broker上Partition上副本的数据，Kafka集群支持配置一个Partition备份数量. <strong>每个<code>Partition</code>都有一个<code>Broker</code>上的副本起到<code>Leader</code>的作用</strong>， <strong><code>0</code>个</strong>或<strong>多个</strong>其他的Broker<strong>副本</strong>作为 <strong><code>Follwers</code></strong> 作用. 作为 <strong><code>Leader</code>的副本处理所有针对该<code>Partition</code>的读写请求，作为Followers的副本被动复制作为Leader的副本的结果，不提供读写，主要是为了保证多副本数据与消费的一致性</strong>. 若<strong>一个<code>Partition</code>分区</strong>中 <strong><code>Leader</code>副本失效</strong>其中一个 <strong><code>Follower</code>副本</strong>将自动变成新的 <strong><code>Leader</code>副本</strong>. </p>
<p><strong>生产者</strong>将消息发送到Topic中去，同时<strong>负责选择</strong>将message发送到 <strong><code>Topic</code>的哪个<code>Partition</code>中</strong>. 通过 <strong><code>round-robin</code></strong> 做简单的<strong>负载均衡</strong>. 也可<strong>根据消息中某个关键字来进行区分</strong>，通常<strong>第二种方式使用更多</strong>. </p>
<p>对于<strong>消费者</strong>，传统的消<strong>息传递模式</strong>有<strong>队列模式</strong>和<strong>发布订阅</strong>模式，且基于这2种模式提供了一种Consumer的抽象概念 <strong><code>Consumer Group</code></strong> . </p>
<ul>
<li><strong><code>Queue</code>模式</strong>：多个Consumer从服务器中读取数据，<strong>消息只会到达一个<code>Consumer</code></strong> ，<strong>所有<code>Consumer</code>都位于同一<code>Consumer Group</code>下</strong></li>
<li><strong><code>Publish-Subscribe</code>模式</strong>：消息会被<strong>广播给所有Consumer</strong>，<strong>所有Consumer都有唯一的<code>Consumer Group</code></strong> .</li>
</ul>
<p><img src="/../../../../assets/png/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/Kafka/Kafka-Cluster.png" alt="Kafka-Cluster"></p>
<p>通常一个Topic会有几个Consumer Group，每个Consumer Group都是一个逻辑上的订阅者，每个Consumer Group由多个Consumer 实例组成，从而达到可扩展和容灾的功能. </p>
<p><strong>一个Partition同一时刻在一个Consumer Group中只能有一个Consumer在消费</strong>， <strong><code>Partition</code>分区类似于<code>RocketMQ</code>中的队列</strong>，从而保证<strong>消费顺序</strong>. <strong>Consumer Group中的Consumer数不能比一个Topic中Partition的数量多，否则多出来的Consumer消费不到消息</strong>. Kafka只在<strong>Partition范围内</strong>保证消息消费的<strong>局部顺序性</strong>，不能在同一个Topic中多个Partition中保证总的消费顺序性. </p>
<p>若有在总体上保证消费顺序的需求，则可通过<strong>将<code>Topic</code>的<code>Partition</code>数量设置为<code>1</code></strong> ，<strong>将Consumer Group中的<code>Consumer</code>数量也设置为<code>1</code></strong> ，但会影响性能，故Kafka顺序消费很少用. </p>
<h2 id="客户端调用"><a href="#客户端调用" class="headerlink" title="客户端调用"></a>客户端调用</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;2.4.1&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure>
<p>生产者包含一些关键的参数，包括发送消息<strong>持久化机制参数<code>ProducerConfig.ACKS_CONFIG</code></strong> ，发送失败会重试次数 <strong><code>ProducerConfig.RETRIES_CONFIG</code></strong> ，重试时间间隔 <strong><code>ProducerConfig.RETRY_BACKOFF_MS_CONFIG</code></strong> ，以及发送时可指定Partition分区，同步发送异步发送等. </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">Properties props = new Properties();</span><br><span class="line">props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG,  &quot;localhost:9092&quot;);</span><br><span class="line">/*</span><br><span class="line"> 发出消息持久化机制参数</span><br><span class="line">（1）acks=0：表示producer不需要等待任何broker确认收到消息的回复，就可以继续发送下一条消息. 性能最高，但是最容易丢消息. </span><br><span class="line">（2）acks=1：至少等待leader成功将数据写入本地log，但不用等待所有followe都成功写入. 就可继续发送下一条消息. 该情况下若follower没有成功备份数据，而此时leader又挂掉，则消息会丢失. </span><br><span class="line">（3）acks=-1或all：需等待min.insync.replicas，默认为1，推荐配置大于等于2，该参数配置的副本个数都成功写入日志，这种策略会保证</span><br><span class="line">    只要有一个备份存活就不会丢失数据. 这是最强的数据保证. 一般除非是金融级别，或跟钱打交道的场景才会使用这种配置. </span><br><span class="line"> */</span><br><span class="line">props.put(ProducerConfig.ACKS_CONFIG,  &quot;1&quot;);</span><br><span class="line">// 发送失败会重试，默认重试间隔100ms，重试能保证消息发送的可靠性，但也可能造成消息重复发送，如网络抖动，故需在接收者处做好消息接收的幂等性处理</span><br><span class="line">props.put(ProducerConfig.RETRIES_CONFIG,  3);</span><br><span class="line">// 重试时间间隔设置</span><br><span class="line">props.put(ProducerConfig.RETRY_BACKOFF_MS_CONFIG,  300);</span><br><span class="line">// 设置发送消息的本地缓冲区，如果设置了该缓冲区，消息会先发送到本地缓冲区，可以提高消息发送性能，默认值是33554432，即32MB</span><br><span class="line">props.put(ProducerConfig.BUFFER_MEMORY_CONFIG,  33554432);</span><br><span class="line">// kafka本地线程会从缓冲区取数据，批量发送到broker，设置批量发送消息的大小，默认值是16384，即16kb，就是说一个batch满了16kb就发送出去</span><br><span class="line">props.put(ProducerConfig.BATCH_SIZE_CONFIG,  16384);</span><br><span class="line">// 默认值是0，消息必须立即被发送，但这样会影响性能，一般设置10毫秒左右，该消息发送完后会进入本地的一个batch，</span><br><span class="line">// 若10毫秒内该batch满了16kb就随batch一起被发送出去，若10毫秒内batch没满，则也必须把消息发送出去，不能让消息的发送延迟时间太长</span><br><span class="line">props.put(ProducerConfig.LINGER_MS_CONFIG,  10);</span><br><span class="line">// 把发送的key从字符串序列化为字节数组</span><br><span class="line">props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG,  StringSerializer.class.getName());</span><br><span class="line">// 把发送消息value从字符串序列化为字节数组</span><br><span class="line">props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG,  StringSerializer.class.getName());</span><br><span class="line">Producer&lt;String,  String&gt; producer = new KafkaProducer&lt;String,  String&gt;(props);</span><br><span class="line">int msgNum = 5;</span><br><span class="line">final CountDownLatch countDownLatch = new CountDownLatch(msgNum);</span><br><span class="line">for (int i = 1; i &lt;= msgNum; i++) &#123;</span><br><span class="line">    // 指定发送分区</span><br><span class="line">    // ProducerRecord&lt;String,  String&gt; producerRecord = new ProducerRecord&lt;String,  String&gt;(TOPIC_NAME ,  0,  order.getOrderId().toString(),  JSON.toJSONString(order));</span><br><span class="line">    // 未指定发送分区，具体发送的分区计算公式：hash(key)%partitionNum</span><br><span class="line">    ProducerRecord&lt;String,  String&gt; producerRecord = new ProducerRecord&lt;String,  String&gt;(TOPIC_NAME,  String.valueOf(i),  &quot;order &quot; + i);</span><br><span class="line">    // 等待消息发送成功的同步阻塞方法</span><br><span class="line">    // RecordMetadata metadata = producer.send(producerRecord).get();</span><br><span class="line">    // System.out.println(&quot;同步方式发送消息结果：&quot; + &quot;topic-&quot; + metadata.topic() + &quot;|partition-&quot; + metadata.partition() + &quot;|offset-&quot; + metadata.offset());</span><br><span class="line">    //异步回调方式发送消息</span><br><span class="line">    producer.send(producerRecord,  new Callback() &#123;</span><br><span class="line">        @Override</span><br><span class="line">        public void onCompletion(RecordMetadata metadata,  Exception exception) &#123;</span><br><span class="line">            if (exception != null) &#123;</span><br><span class="line">                System.err.println(&quot;发送消息失败：&quot; + exception.getStackTrace());</span><br><span class="line">            &#125;</span><br><span class="line">            if (metadata != null) &#123;</span><br><span class="line">                System.out.println(&quot;异步方式发送消息结果：&quot; + &quot;topic-&quot; + metadata.topic() + &quot;|partition-&quot; + metadata.partition() + &quot;|offset-&quot; + metadata.offset());</span><br><span class="line">            &#125;</span><br><span class="line">            countDownLatch.countDown();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br><span class="line">&#125;</span><br><span class="line">countDownLatch.await(5,  TimeUnit.SECONDS);</span><br><span class="line">producer.close();</span><br></pre></td></tr></table></figure>

<p>消费者同样<strong>可指定消费的分区</strong>，指定<strong>消费者组名称</strong>，<strong>是否自动提交</strong>，<strong>自动提交时间间隔</strong>，<strong>心跳时间间隔等</strong>. </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line">String TOPIC_NAME = &quot;my-replicated-topic&quot;;</span><br><span class="line">String CONSUMER_GROUP_NAME = &quot;testGroup&quot;;</span><br><span class="line">Properties props = new Properties();</span><br><span class="line">props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG,  &quot;localhost:9092&quot;);</span><br><span class="line">// 消费分组名</span><br><span class="line">props.put(ConsumerConfig.GROUP_ID_CONFIG,  CONSUMER_GROUP_NAME);</span><br><span class="line">// 是否自动提交offset，默认就是true</span><br><span class="line">props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG,  &quot;true&quot;);</span><br><span class="line">// 自动提交offset的间隔时间</span><br><span class="line">props.put(ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG,  &quot;1000&quot;);</span><br><span class="line">//props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG,  &quot;false&quot;);</span><br><span class="line">/*</span><br><span class="line">当消费主题的是一个新的消费组，或指定offset的消费方式，offset不存在，则可通过以下两种方式消费消息</span><br><span class="line">- latest(默认) ：只消费自己启动之后发送到主题的消息</span><br><span class="line">- earliest：第一次从头开始消费，以后按照消费offset记录继续消费，这个需要区别于consumer.seekToBeginning(每次都从头开始消费)</span><br><span class="line">*/</span><br><span class="line">//props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG,  &quot;earliest&quot;);</span><br><span class="line">// consumer给broker发送心跳的间隔时间，broker接收到心跳若此时有rebalance发生会通过心跳响应将rebalance方案下发给consumer，该时间可以稍微短一点</span><br><span class="line">props.put(ConsumerConfig.HEARTBEAT_INTERVAL_MS_CONFIG,  1000);</span><br><span class="line">// 服务端broker多久感知不到一个consumer心跳就认为他故障了，会将其踢出消费组，对应的Partition也会被重新分配给其他consumer，默认是10秒</span><br><span class="line">props.put(ConsumerConfig.SESSION_TIMEOUT_MS_CONFIG,  10 * 1000);</span><br><span class="line">// 一次poll最大拉取消息的条数，若消费者处理速度很快，可以设置大点，若处理速度一般，可以设置小点</span><br><span class="line">props.put(ConsumerConfig.MAX_POLL_RECORDS_CONFIG,  500);</span><br><span class="line">// 若两次poll操作间隔超过该时间，则broker认为该consumer处理能力太弱，会将其踢出消费组，将分区分配给别的consumer消费</span><br><span class="line">props.put(ConsumerConfig.MAX_POLL_INTERVAL_MS_CONFIG,  30 * 1000);</span><br><span class="line">props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG,  StringDeserializer.class.getName());</span><br><span class="line">props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG,  StringDeserializer.class.getName());</span><br><span class="line">KafkaConsumer&lt;String,  String&gt; consumer = new KafkaConsumer&lt;String,  String&gt;(props);</span><br><span class="line">consumer.subscribe(Arrays.asList(TOPIC_NAME));</span><br><span class="line">// 消费指定分区</span><br><span class="line">//consumer.assign(Arrays.asList(new TopicPartition(TOPIC_NAME,  0)));</span><br><span class="line">// 消息回溯消费</span><br><span class="line">//consumer.seekToBeginning(Arrays.asList(new TopicPartition(TOPIC_NAME,  0)));</span><br><span class="line">// 指定offset消费</span><br><span class="line">//consumer.seek(new TopicPartition(TOPIC_NAME,  0),  10);</span><br><span class="line">// 从指定时间点开始消费</span><br><span class="line">/*</span><br><span class="line">List&lt;PartitionInfo&gt; topicPartitions = consumer.partitionsFor(TOPIC_NAME);</span><br><span class="line">//从1小时前开始消费</span><br><span class="line">long fetchDataTime = new Date().getTime() - 1000 * 60 * 60;</span><br><span class="line">Map&lt;TopicPartition,  Long&gt; map = new HashMap&lt;&gt;();</span><br><span class="line">for (PartitionInfo par : topicPartitions) &#123;</span><br><span class="line">    map.put(new TopicPartition(topicName,  par.partition()),  fetchDataTime);</span><br><span class="line">&#125;</span><br><span class="line">Map&lt;TopicPartition,  OffsetAndTimestamp&gt; parMap = consumer.offsetsForTimes(map);</span><br><span class="line">for (Map.Entry&lt;TopicPartition,  OffsetAndTimestamp&gt; entry : parMap.entrySet()) &#123;</span><br><span class="line">    TopicPartition key = entry.getKey();</span><br><span class="line">    OffsetAndTimestamp value = entry.getValue();</span><br><span class="line">    if (key == null || value == null) continue;</span><br><span class="line">    Long offset = value.offset();</span><br><span class="line">    System.out.println(&quot;partition-&quot; + key.partition() + &quot;|offset-&quot; + offset);</span><br><span class="line">    System.out.println();</span><br><span class="line">    //根据消费里的timestamp确定offset</span><br><span class="line">    if (value != null) &#123;</span><br><span class="line">        consumer.assign(Arrays.asList(key));</span><br><span class="line">        consumer.seek(key,  offset);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;*/</span><br><span class="line">while (true) &#123;</span><br><span class="line">    // poll() API 是拉取消息的长轮询</span><br><span class="line">    ConsumerRecords&lt;String,  String&gt; records = consumer.poll(Duration.ofMillis(1000));</span><br><span class="line">    for (ConsumerRecord&lt;String,  String&gt; record : records) &#123;</span><br><span class="line">        System.out.printf(&quot;收到消息：partition = %d, offset = %d,  key = %s,  value = %s%n&quot;,  record.partition(),  record.offset(),  record.key(),  record.value());</span><br><span class="line">    &#125;</span><br><span class="line">    /*if (records.count() &gt; 0) &#123;</span><br><span class="line">        // 手动同步提交offset，当前线程会阻塞直到offset提交成功一般使用同步提交，因为提交之后一般也没有什么逻辑代码了</span><br><span class="line">        consumer.commitSync();</span><br><span class="line">        // 手动异步提交offset，当前线程提交offset不会阻塞，可以继续处理后面的程序逻辑</span><br><span class="line">        consumer.commitAsync(new OffsetCommitCallback() &#123;</span><br><span class="line">            @Override</span><br><span class="line">            public void onComplete(Map&lt;TopicPartition,  OffsetAndMetadata&gt; offsets,  Exception exception) &#123;</span><br><span class="line">                if (exception != null) &#123;</span><br><span class="line">                    System.err.println(&quot;Commit failed for &quot; + offsets);</span><br><span class="line">                    System.err.println(&quot;Commit failed exception: &quot; + exception.getStackTrace());</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">    &#125;*/</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="Spring整合"><a href="#Spring整合" class="headerlink" title="Spring整合"></a>Spring整合</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.springframework.kafka&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;spring-kafka&lt;/artifactId&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">spring:</span><br><span class="line">  kafka:</span><br><span class="line">    bootstrap-servers: localhost:9092, localhost:9093, localhost:9094</span><br><span class="line">    producer: # 生产者</span><br><span class="line">      retries: 3 # 设置大于0的值，则客户端会将发送失败的记录重新发送</span><br><span class="line">      batch-size: 16384</span><br><span class="line">      buffer-memory: 33554432</span><br><span class="line">      acks: 1</span><br><span class="line">      # 指定消息key和消息体的编解码方式</span><br><span class="line">      key-serializer: org.apache.kafka.common.serialization.StringSerializer</span><br><span class="line">      value-serializer: org.apache.kafka.common.serialization.StringSerializer</span><br><span class="line">    consumer:</span><br><span class="line">      group-id: default-group</span><br><span class="line">      enable-auto-commit: false</span><br><span class="line">      auto-offset-reset: earliest</span><br><span class="line">      key-deserializer: org.apache.kafka.common.serialization.StringDeserializer</span><br><span class="line">      value-deserializer: org.apache.kafka.common.serialization.StringDeserializer</span><br><span class="line">    listener:</span><br><span class="line">      # 当每一条记录被消费者监听器（ListenerConsumer）处理之后提交</span><br><span class="line">      # RECORD</span><br><span class="line">      # 当每一批poll()的数据被消费者监听器（ListenerConsumer）处理之后提交</span><br><span class="line">      # BATCH</span><br><span class="line">      # 当每一批poll()的数据被消费者监听器（ListenerConsumer）处理之后，距离上次提交时间大于TIME时提交</span><br><span class="line">      # TIME</span><br><span class="line">      # 当每一批poll()的数据被消费者监听器（ListenerConsumer）处理之后，被处理record数量大于等于COUNT时提交</span><br><span class="line">      # COUNT</span><br><span class="line">      # TIME |　COUNT　有一个条件满足时提交</span><br><span class="line">      # COUNT_TIME</span><br><span class="line">      # 当每一批poll()的数据被消费者监听器（ListenerConsumer）处理之后,  手动调用Acknowledgment.acknowledge()后提交</span><br><span class="line">      # MANUAL</span><br><span class="line">      # 手动调用Acknowledgment.acknowledge()后立即提交，一般使用这种</span><br><span class="line">      # MANUAL_IMMEDIATE</span><br><span class="line">      ack_mode: manual_immediate</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">private final static String TOPIC_NAME = &quot;my-replicated-topic&quot;;</span><br><span class="line">@Autowired</span><br><span class="line">private KafkaTemplate&lt;String,  String&gt; kafkaTemplate;</span><br><span class="line">public void send() &#123;</span><br><span class="line">    kafkaTemplate.send(TOPIC_NAME,  0,  &quot;key&quot;,  &quot;this is a msg&quot;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line"> * @KafkaListener(groupId = &quot;testGroup&quot;,  topicPartitions = &#123;</span><br><span class="line"> * @TopicPartition(topic = &quot;topic1&quot;,  partitions = &#123;&quot;0&quot;,  &quot;1&quot;&#125;), </span><br><span class="line"> * @TopicPartition(topic = &quot;topic2&quot;,  partitions = &quot;0&quot;, </span><br><span class="line"> * partitionOffsets = @PartitionOffset(partition = &quot;1&quot;,  initialOffset = &quot;100&quot;))</span><br><span class="line"> * &#125;,  concurrency = &quot;6&quot;)</span><br><span class="line"> * concurrency就是同组下的消费者个数，就是并发消费数，必须小于等于分区总数</span><br><span class="line"> */</span><br><span class="line">@KafkaListener(topics = &quot;my-replicated-topic&quot;,  groupId = &quot;testGroup&quot;)</span><br><span class="line">public void listenTestGroup(ConsumerRecord&lt;String,  String&gt; record,  Acknowledgment ack) &#123;</span><br><span class="line">    String value = record.value();</span><br><span class="line">    System.out.println(value);</span><br><span class="line">    System.out.println(record);</span><br><span class="line">    //手动提交offset</span><br><span class="line">    //ack.acknowledge();</span><br><span class="line">&#125;</span><br><span class="line">// 配置多个消费组</span><br><span class="line">@KafkaListener(topics = &quot;my-replicated-topic&quot;,  groupId = &quot;elevenGroup&quot;)</span><br><span class="line">public void listenElevenGroup(ConsumerRecord&lt;String,  String&gt; record,  Acknowledgment ack) &#123;</span><br><span class="line">    String value = record.value();</span><br><span class="line">    System.out.println(value);</span><br><span class="line">    System.out.println(record);</span><br><span class="line">    ack.acknowledge();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="设计原理"><a href="#设计原理" class="headerlink" title="设计原理"></a>设计原理</h2><p><img src="/../../../../assets/png/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/Kafka/%E8%AE%BE%E8%AE%A1%E5%8E%9F%E7%90%86.png" alt="设计原理"></p>
<p><img src="/../../../../assets/png/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/Kafka/Kafka-zookeeper%E8%8A%82%E7%82%B9.png" alt="Kafka-zookeeper节点"></p>
<h2 id="总控制器Controller"><a href="#总控制器Controller" class="headerlink" title="总控制器Controller"></a>总控制器Controller</h2><p>Kafka集群中会有<strong>一个或多个<code>Broker</code></strong> ，其中有一个Broker会被<strong>选举</strong>为Kafka   <strong><code>Controller</code>控制器</strong>，其<strong>负责管理整个集群中所有分区和副本的状态</strong>. </p>
<ul>
<li>当某个分区的 <strong><code>Leader</code>副本出现故障时</strong>，由<strong>控制器</strong>负责<strong>为该分区选举新的<code>Leader</code>副本</strong>. </li>
<li>当检测到某个分区的 <strong><code>ISR</code>集合</strong>发生变化时，由<strong>控制器</strong>负责<strong>通知所有Broker更新其元数据信息</strong>. </li>
<li>当使用 <strong><code>kafka-topics.sh</code></strong> 脚本为某个 <strong><code>Topic</code>增加分区数量时</strong>，同样还是由<strong>控制器</strong>负责<strong>让新分区被其他节点感知到</strong>.</li>
</ul>
<p>Kafka集群启动时<strong>自动选举一台<code>Broker</code>作为<code>Controller</code>来管理整个集群</strong>，选举过程是集群中每个Broker都会尝试在Zookeeper上创建一个  <strong><code>/controller</code>临时节点</strong>，Zookeeper会保证有且仅有一个Broker能创建成功，创建成功的Broker则成为集群的总控器Controller. </p>
<h2 id="Controller选举机制"><a href="#Controller选举机制" class="headerlink" title="Controller选举机制"></a>Controller选举机制</h2><p>当<strong>Controller角色的Broker宕机</strong>时Zookeeper<strong>临时节点会消失</strong>，集群里其他Broker会一直<strong>监听<code>/controller</code>临时节点</strong>，发现临时节点消失则<strong>竞争再次创建<code>/controller</code>临时节点</strong>，这就是 <strong><code>Controller</code>的选举机制</strong>. 具备控制器身份的Broker需要比其他普通Broker多一份职责：</p>
<ol>
<li><strong>监听Broker相关的变化</strong>，为Zookeeper中的 <strong><code>/brokers/ids</code></strong> 节点添加 <strong><code>BrokerChangeListener</code></strong> ，用来<strong>处理<code>Broker</code>增减变化</strong>. </li>
<li><strong>监听Topic相关的变化</strong>，为Zookeeper中的 <strong><code>/brokers/topics</code></strong> 节点添加 <strong><code>TopicChangeListener</code></strong> ，用来<strong>处理<code>Topic</code>增减的变化</strong>；为Zookeeper中的 <strong><code>/admin/delete_topics</code></strong> 节点添加 <strong><code>TopicDeletionListener</code></strong> ，用来<strong>处理删除<code>Topic</code>动作</strong>. </li>
<li><strong>从Zookeeper中读取当前所有与<code>Topic</code>、 <code>Partition</code>以及<code>Broker</code>有关信息并进行相应的管理</strong>. 对所有Topic所对应的Zookeeper中的 <strong><code>/brokers/topics/[topic]</code></strong> 节点添加 <strong><code>PartitionModificationsListener</code></strong> ，用来<strong>监听Topic中分区分配变化</strong>. </li>
<li><strong>更新集群元数据信息</strong>，同步到其他普通Broker节点中.</li>
</ol>
<h2 id="Partition副本选举Leader机制"><a href="#Partition副本选举Leader机制" class="headerlink" title="Partition副本选举Leader机制"></a>Partition副本选举Leader机制</h2><p>Controller会监听 <strong><code>/brokers/ids</code></strong> 节点可感知Broker是否存活，当Controller感知到分区Leader所在Broker挂了，参数 <strong><code>unclean.leader.election.enable=false</code></strong> 的前提下，Controller会从 <strong><code>ISR</code>列表</strong>里挑第一个Broker作为Leader，因为第一个Broker最先放进ISR列表可能是同步数据最多的副本，若参数 <strong><code>unclean.leader.election.enable=true</code></strong> 代表在 <strong><code>ISR</code>列表</strong>里所有副本都挂了时<strong>可在<code>ISR</code>列表以外的副本中选Leader</strong>，该设置可<strong>提高可用性</strong>，但选出的新Leader可能数据少很多. </p>
<p>副本进入ISR列表有两个条件：<strong>副本节点不能产生分区</strong>，必须能与Zookeeper保持会话以及跟Leader副本网络连通；<strong>副本能复制Leader上的所有写操作</strong>，且不能落后太多，超过 <strong><code>replica.lag.time.max.ms</code></strong> 时间都没跟Leader同步过一次的副本会被移出 <strong><code>ISR</code></strong> 列表；</p>
<h2 id="offset记录机制"><a href="#offset记录机制" class="headerlink" title="offset记录机制"></a>offset记录机制</h2><p>每个Consumer会<strong>定期</strong>将自己<strong>消费分区的<code>offset</code></strong> 提交给Kafka内部名称为 <strong><code>__consumer_offsets</code></strong> 的Topic，提交过去时<strong>key为<code>consumerGroupId+topic+</code>分区号，value就是当前offset的值</strong>，Kafka会定期清理该Topic里的消息<strong>保留最新的那条数据</strong>，因为 <strong><code>__consumer_offsets</code>可能会接收高并发的请求</strong>，Kafka默认给其<strong>分配<code>50</code>个分区</strong>，可通过 <strong><code>offsets.topic.num.partitions</code></strong> 设置，这样可通过加机器的方式抗大并发. </p>
<p>通过公式 <strong><code>hash(consumerGroupId) % __consumer_offsets</code>主题的分区数</strong>可选出Consumer<strong>消费的<code>offset</code></strong> 要提交到 <strong><code>__consumer_offsets</code>的哪个分区</strong>. </p>
<h2 id="消费者Rebalance机制"><a href="#消费者Rebalance机制" class="headerlink" title="消费者Rebalance机制"></a>消费者Rebalance机制</h2><p> <strong><code>Rebalance</code>机制</strong>：若<strong>消费组</strong>中<strong>消费者数量变化</strong>或<strong>消费分区数变化</strong>，Kafka会<strong>重新分配消费者消费分区的关系</strong>. 如Consumer Group中某个消费者挂了，此时会自动把分配给它的分区交给其它消费者，若其恢复则又会把一些分区重新交还给它. </p>
<p> <strong><code>Rebalance</code>只针对<code>Subscribe</code>这种不指定分区消费的情况</strong>，若通过 <strong><code>assign</code></strong> 消费方式<strong>指定了分区</strong>Kafka<strong>不会进行<code>Rebanlance</code></strong> . 当<strong>消费组中<code>Consumer</code>增加或减少</strong>、 <strong>动态给<code>Topic</code>增加分区</strong>、 <strong>消费组订阅了更多的<code>Topic</code></strong> 等情况<strong>可能触发消费者Rebalance</strong>. </p>
<p> <strong><code>Rebalance</code>过程中消费者无法从<code>Kafka</code>消费消息</strong>，对Kafka的<code>TPS</code>会有影响，<strong>若Kafka集群内节点较多</strong>， <strong><code>Rebalance</code>重平衡可能会耗时极多</strong>，应<strong>尽量避免在系统高峰期<code>Rebalance</code>重平衡</strong>. </p>
<h2 id="消费者Rebalance分区分配策略"><a href="#消费者Rebalance分区分配策略" class="headerlink" title="消费者Rebalance分区分配策略"></a>消费者Rebalance分区分配策略</h2><p><strong>消费者<code>Rebalance</code>分区分配策略</strong>主要有 <strong><code>range</code></strong> 、  <strong><code>round-robin</code></strong> 、  <strong><code>sticky</code></strong> 三种 <strong><code>Rebalance</code>策略</strong>，<strong>默认</strong>为 <strong><code>range</code>分配策略</strong>. Kafka提供消费者客户端参数 <strong><code>partition.assignment.strategy</code></strong> 来设置<strong>消费者</strong>与<strong>订阅主题</strong>之间的<strong>分区分配策略</strong>. </p>
<ul>
<li><strong><code>range</code>策略</strong>：按<strong>分区序号排序</strong>，假设 <strong><code>n＝</code>分区数<code>／</code>消费者数量</strong>， <strong><code>m＝</code>分区数<code>%</code>消费者数量</strong>，则<strong>前<code>m</code>个消费者每个分配<code>n+1</code>个分区</strong>，<strong>消费者数量  <code>- m</code>个消费者每个分配<code>n</code>个分区</strong>. </li>
<li><strong><code>round-robin</code>策略</strong>：<strong>轮询分配</strong></li>
<li><strong><code>sticky</code>策略</strong>：初始时分配策略与round-robin类似，但在Rebalance时需要保证<strong>分区分配尽可能均匀</strong>、 <strong>分区分配尽可能与上次分配保持相同</strong>两个原则. 当两者发生冲突时，第一个目标优先于第二个目标 . 这样可以最大程度维持原来的分区分配的策略.</li>
</ul>
<h2 id="Rebalance过程"><a href="#Rebalance过程" class="headerlink" title="Rebalance过程"></a>Rebalance过程</h2><p><img src="/../../../../assets/png/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/Kafka/Rebalance%E8%BF%87%E7%A8%8B.png" alt="Rebalance过程"><br>当<strong>有消费者加入消费组时</strong>，<strong>消费者</strong>、 <strong>消费组</strong>及<strong>组协调器</strong>之间会依次经历<strong>选择组协调器</strong>、 <strong>加入消费组</strong>、  <strong><code>SYNC GROUP</code></strong> 三个阶段；</p>
<ul>
<li><strong>选择组协调器</strong>：每个Consumer Group都会选择一个Broker作为自己的<strong>组协调器<code>GroupCoordinator</code></strong> ，负责<strong>监控</strong>该消费组中所有<strong>消费者心跳</strong>，以及判断是否宕机，然后开启消费者Rebalance. Consumer Group中每个Consumer启动时会向Kafka集群中某个节点发送   <strong><code>FindCoordinatorRequest</code></strong> 请求来查找对应的<strong>组协调器<code>GroupCoordinator</code></strong> ，并跟其<strong>建立网络连接</strong>. Consumer消费的offset要提交到 <strong><code>__consumer_offsets</code></strong> 的哪个分区，该分区Leader对应的Broker就是该Consumer Group的GroupCoordinator. </li>
<li><strong>加入消费组</strong>：消费者会向 <strong><code>GroupCoordinator</code></strong> 发送 <strong><code>JoinGroupRequest</code></strong> 请求<strong>并处理响应</strong>. 然后GroupCoordinator从一个Consumer Group中选择第一个加入Group的Consumer作为<strong>Leader消费组协调器</strong>，把Consumer Group情况发送给该Leader，接着该 <strong><code>Consumer Leader</code></strong> 会负责<strong>制定分区方案</strong>. </li>
<li><strong><code>SYNC GROUP</code></strong> ： <strong><code>Consumer Leader</code></strong> 通过<strong>给<code>GroupCoordinator</code>发送<code>SyncGroupRequest</code></strong> ，接着 <strong><code>GroupCoordinator</code>把分区方案下发给各个<code>Consumer</code></strong> ，具体的 <strong><code>Consumer</code></strong> 根据<strong>指定分区的Leader Broker进行网络连接以及消息消费</strong>.</li>
</ul>
<h2 id="Producer发布消息机制"><a href="#Producer发布消息机制" class="headerlink" title="Producer发布消息机制"></a>Producer发布消息机制</h2><p>Producer采用 <strong><code>push</code>模式</strong>将消息发布到<strong>Broker</strong>，每条消息都被append到<strong>顺序写磁盘</strong>到 <strong><code>Patition</code></strong> 中，Producer发送消息到Broker时，会根据<strong>分区算法</strong>选择将其存储到哪一个Partition，路由机制为：</p>
<ul>
<li><strong>指定了Patition，则直接使用</strong>；</li>
<li><strong>未指定<code>Patition</code>但指定了<code>key</code></strong> ，通过对key的value进行hash选出一个Patition</li>
<li><strong><code>Patition</code>和key都未指定</strong>，使用<strong>轮询</strong>选出一个Patition. </li>
<li><img src="/../../../../assets/png/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/Kafka/Producer%E5%8F%91%E5%B8%83%E6%B6%88%E6%81%AF%E6%9C%BA%E5%88%B6.png" alt="Producer发布消息机制"><br>Producer先从Zookeeper的 <strong><code>/brokers/.../state</code></strong> 节点找到该Partition的Leader，然后将消息发送给该Leader，Leader将消息写入本地commit log，Followers从Leader Pull消息，写入本地commit log后向Leader发送ACK，Leader收到<strong>所有ISR中</strong>的Replica的ACK后，增加最后 commit 的offset即 <strong><code>high watermark</code>高水位</strong>简称 <strong><code>HW</code></strong> 并向Producer发送ACK.</li>
</ul>
<h2 id="HW和LEO"><a href="#HW和LEO" class="headerlink" title="HW和LEO"></a>HW和LEO</h2><p> <strong><code>High Watermark</code></strong> 俗称<strong>高水位</strong>，取一个 <strong><code>Partition</code></strong> 对应的 <strong><code>ISR</code>中最小的<code>log-end-offset</code>即<code>LEO</code>作为<code>HW</code></strong> ，<strong>Consumer最多只能消费到<code>HW</code>所在的位置</strong>. 每个 <strong><code>Replica</code></strong> 都有 <strong><code>HW</code></strong> ， <strong><code>Leader</code></strong> 和 <strong><code>Follower</code></strong> 各自负责更新自己的HW状态. 对于 <strong><code>Leader</code>新写入的消息<code>Consumer</code>不能立刻消费</strong>，Leader会等待该消息<strong>被所有<code>ISR</code>中的<code>Replicas</code>同步后更新<code>HW</code></strong> ，此时消息才能被Consumer消费. 这样保证了若Leader所在Broker失效，该消息仍然可从新选举的Leader中获取. 对于<strong>来自内部Broker的读取请求没有<code>HW</code>的限制</strong>. ISR以及HW和LEO的流转过程：<br><img src="/../../../../assets/png/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/Kafka/%E6%B5%81%E8%BD%AC%E8%BF%87%E7%A8%8B.png" alt="流转过程"></p>
<p>Kafka复制机制既<strong>不是完全的同步复制</strong>，<strong>也不是单纯的异步复制</strong>. 同步复制要求所有能工作的Follower都复制完，这条消息才会被commit极大影响了吞吐率. 异步复制方式下Follower异步从Leader复制数据，数据只要被Leader写入log就被认为已经commit，这种情况下若Follower还未复制完落后于Leader时，若Leader宕机则会丢失数据. Kafka使用 <strong><code>ISR</code></strong> 方式则很好<strong>均衡了确保数据不丢失以及吞吐率</strong>.  <strong><code>acks=1</code></strong> 的情况:<br><img src="/../../../../assets/png/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/Kafka/acks=1.png" alt="acks=1"></p>
<h2 id="日志分段存储"><a href="#日志分段存储" class="headerlink" title="日志分段存储"></a>日志分段存储</h2><p><strong>Kafka一个分区的消息数据对应存储在一个文件夹下</strong>，<strong>以Topic名称+分区号命名</strong>，消息在<strong>分区内是分段存储</strong>，每个段的消息都存储在不一样的log文件里，这种特性方便<strong>过期分段文件</strong>快速被删除，Kafka规定<strong>一个段位</strong>的 <strong><code>log</code>文件最大</strong>为 <strong><code>1G</code></strong> . </p>
<p>Kafka每次往分区发 <strong><code>4K</code></strong> 消息就会记录一条当前消息的 <strong><code>offset</code></strong> 到 <strong><code>index</code>文件</strong>以及记录一条当前消息的<strong>发送时间戳</strong>与对应的 <strong><code>offset</code></strong> 到 <strong><code>timeindex</code>文件</strong>，若要定位消息的 <strong><code>offset</code></strong> 会<strong>先在<code>index</code>文件里快速定位</strong>，若需要<strong>按照时间来定位消息的<code>offset</code></strong> ，会<strong>先在<code>timeindex</code>文件里查找</strong>，再去log文件里找具体消息，相当于一个<strong>稀疏索引</strong>；</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># 部分消息的offset索引文件，Kafka每次往分区发4K(可配置)消息就会记录一条当前消息的offset到index文件，若要定位消息的offset会先在该文件里快速定位，再去log文件里找具体消息，相当于一个稀疏索引</span><br><span class="line">00000000000000000000.index</span><br><span class="line"># 消息存储文件，主要存offset和消息体</span><br><span class="line">00000000000000000000.log</span><br><span class="line"># 消息的发送时间索引文件，kafka每次往分区发4K(可配置)消息就会记录一条当前消息的发送时间戳与对应的offset到timeindex文件，若需要按照时间来定位消息的offset，会先在这个文件里查找</span><br><span class="line">00000000000000000000.timeindex</span><br></pre></td></tr></table></figure>

<p>一个日志段文件满了，会自动开一个新的日志段文件来写入，避免单个文件过大，影响文件的读写性能，该过程叫做 <strong><code>log rolling</code></strong> ，正在被写入的日志段文件叫做 <strong><code>active log segment</code></strong> </p>
<h2 id="实际问题"><a href="#实际问题" class="headerlink" title="实际问题:"></a>实际问题:</h2><h3 id="消息丢失"><a href="#消息丢失" class="headerlink" title="消息丢失"></a>消息丢失</h3><h4 id="消息发送端"><a href="#消息发送端" class="headerlink" title="消息发送端"></a>消息发送端</h4><ul>
<li><strong><code>acks=0</code></strong> ： 表示<strong>Producer不需要等待任何Broker确认收到消息的回复，就可继续发送下一条消息</strong>. <strong>性能最高</strong>，但是最容易丢消息. 大数据统计报表场景，对性能要求很高，对数据丢失不敏感的情况可用这种. </li>
<li><strong><code>acks=1</code></strong> ：  <strong>至少要等待Leader已经成功将数据写入本地log，但不需要等待所有Follower是否成功写入</strong>. 就可继续发送下一条消息. 该情况下若Follower没有成功备份数据，而此时Leader挂掉则消息会丢失. </li>
<li><strong><code>acks=-1</code>或<code>all</code></strong> ： Leader需等待所有备份即 <strong><code>min.insync.replicas</code></strong> 配置的备份个数都成功写入日志，该策略会保证只要有一个备份存活就不会丢失数据</li>
</ul>
<h4 id="消息消费端"><a href="#消息消费端" class="headerlink" title="消息消费端"></a>消息消费端</h4><p>若消费这边配置的是<strong>自动提交</strong>，万一消费到数据还没处理完，就自动提交offset了，但此时Consumer直接宕机了，未处理完的数据丢失了，下次也消费不到了. </p>
<h4 id="消息重复消费"><a href="#消息重复消费" class="headerlink" title="消息重复消费"></a>消息重复消费</h4><p><strong>消息发送端</strong>：发送消息若<strong>配置了重试机制</strong>，如网络抖动时间过长导致发送端发送超时，实际broker可能已经接收到消息，但发送方会重新发送消息</p>
<p><strong>消息消费端</strong>：若消费这边配置的是<strong>自动提交</strong>，刚拉取了一批数据处理了一部分，但还没来得及提交，服务挂了，下次重启又会拉取相同的一批数据重复处理，一般消费端都是要做<strong>消费幂等</strong>处理. </p>
<h4 id="消息乱序"><a href="#消息乱序" class="headerlink" title="消息乱序"></a>消息乱序</h4><p>若<strong>发送端</strong>配置了<strong>重试机制</strong>，Kafka不会等之前那条消息完全发送成功才去发送下一条消息，可能会出现发送了1，2，3条消息，第一条超时了，后面两条发送成功，再重试发送第1条消息，这时消息在broker端的顺序就是2，3，1了，是否一定要配置重试要根据业务情况而定. <strong>也可用同步发送的模式</strong>去发消息，当然acks不能设置为0，这样也能保证消息发送的有序. </p>
<p>kafka保证全链路消息顺序消费，需要从发送端开始，将所有有序消息发送到同一个分区，然后<strong>用一个消费者去消费</strong>，但性能比较低，可在消费者端接收到消息后将需要保证顺序消费的几条消费发到内存队列，一个内存队列开启一个线程顺序处理消息. </p>
<h4 id="消息积压"><a href="#消息积压" class="headerlink" title="消息积压"></a>消息积压</h4><p>线上有时因为发送方发送消息速度过快，或者消费方处理消息过慢，可能会导致Broker积压大量未消费消息. 若积压了上百万未消费消息需要紧急处理，可修改消费端程序，让其<strong>将收到的消息快速转发到其他<code>Topic</code></strong> ，可设置很多分区，然后再启动多个消费者同时消费新主题的不同分区. </p>
<p>由于消息数据格式变动或消费者程序有bug，导致消费者一直消费不成功，也可能导致broker积压大量未消费消息. 此种情况可将这些消费不成功的消息转发到其它队列里去，类似<strong>死信队列</strong>，后面再慢慢分析死信队列里的消息处理问题. </p>
<h4 id="延时队列"><a href="#延时队列" class="headerlink" title="延时队列"></a>延时队列</h4><p>延时队列存储的对象是延时消息. 指消息被发送以后，并不想让消费者立刻获取，而是等待特定的时间后，消费者才能获取该消息进行消费，但Kafka不支持延时队列. </p>
<p><strong>实现思路</strong>：发送延时消息时先把消息按照不同的延迟时间段发送到指定的队列中如topic_1s，topic_5s，topic_10s，…topic_2h，一般不能支持任意时间段的延时，然后通过定时器进行轮训消费这些Topic，查看消息是否到期，若到期则把该消息发送到具体业务处理的Topic中，队列中消息越靠前的到期时间越早，具体来说就是定时器在一次消费过程中，<strong>对消息的发送时间做判断</strong>，看下是否延迟到对应时间了，若到了就转发，如果还没到这一次定时任务就可以提前结束了. </p>
<h4 id="消息回溯"><a href="#消息回溯" class="headerlink" title="消息回溯"></a>消息回溯</h4><p>若某段时间对已消费消息计算的结果觉得有问题，可能是由于程序bug导致的计算错误，当程序bug修复后，这时可能需要对之前已消费的消息重新消费，可以指定从多久之前的消息回溯消费，这种可以用Consumer的offsetsForTimes、 seek等方法指定从某个offset偏移的消息开始消费. </p>
<h4 id="消息传递保障"><a href="#消息传递保障" class="headerlink" title="消息传递保障"></a>消息传递保障</h4><p><strong>kafka生产者的幂等性</strong>，因发送端重试导致的消息重复发送问题，Kafka幂等性可保证重复发送的消息只接收一次，只需在生产者加上参数 <strong><code>props.put(&quot;enable.idempotence&quot;,  true)</code></strong> 即可，<strong>默认是<code>false</code>不开启</strong>，Kafka每次发送消息会生成 <strong><code>PID</code></strong> 和 <strong><code>Sequence Number</code></strong> 并将这两个属性一起发送给Broker，Broker将PID和Sequence Number跟消息绑定一起存起来，若生产者重发相同消息，Broker会检查PID和Sequence Number，若相同不会再接收. </p>
<p>每个新的Producer在初始化时会被分配一个唯一的PID，PID对用户完全是透明的，生产者若重启则会生成新的PID，每个PID该Producer发送到每个Partition的数据都有对应的序列号即 <strong><code>Sequence Number</code></strong> ，这些序列号是从0开始单调递增的. </p>
<h4 id="Kafka的事务"><a href="#Kafka的事务" class="headerlink" title="Kafka的事务"></a>Kafka的事务</h4><p>Kafka事务不同于Rocketmq，Rocketmq是保障本地事务与MQ消息发送的事务一致性，Kafka的事务主要是保障<strong>一次发送多条消息的事务一致性</strong>，一般在Kafka流式计算场景用得多一点，如kafka需要对一个Topic中的消息做不同的流式计算处理，处理完分别发到不同的Topic里，这些Topic分别被不同的下游系统消费如hbase，redis，es等，这种肯定希望系统发送到多个topic的数据保持事务一致性. </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">Properties props = new Properties();</span><br><span class="line">props.put(&quot;bootstrap.servers&quot;,  &quot;localhost:9092&quot;);</span><br><span class="line">props.put(&quot;transactional.id&quot;,  &quot;my-transactional-id&quot;);</span><br><span class="line">Producer&lt;String,  String&gt; producer = new KafkaProducer&lt;&gt;(props,  new StringSerializer(),  new StringSerializer());</span><br><span class="line">producer.initTransactions(); //初始化事务</span><br><span class="line">try &#123;</span><br><span class="line"></span><br><span class="line">    producer.beginTransaction(); //开启事务</span><br><span class="line">    for (int i = 0; i &lt; 100; i++) &#123;//发到不同的主题的不同分区</span><br><span class="line">        producer.send(new ProducerRecord&lt;&gt;(&quot;hdfs-topic&quot;,  Integer.toString(i),  Integer.toString(i)));</span><br><span class="line">        producer.send(new ProducerRecord&lt;&gt;(&quot;es-topic&quot;,  Integer.toString(i),  Integer.toString(i)));</span><br><span class="line">        producer.send(new ProducerRecord&lt;&gt;(&quot;redis-topic&quot;,  Integer.toString(i),  Integer.toString(i)));</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    producer.commitTransaction(); //提交事务</span><br><span class="line">&#125; catch (ProducerFencedException | OutOfOrderSequenceException | AuthorizationException e) &#123;</span><br><span class="line">    producer.close();</span><br><span class="line">&#125; catch (KafkaException e) &#123;</span><br><span class="line">    </span><br><span class="line">    producer.abortTransaction(); //回滚事务</span><br><span class="line">&#125;</span><br><span class="line">producer.close();</span><br></pre></td></tr></table></figure>

            </div>
        
        <footer class="article-footer">
        </footer>
    </div>
</article>


    
<nav id="article-nav">
    
        <a href="/blog/%E5%B7%A5%E5%85%B7%E5%92%8C%E4%B8%AD%E9%97%B4%E4%BB%B6/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/Activemq/ActiveMQ-%E5%9F%BA%E7%A1%80/" id="article-nav-newer" class="article-nav-link-wrap">
            <strong class="article-nav-caption">Newer</strong>
            <div class="article-nav-title">
                
                    ActiveMQ-基础
                
            </div>
        </a>
    
    
        <a href="/blog/%E5%B7%A5%E5%85%B7%E5%92%8C%E4%B8%AD%E9%97%B4%E4%BB%B6/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/RabbitMQ/RabbitMQ-%E5%9F%BA%E7%A1%80/" id="article-nav-older" class="article-nav-link-wrap">
            <strong class="article-nav-caption">Older</strong>
            <div class="article-nav-title">RabbitMQ-基础</div>
        </a>
    
</nav>





    
    




<!-- baidu url auto push script -->
<script type="text/javascript">
    !function(){var e=/([http|https]:\/\/[a-zA-Z0-9\_\.]+\.baidu\.com)/gi,r=window.location.href,o=document.referrer;if(!e.test(r)){var n="//api.share.baidu.com/s.gif";o?(n+="?r="+encodeURIComponent(document.referrer),r&&(n+="&l="+r)):r&&(n+="?l="+r);var t=new Image;t.src=n}}(window);
</script>     
</section>
        </div>
        <footer id="footer">
    <div class="outer">
        <div id="footer-info" class="inner">
            Tao Liu &copy; 2022 
            <!-- <a rel="license noopener" target="_blank" href="http://creativecommons.org/licenses/by-nc-nd/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-nd/4.0/80x15.png" /></a> -->
            <br> Powered by <a href="http://hexo.io/" target="_blank" rel="external nofollow noopener noreferrer">Hexo</a>. Theme - <a target="_blank" rel="external nofollow noopener noreferrer" href="https://github.com/taoliu-hub/hexo-theme-Wikitten">wikitten</a>
            <!-- 
                <br>
                <span id="busuanzi_container_site_pv"><i class="fa fa-eye"></i> <span id="busuanzi_value_site_pv"></span></span>
                &nbsp;|&nbsp;
                <span id="busuanzi_container_site_pv"><i class="fa fa-user"></i> <span id="busuanzi_value_site_uv"></span></span>
             -->
        </div>
    </div>
</footer>

        

    
        
<script src="/libs/lightgallery/js/lightgallery.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-thumbnail.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-pager.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-autoplay.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-fullscreen.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-zoom.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-hash.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-share.min.js"></script>

        
<script src="/libs/lightgallery/js/lg-video.min.js"></script>

    
    
        
<script src="/libs/justified-gallery/jquery.justifiedGallery.min.js"></script>

    
    
        <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true,
            TeX: {
                equationNumbers: {
                  autoNumber: 'AMS'
                }
            }
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    



<!-- Custom Scripts -->

<script src="/js/main.js"></script>


    </div>
</body>
</html>